\section{Introduction}
% Main goal: Argue for SC, and \xQ. Present \xQ{} and justify decisions. Demonstrate the performance on `realistic simulations'.

Thanks to recent advances in AI and machine learning, unmanned autonomous physical systems (APS) are poised to tackle complex decision making problems for high-consequence applications, such as wilderness search and rescue, air and ground transportation, national defense, agriculture, and remote science and space exploration. 
%Unlike low-level automation for assembly lines, cruise control, thermostats, etc., 
APS are expected to be self-sufficient and to make self-guided decisions about complex problems delegated by users/stakeholders. Hence, APS that are taskable---able to translate high-level commands into suitable processes for sensing, learning, reasoning, communicating, and acting under uncertainty---must also be cognizant and knowledge-rich---capable of introspectively reasoning about the capabilities and limitations of their own processes, anticipating possible failures, and able to recognize when they are operating incorrectly in order to adapt accordingly. %To ensure long-term robustness and resilience for minimally supervised operations, APS behaviors must be predictable, understandable, and explainable. %to human users/stakeholders. %, who in many cases can also provide collaborative high-level assistance or supervisory directives in difficult situations. 
\nisar{cite DOD, etc.}

This work is motivated by the need to develop new computational strategies for assessing when an APS reaches its \emph{competency boundaries}. If computed and communicated correctly, such assessments can provide users with clearer predictions of APS behavior and understanding of actual APS capabilities. This can not only allow APS to take initiatives to stay within its competency boundary in untested situations, but also provide users/stakeholders with assurances that allow them to properly calibrate trust in (and hence make proper use of) intelligent APS \cite{Israelsen2018-es}. 

These properties are especially important for high-consequence APS that must rely heavily on non-deterministic algorithms for decision-making under uncertainty, i.e. to efficiently make approximate inferences with imperfect models, learn from limited data, and execute potentially risky actions with limited information. 
%From self-driving cars and unmanned aircraft on Earth to planetary rovers in space \cite{}, and from networked smart devices in the home to smart building systems \cite{}, such APS are now becoming a pervasive part of everyday reality. 
%Crucially, these systems require interaction among several different algorithmic components to support intelligent APS capabilities (for sensing, perception, planning, control, communication, etc.). 
%Algorithms for these capabilities are often studied in isolation, but not together... 
%\nisar{but not much work has been done in terms of holistically looking at APS operating under uncertainty -- expand on this in background...} 
Whereas most relevant and recent work on algorithmic introspection and meta-reasoning to date has focused on outcome-based analyses for  AI/learning agents with narrow well-defined tasks,
%such techniques are typically best suited to APS with relatively narrow well-defined capabilities and few computational resource constraints. Since many current and future APS must operate in open-ended task settings in physical environments with significant limitations (due to constrained platform size, weight, power, etc.), the interpretation of `favorable/unfavorable' outcomes can shift in subtle yet significant ways as a function of APS design and task context. To cope with broader classes of APS, %(i.e. engineered sum of interconnected algorithmic and physical parts), 
holistic process-based techniques for algorithmic competency boundary self-assessment are needed to accommodate broader classes of APS operating in complex, dynamic and uncertain real-world settings -- whose computational models and approximations are expected to break down in less obvious/foreknown ways. \nisar{meh?}

This paper presents and builds on a recently developed algorithmic framework for computing and evaluating self-assessments in APS that leads to shorthand metrics of \emph{machine self-confidence}. Self-confidence is defined as an APS' perceived ability to achieve assigned goals after accounting for uncertainties in its knowledge of the world, its own state, and its own reasoning and execution abilities \cite{Aitken2016-cv, Aitken2016-fb, Sweet2016-tz}. 
Algorithmic computation of self-confidence is strongly linked to model-based assessments of probabilities pertaining to task outcomes and completion---but crucially goes further to provide insight into how well an APS's processes for decision-making, learning, perception, etc. are matched to intended tasks \cite{Hutchins2015-if}. 
We argue that the short-hand insight provided by self-confidence assessments can serve as a transparent and decomposable/traceable feedback signal to anticipate degraded, nominal, or enhanced APS performance, %adapt autonomous behavior, 
and thereby can be used to calibrate user trust in APS for uncertain task settings. 

The main contributions of this paper include: 1) A formal definition of `solver-quality' (\xq) which is one of several factors that make up `self-confidence'. Herein, solver-quality is presented as a metric for assessing how competent an MDP solver is for a given task. 2) \xQ{} is then derived borrowing inspiration from empirical hardness models (EHMs \cite{Leyton-Brown2009-yr}. 3) \xq{} is evaluated using numerical experiments.

The document is organized as follows: In Section~\ref{sec:background} we further explore motivations and background for self-confidence, including concepts like trust between humans and autonomous systems, and a useful example application. In Section~\ref{sec:self-confidence} Factorized Machine Self-Confidence (\famsec) is introduced and a framework outlined. At the end of Section~\ref{sec:self-confidence} we turn our attention to one of the \famsec{} factors, `Solver Quality', and outline specific challenges and desiderata in the context of the broadly useful family of Markov Decision Process (MDP)-based planners. A learning-based technique for computing solver quality factors in MDP-family planners is then derived in Section~\ref{sec:methodology}. In Section~\ref{sec:results} we present results from numerical experiments for an unmanned autonomous vehicle navigation problem. Finally we present conclusions in Section~\ref{sec:conclusions}.

\section{Background and Related Work} \label{sec:background}
This section reviews several key concepts and related works which set the stage for our proposed computational machine self-confidence framework. To make the concepts discussed throughout the paper concrete and provide an accessible proof-of-concept testbed in later sections, we also describe a motivating APS application example inspired by ongoing research in unmanned robotic systems.  

\subsection{Autonomous Systems and User Trust}
An APS is generally any physical agent comprised of a machine controlled by some form of software-based autonomy. Autonomy defines the ability of the system to perform a complex set of tasks with little/no human supervisory intervention for extended periods of time \nisar{cite DOD.} This generally means that an APS has at least one or more of the capabilities of an artificially intelligent physical agent, i.e. reasoning, knowledge representation, planning, learning, perception, motion/manipulation, and/or communication \cite{Israelsen2017-ym}. 
Despite many popular myths and misconceptions, an APS always interacts with a human user in some way \cite{Bradshaw2013-ck}. 
That is, the aforementioned capabilities are the means by which an APS achieves some \emph{intended} degree of self-sufficiency and self-directedness for tasks that are \emph{delegated} by a user in order to meet an `intent frame' (desired set of goals, plans, constraints, stipulations, and/or value statements) \cite{Miller2014-av}. `Transparency' in this context thus shifts away from concern over how exactly an APS accomplishes a task, towards concern over whether or not an autonomous system can execute the task per the user's intent frame. \nisar{hook to `drill down' req: user in high-consequence situation would want to know why/why not...and ask more questions as needed or as time/context permits... }

This view naturally sets up several questions related to user trust in autonomous systems. Trust defines a user's willingness and security in depending on an APS to carry out a delegated set of tasks, having taken into consideration its characteristics and capabilities. 
We focus here on the problem of how an APS can be designed to actively assist users in appropriately calibrating their trust in the APS. As surveyed in \cite{Israelsen2018-es} \nisar{not repeating/aliasing citations in the back?}, several broad classes of \emph{algorithmic assurances} for APS have been developed, where an assurance is defined as any property or behavior that can serve to increase or decrease a user's trust. 
Good assurances are challenging to develop because they must allow users to gain better insight and understanding of APS behaviors for effectively managing operations, without undermining autonomous operations or burdening users in the process. 
Many assurance strategies, such as value alignment \cite{Dragan2014-gu} (where an APS adapts its behavioral objectives with a user's intent frame via interactive learning) and interpretable reasoning \cite{} (where capabilities for planning, learning, reasoning, etc. are made accessible and easy to understand for non-expert users) put the honus on the APS (and designers) to integrate naturally transparent trust-calibrating behaviors into core system functionality. 
Other strategies, such as those based on post hoc explanation for learning and reasoning systems \cite{Lacave2004-gq, ...} and data visualization \cite{}, require users to render their own judgments via processing of information provided by the APS (possibly in response to specific user queries). 
Indeed, the full range of assurance design strategies for APS have much in common with techniques for ensuring transparency and accountability for more general AI and learning systems. 
Assurances based on algorithmic self-monitoring merit special attention as promising mechanisms for assessing APS competency boundaries. 

\subsection{Self-Monitoring and Self-Confidence}
State of the art machine learning and statistical AI methods have ushered in major improvements to APS capabilities in recent years. 
Yet, as these methods and capabilities continue to improve and find new high-consequence applications, resulting APS implementations are also becoming more complex, opaque and difficult for users (as well as designers and certifying authorities) to fully comprehend. 
In particular, for sophisticated APS characterized by uncertainty-based reasoning and data-driven learning, it can become extremely difficult to make precise predictions about APS behavior and performance limits in noisy, untested, and `out of scope' task conditions with any degree of certainty. Formal verification and validation tools are being developed to tackle these issues at design time \cite{...}, but do not provide assurances that can be readily conveyed to or understood by (non-expert) users at run-time. 
It can thus be argued that the task of assessing APS competency at run-time is in general so complex and burdensome that it should be automatically delegated to the APS itself. 

This leads to consideration of algorithmic self-monitoring methods, e.g. for introspective reasoning/learning \cite{Huang2017-zt, Huang2017-lk,...}, fault diagnosis and anomaly detection \cite{...}, and computational meta-reasoning/meta-learning \cite{Griffiths,...}. 
While promising for a wide variety of applications, these methods depend heavily on task outcome and performance assessments, and often require data intensive evaluations. 
As such, these methods are often best-suited to APS with narrow well-defined capabilities and few computational resource constraints. 
However, many current and future APS must operate in open-ended task settings in physical environments with significant computational limitations (due to constrained platform size, weight, power, etc.). The interpretation of `favorable vs. unfavorable' task outcomes can also shift in subtle yet significant ways, depending on the interactions of designed APS capabilities and task context (all of which may change drastically over the course of a given operational instance). \nisar{...makes it difficult for non-expert users to trace and drill down into and understand assessments... } 
%\nisar{TODO: briefly talk about other background things related to transparency for introspection, Dragan's critical states, meta-RL, meta-cog architectures, etc.; ... what are the drawbacks/limitations in context of autonomous systems? what moves us towards something different like self-confidence? key idea: these are largely outcome based and driven by achievement of robustness/resilience for task performance, as opposed to process-based and do not provide clear assessment/ability to quantify/qualify/communicate what APS actually can or cannot accomplish...taking meta RL as example: system will keep trying to do its best to learn... for critical state MDP: presumes that solver/models/reward functions used to come up with solution are in fact appropriate and that capabilities/scope given to system is in fact appropriate (solution/implementation of APS is not placed into context)}

%\nisar{something here to get into the idea of APS being different but not totally unrelated to ML/AI (e.g. consequences of failure might be loss of APS or damage/irreparable loss to surroundings, as well as dynamic settings...) --  (licensing argument, i.e. we don't certify humans, but do put them through a process-based licensing approach? basically: a big question/central tension here is: how to monitor/supervise APS within competency limits without undercutting very point of autonomy itself? )}

\nisar{TODO: %describe relationship to metacog/human self-confidence: recent results in computational cog sci show that this is a fundamental component of how humans handle decision making under uncertainty...though, main thrust of this work so far has been neuro/psychophysics and neurocompsci, other evidence from areas like sports psychology, etc. suggest that introspective self-assessment and `self-trust' play an important role in more complex tasks 
go from self-monitoring/scoring to self-qualification: more process-based:  key theme is that decision makers often better at assessing competency boundaries on unknown/unfamiliar tasks, which in turn requires looking beyond mere assessments of uncertainty to look more deeply at uncertainties in uncertainty as well as self-assessments of own reasoning processes... then go onto Hutchins' paper idea: expert evaluations/scoring of specific algorithms in context of SOVA loops; limitation: no obvious way to have APS self-generate assessments: must rely on hand-tailored human expert assessments, which gets cumbersome for a lot of different situations...talk about other computational self-confidence ideas and limitations, and what we expect to do...}

\nisar{will trim this down...} Several definitions and computing techniques for machine self-confidence have been proposed recently. Much of this work is reviewed in \cite{Israelsen2017-ym} in the context of algorithmic interactions for human-autonomous system trust relationships. %, where self-confidence is identified as an explicit assurance in a human-autonomy trust relationship. 
According to \cite{Sweet2016-tz} the four views on self-confidence are the \textit{anthropomorphic view}, the \textit{uncertainty view}, the \textit{experiential view}, and the \textit{stability view}. The anthropomorphic view defines self-confidence to be similar to how humans express self-confidence, while the experiential view expresses self-confidence based on past experience. The uncertainty view simply defines self-confidence to be the probability of success or failure, and the stability view defines self-confidence to be the sensitivity of the probability of success to uncertainty. All of these views seem to reflect different parts of a more general concept: understanding an autonomous system's ability to do a specific task. 
This leads to our definition of self-confidence: \textbf{An agent's perceived ability to achieve assigned goals (within a defined region of autonomous behavior) after accounting for (1) uncertainties in its knowledge of the world, (2) uncertainties of its own state, and (3) uncertainties about its reasoning process and execution abilities.}

\hlr{...also consider other self-confidence ideas/approaches (e.g. four different approaches considered in InfoTech paper) and algorithms (e.g. Ugur's plan robustness idea, surprise index, perception robustness/statistical residuals test)... one limitation of algs: only limited to specific segments or types of reasoning, rather than looking at interconnected processes holistically for autonomous *systems* }

\subsection{MDP-based Planning and Learning}
WHY DO WE USE MDPS? THEY ARE A GOOD PLACE TO START FROM, AND ARE CONNECTED TO MANY OTHER LEARNING APPROACHES (REINFORCEMENT, INVERSE REINFORCEMENT, \ldots)...also POMDPs for dealing with partially observable systems, which describe wide range of practical systems... 

\nisar{MDP nomenclature...?}

...Reinforcement learning is finding a policy when transitions, and states are unknown. Basically learn policy and model simultaneously. % inverse RL is learning a reward function by observing a policy.

...MOMDPs and POMDPs as generalization...

...need for approximations...

\nisar{TODO: also help bridge gap to \famsec{} in next section...\famsec{} not exclusive to MDPs, but it's a sensible place to start...note: approximations of reality needed to set up models of decision processes, and then require even more approximations on top of these to actually implement...so this gives a good consequential focus to develop s/c framework}

\subsection{VIP Escort Example Application} \label{sec:vip_escort}
Consider a concrete grounding example problem based on the ``VIP escort'' scenario~\cite{Humphrey2012-lr}, which %can be viewed as a variant of the ``Minotaur's Labyrinth'' and 
serves as a useful proxy for security and surveillance applications with unmanned robotic vehicles. An unmanned ground vehicle (UGV) leads a small convoy protecting a VIP through a road network monitored by friendly unattended ground sensors (UGS). The road network also contains a hostile pursuer that the UGV is trying to evade while exiting the network as quickly as possible. The pursuer's location is unknown but can be estimated using intermittent data from the UGS, which only sense portions of the network and can produce false alarms. The UGV's decision space involves selecting a sequence of discrete actions (i.e. go straight, turn left, turn right, go back, stay in place). The UGS data, UGV motion, and pursuer behavior are all stochastic, and the problems of decision making and sensing are strongly coupled: some trajectories through the network allow the UGV to localize the pursuer before heading to the exit but incur a high time penalty); other trajectories afford rapid exit with high pursuer location uncertainty but increase the risk of getting caught by the pursuer, which can take multiple paths. A human supervisor monitors the UGV during operation. The supervisor does not have detailed knowledge of the UGV -- but can interrogate its actions, as well as potentially modify its decision making stance (`aggressively pursue exit' vs. `be very conservative and cautious') and provide extra information about the pursuer (which is sporadically observed and follows an unknown course). \nisar{add in more formal math notation...assume STM, obs model, rewards, etc...}
    
	\begin{figure}[t]%[htbp]
    	\centering
     	\includegraphics[width=0.4\textwidth]{Figures/RoadNet}
    	\caption{UGV in road network evading pursuer with information from noisy UGS.} 
        \label{fig:RoadNet}
       % \vspace{-0.2 in}
    \end{figure}

\nisar{TODO: edit this: use some more formal MDP/POMDP notation...remove/compress bits at end...}One way to construct an autonomous UGV path planner is to discretize time and spatial variables to build a partially observable Markov decision process (POMDP) model \cite{Kochenderfer2015-uu} of the navigation task. The ideal POMDP solution is an optimal UGV action selection policy that will, \emph{on average}, maximize some utility function whose optimum value coincides with desirable UGV behaviors (i.e. avoiding the pursuer and exiting quickly). POMDP policies can be calculated by any number of sophisticated approximations that operate on probability distributions for the unknown pursuer state, which in turn can be found via Bayesian sensor fusion \cite{Ahmed2017-ph}. This defines at least two AIA capabilities per Fig. \ref{fig:AIcapabilities}: knowledge representation and planning \footnote{consideration of lower-level UGV state estimation and control also leads to perception and motor control/execution.}. 
The trust-cycle terms here can then be defined as follows relative to the supervisor (user): \textit{AIA:} the combined POMDP planning and data fusion agent, which must make decisions under uncertainty; \textit{Trust:} the supervisor's willingness to rely on the UGV's planning and data fusion algorithms when the safety of the VIP being escorted is at stake; \textit{TRBs:} supervisor's behaviors that indicate trust (or lack thereof) in the UGV's planner; these include approving/rejecting the planner's actions, or real-time adjustments of the data fusion output based on what the supervisor receives from other intelligence sources; \textit{Assurances:} properties and behaviors of the planning agent that effect the supervisor's trust, e.g. communication of the escape success probability, reports that unexpected UGS data have been registered, or explanations of actions taken.