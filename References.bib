% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Browne2012-fo,
  title    = "A Survey of Monte Carlo Tree Search Methods",
  author   = "Browne, C B and Powley, E and Whitehouse, D and Lucas, S M and
              Cowling, P I and Rohlfshagen, P and Tavener, S and Perez, D and
              Samothrakis, S and Colton, S",
  abstract = "Monte Carlo tree search (MCTS) is a recently proposed search
              method that combines the precision of tree search with the
              generality of random sampling. It has received considerable
              interest due to its spectacular success in the difficult problem
              of computer Go, but has also proved beneficial in a range of
              other domains. This paper is a survey of the literature to date,
              intended to provide a snapshot of the state of the art after the
              first five years of MCTS research. We outline the core
              algorithm's derivation, impart some structure on the many
              variations and enhancements that have been proposed, and
              summarize the results from the key game and nongame domains to
              which MCTS methods have been applied. A number of open research
              questions indicate that the field is ripe for future work.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  4,
  number   =  1,
  pages    = "1--43",
  month    =  mar,
  year     =  2012,
  keywords = "game theory;Monte Carlo methods;tree searching;Monte carlo tree
              search methods;random sampling generality;computer Go;MCTS
              research;key game;nongame domains;Games;Monte Carlo
              methods;Artificial intelligence;Game theory;Computers;Markov
              processes;Decision theory;Artificial intelligence
              (AI);bandit-based methods;computer Go;game search;Monte Carlo
              tree search (MCTS);upper confidence bounds (UCB);upper confidence
              bounds for trees (UCT)"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chatila2017-lb,
  title     = "Toward {Self-Aware} Robots",
  author    = "Chatila, Raja and Renaudo, Erwan and Andries, Mihai and
               Chavez-Garcia, Ricardo-Omar and Luce-Vayrac, Pierre and
               Gottstein, Rapha{\"e}l and Alami, Rachid and Clodic, Aur{\'e}lie
               and Devin, Sandra and Girard, Beno{\^\i}t and {Others}",
  abstract  = "Despite major progress in Robotics and AI, robots are still
               basically`` zombies'' repeatedly achieving actions and tasks
               without understanding what they are doing. Deep-Learning AI
               programs classify tremendous amounts of data without grasping
               the meaning of their inputs …",
  journal   = "Frontiers in Robotics and AI",
  publisher = "hal.archives-ouvertes.fr",
  volume    =  5,
  pages     = "88",
  year      =  2017
}

@INPROCEEDINGS{Dosilovic2018-cu,
  title     = "Explainable artificial intelligence: A survey",
  booktitle = "2018 41st International Convention on Information and
               Communication Technology, Electronics and Microelectronics
               ({MIPRO})",
  author    = "Do{\v s}ilovi{\'c}, F K and Br{\v c}i{\'c}, M and Hlupi{\'c}, N",
  abstract  = "In the last decade, with availability of large datasets and more
               computing power, machine learning systems have achieved
               (super)human performance in a wide variety of tasks. Examples of
               this rapid development can be seen in image recognition, speech
               analysis, strategic game planning and many more. The problem
               with many state-of-the-art models is a lack of transparency and
               interpretability. The lack of thereof is a major drawback in
               many applications, e.g. healthcare and finance, where rationale
               for model's decision is a requirement for trust. In the light of
               these issues, explainable artificial intelligence (XAI) has
               become an area of interest in research community. This paper
               summarizes recent developments in XAI in supervised learning,
               starts a discussion on its connection with artificial general
               intelligence, and gives proposals for further research
               directions.",
  publisher = "ieeexplore.ieee.org",
  pages     = "0210--0215",
  month     =  may,
  year      =  2018,
  keywords  = "interpretability;healthcare;finance;explainable artificial
               intelligence;XAI;recent developments;supervised
               learning;artificial general intelligence;datasets;computing
               power;machine learning systems;(super)human performance;image
               recognition;speech analysis;strategic game
               planning;state-of-the-art models;transparency;Predictive
               models;Machine learning;Support vector machines;Decision
               trees;Supervised learning;Optimization;explainable artificial
               intelligence;interpretability;explainability;comprehensibility"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Definition_undated-jq,
  title    = "``Dave. . . {I} can assure you . . . that it's going to be all
              right . . . ''∗",
  author   = "Definition, A and For, Case",
  keywords = "Paper Reviews"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Bishop1994-tg,
  title       = "Mixture density networks",
  author      = "Bishop, Christopher M",
  abstract    = "Minimization of a sum-of-squares or cross-entropy error
                 function leads to network outputs which approximate the
                 conditional averages of the target data, conditioned on the
                 input vector. For classi cations problems, with a suitably
                 chosen target coding scheme, these averages represent the
                 posterior probabilities of class membership, and so can be
                 regarded as optimal. For problems involving the prediction of
                 continuous variables, however, the conditional averages
                 provide only a very limited description of the properties of
                 the target …",
  publisher   = "Citeseer",
  institution = "Citeseer",
  year        =  1994,
  keywords    = "SelfConfidence"
}

@ARTICLE{Schweighofer2003-qa,
  title     = "Meta-learning in reinforcement learning",
  author    = "Schweighofer, Nicolas and Doya, Kenji",
  abstract  = "Meta-parameters in reinforcement learning should be tuned to the
               environmental dynamics and the animal performance. Here, we
               propose a biologically plausible meta-reinforcement learning
               algorithm for tuning these meta-parameters in a dynamic,
               adaptive manner. We tested our algorithm in both a simulation of
               a Markov decision task and in a non-linear control task. Our
               results show that the algorithm robustly finds appropriate
               meta-parameter values, and controls the meta-parameter time
               course, in both static and dynamic environments. We suggest that
               the phasic and tonic components of dopamine neuron firing can
               encode the signal required for meta-learning of reinforcement
               learning.",
  journal   = "Neural Netw.",
  publisher = "Elsevier",
  volume    =  16,
  number    =  1,
  pages     = "5--9",
  month     =  jan,
  year      =  2003,
  language  = "en"
}

@ARTICLE{Wang2016-rv,
  title         = "Learning to reinforcement learn",
  author        = "Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and
                   Soyer, Hubert and Leibo, Joel Z and Munos, Remi and
                   Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt",
  abstract      = "In recent years deep reinforcement learning (RL) systems
                   have attained superhuman performance in a number of
                   challenging task domains. However, a major limitation of
                   such applications is their demand for massive amounts of
                   training data. A critical present objective is thus to
                   develop deep RL methods that can adapt rapidly to new tasks.
                   In the present work we introduce a novel approach to this
                   challenge, which we refer to as deep meta-reinforcement
                   learning. Previous work has shown that recurrent networks
                   can support meta-learning in a fully supervised context. We
                   extend this approach to the RL setting. What emerges is a
                   system that is trained using one RL algorithm, but whose
                   recurrent dynamics implement a second, quite separate RL
                   procedure. This second, learned RL algorithm can differ from
                   the original one in arbitrary ways. Importantly, because it
                   is learned, it is configured to exploit structure in the
                   training domain. We unpack these points in a series of seven
                   proof-of-concept experiments, each of which examines a key
                   aspect of deep meta-RL. We consider prospects for extending
                   and scaling up the approach, and also point out some
                   potentially important implications for neuroscience.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.05763"
}

@INPROCEEDINGS{Schaul2015-fm,
  title     = "Universal Value Function Approximators",
  booktitle = "Proceedings of the 32nd International Conference on Machine
               Learning",
  author    = "Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver,
               David",
  editor    = "Bach, Francis and Blei, David",
  abstract  = "Value functions are a core component of reinforcement learning.
               The main idea is to to construct a single function approximator
               V(s; theta) that estimates the long-term reward from any state
               s, using parameters $\vartheta$. In this paper we introduce
               universal value function approximators (UVFAs) V(s,g;theta) that
               generalise not just over states s but also over goals g. We
               develop an efficient technique for supervised learning of UVFAs,
               by factoring observed values into separate embedding vectors for
               state and goal, and then learning a mapping from s and g to
               these factored embedding vectors. We show how this technique may
               be incorporated into a reinforcement learning algorithm that
               updates the UVFA solely from observed rewards. Finally, we
               demonstrate that a UVFA can successfully generalise to
               previously unseen goals.",
  publisher = "PMLR",
  volume    =  37,
  pages     = "1312--1320",
  series    = "Proceedings of Machine Learning Research",
  year      =  2015,
  address   = "Lille, France"
}

@INPROCEEDINGS{Zheng1995-oo,
  title     = "Nonlinear system identification for control using
               {Volterra-Laguerre} expansion",
  booktitle = "American Control Conference, Proceedings of the 1995",
  author    = "Zheng, Qingsheng and Zafiriou, E",
  abstract  = "The large number of parameters needed to represent the kernels
               is the major criticism in using Volterra series as nonlinear
               models. Kurth and Rake (1994) proposed a scheme to reduce this
               number for Hammerstein and Wiener systems. In this paper, the
               authors extend this scheme to general nonlinear systems and
               investigate the conditions under which a nonlinear system can be
               represented by the Volterra-Laguerre expansion. It is found that
               systems which possess such expansion can be characterized by the
               concept of stably separable kernels. As such, a large class of
               systems of practical interest, like fading memory nonlinear
               systems, can be approximated by the Volterra-Laguerre expansion.
               An orthogonal regression analysis method is introduced to
               further reduce the parameter number. The control-relevant
               identification issue pertinent to high performance nonlinear
               internal model control (NIMC) is accordingly addressed in the
               CSTR and rapid thermal processing (RTP) examples are provided to
               illustrate the usefulness of this technique",
  publisher = "ieeexplore.ieee.org",
  volume    =  3,
  pages     = "2195--2199 vol.3",
  month     =  jun,
  year      =  1995,
  keywords  = "Volterra series;chemical technology;identification;nonlinear
               control systems;rapid thermal processing;statistical
               analysis;stochastic processes;CSTR;Hammerstein systems;Volterra
               series;Volterra-Laguerre expansion;Wiener systems;fading memory
               nonlinear systems;high performance nonlinear internal model
               control;nonlinear system identification;orthogonal regression
               analysis;rapid thermal processing;stably separable
               kernels;Chemical engineering;Continuous-stirred tank
               reactor;Control systems;Educational institutions;Kernel;Linear
               systems;Nonlinear control systems;Nonlinear systems;Rapid
               thermal processing;Regression analysis"
}

@ARTICLE{Kizilcec_undated-kj,
  title  = "How Much Information? Effects of Transparency on Trust in an
            Algorithmic Interface",
  author = "Kizilcec, Rene F"
}

@BOOK{Schetzen1980-ao,
  title     = "The Volterra and Wiener theories of nonlinear systems",
  author    = "Schetzen, Martin",
  abstract  = "This text presents a complete and detailed development of the
               analysis, design and characterization of non-linear systems
               using the Volterra and Wiener theories, as well as gate
               functions, thus yielding new insights and a better comprehension
               of the subject. The Volterra and Wiener theories are useful in
               the study of systems in biological, mechanical, and electrical
               fields.",
  publisher = "Wiley",
  month     =  apr,
  year      =  1980
}

@BOOK{Sutton1998-qn,
  title     = "Reinforcement Learning: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G and {Co-Director
               Autonomous Learning Laboratory Andrew G Barto}",
  abstract  = "Richard Sutton and Andrew Barto provide a clear and simple
               account of the key ideas and algorithms of reinforcement
               learning. Their discussion ranges from the history of the
               field's intellectual foundations to the most recent developments
               and applications. Reinforcement learning, one of the most active
               research areas in artificial intelligence, is a computational
               approach to learning whereby an agent tries to maximize the
               total amount of reward it receives when interacting with a
               complex, uncertain environment. In Reinforcement Learning,
               Richard Sutton and Andrew Barto provide a clear and simple
               account of the key ideas and algorithms of reinforcement
               learning. Their discussion ranges from the history of the
               field's intellectual foundations to the most recent developments
               and applications. The only necessary mathematical background is
               familiarity with elementary concepts of probability. The book is
               divided into three parts. Part I defines the reinforcement
               learning problem in terms of Markov decision processes. Part II
               provides basic solution methods: dynamic programming, Monte
               Carlo methods, and temporal-difference learning. Part III
               presents a unified view of the solution methods and incorporates
               artificial neural networks, eligibility traces, and planning;
               the two final chapters present case studies and consider the
               future of reinforcement learning.",
  publisher = "MIT Press",
  year      =  1998,
  language  = "en"
}

@ARTICLE{Ahmed2017-ph,
  title     = "Multitarget Localization on Road Networks with Hidden Markov
               {Rao--Blackwellized} Particle Filters",
  author    = "Ahmed, Nisar and Casbeer, David and Cao, Yongcan and Kingston,
               Derek",
  abstract  = "This paper considers the problem of tracking multiple moving
               targets on a road network with sparse, highly localized,
               unattended ground sensor data that are subject to clutter and
               missed detections. Hidden Markov models for single-target
               localization with unattended ground sensor data are first
               derived for road networks, under the assumption of perfect data
               association. These hidden Markov models are then used to solve
               the data association problem in the presence of clutter and
               missed detections for multitarget tracking using a
               Rao?Blackwellized particle filter. The proposed hidden Markov
               model tracking approach permits easy generation of accurate
               probabilistic models from a priori road network structure
               information, and it naturally enables sparse computationally
               efficient handling of multimodal target state uncertainties
               using both positive and negative unattended ground sensor
               information. The Rao?Blackwellized particle filter provides a
               fully Bayesian solution to the data association problem,
               enabling exploration of the association hypotheses space that
               leverages the computational advantages of exact hidden Markov
               model inference for multimodal state estimation. Numerical
               simulations demonstrate the effectiveness of the hidden Markov
               model/Rao?Blackwellized particle filter on challenging
               multitarget tracking scenarios with high false-alarm and missed
               detection rates.",
  journal   = "Journal of Aerospace Information Systems",
  publisher = "American Institute of Aeronautics and Astronautics",
  volume    =  14,
  number    =  11,
  pages     = "573--596",
  month     =  sep,
  year      =  2017
}

@ARTICLE{Platanios2017-xr,
  title         = "Estimating Accuracy from Unlabeled Data: A Probabilistic
                   Logic Approach",
  author        = "Platanios, Emmanouil A and Poon, Hoifung and Mitchell, Tom M
                   and Horvitz, Eric",
  abstract      = "We propose an efficient method to estimate the accuracy of
                   classifiers using only unlabeled data. We consider a setting
                   with multiple classification problems where the target
                   classes may be tied together through logical constraints.
                   For example, a set of classes may be mutually exclusive,
                   meaning that a data instance can belong to at most one of
                   them. The proposed method is based on the intuition that:
                   (i) when classifiers agree, they are more likely to be
                   correct, and (ii) when the classifiers make a prediction
                   that violates the constraints, at least one classifier must
                   be making an error. Experiments on four real-world data sets
                   produce accuracy estimates within a few percent of the true
                   accuracy, using solely unlabeled data. Our models also
                   outperform existing state-of-the-art solutions in both
                   estimating accuracies, and combining multiple classifier
                   outputs. The results emphasize the utility of logical
                   constraints in estimating accuracy, thus validating our
                   intuition.",
  month         =  may,
  year          =  2017,
  keywords      = "Quantify Uncertainty;Supplemental Assurance;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1705.07086"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mitchell2018-jw,
  title     = "Never-ending Learning",
  author    = "Mitchell, T and Cohen, W and Hruschka, E and Talukdar, P and
               Yang, B and Betteridge, J and Carlson, A and Dalvi, B and
               Gardner, M and Kisiel, B and Krishnamurthy, J and Lao, N and
               Mazaitis, K and Mohamed, T and Nakashole, N and Platanios, E and
               Ritter, A and Samadi, M and Settles, B and Wang, R and Wijaya, D
               and Gupta, A and Chen, X and Saparov, A and Greaves, M and
               Welling, J",
  abstract  = "… 39. Thrun, S., Pratt, L. (eds) Learning to learn , Kluwer
               Academic Publishers, Norwell, MA, USA, 1998. 40. Tong, S.,
               Koller, D. Active learning for structure in bayesian networks …
               In Proceedings of the International Conference on Learning
               Representations (ICLR) (2015). 43 …",
  journal   = "Commun. ACM",
  publisher = "ACM",
  volume    =  61,
  number    =  5,
  pages     = "103--115",
  month     =  apr,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "Supplemental Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Miller2014-av,
  title     = "Delegation and Transparency: Coordinating Interactions So
               Information Exchange Is No Surprise",
  booktitle = "Virtual, Augmented and Mixed Reality. Designing and Developing
               Virtual and Augmented Environments",
  author    = "Miller, Christopher A",
  abstract  = "We argue that the concept and goal of ``transparency'' in
               human-automation interactions does not make sense as naively
               formulated; humans cannot be aware of everything automation is
               doing and why in most circumstances if there is to be any
               cognitive workload savings. Instead, we argue, a concept of
               transparency based on and shaped by delegation interactions
               provides a framework for what should be communicated in
               ``transparent'' interactions and facilitates that communication
               and comprehension. Some examples are provided from recent work
               in developing delegation systems.",
  publisher = "Springer International Publishing",
  pages     = "191--202",
  year      =  2014,
  keywords  = "Assurances"
}

@ARTICLE{Sacha2016-tu,
  title     = "The Role of Uncertainty, Awareness, and Trust in Visual
               Analytics",
  author    = "Sacha, Dominik and Senaratne, Hansi and Kwon, Bum Chul and
               Ellis, Geoffrey and Keim, Daniel A",
  abstract  = "Visual analytics supports humans in generating knowledge from
               large and often complex datasets. Evidence is collected,
               collated and cross-linked with our existing knowledge. In the
               process, a myriad of analytical and visualisation techniques are
               employed to generate a visual representation of the data. These
               often introduce their own uncertainties, in addition to the ones
               inherent in the data, and these propagated and compounded
               uncertainties can result in impaired decision making. The user's
               confidence or trust in the results depends on the extent of
               user's awareness of the underlying uncertainties generated on
               the system side. This paper unpacks the uncertainties that
               propagate through visual analytics systems, illustrates how
               human's perceptual and cognitive biases influence the user's
               awareness of such uncertainties, and how this affects the user's
               trust building. The knowledge generation model for visual
               analytics is used to provide a terminology and framework to
               discuss the consequences of these aspects in knowledge
               construction and though examples, machine uncertainty is
               compared to human trust measures with provenance. Furthermore,
               guidelines for the design of uncertainty-aware systems are
               presented that can aid the user in better decision making.",
  journal   = "IEEE Trans. Vis. Comput. Graph.",
  publisher = "ieeexplore.ieee.org",
  volume    =  22,
  number    =  1,
  pages     = "240--249",
  month     =  jan,
  year      =  2016,
  keywords  = "Supplemental Assurance;vis\_dr;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Gal2016-eq,
  title      = "Dropout as a Bayesian Approximation: Representing Model
                Uncertainty in Deep Learning",
  booktitle  = "International Conference on Machine Learning",
  author     = "Gal, Yarin and Ghahramani, Zoubin",
  abstract   = "Deep learning tools have gained tremendous attention in applied
                machine learning. However such tools for regression and
                classification do not capture model uncertainty. In comparison,
                Bayesian models offer a mathematically grounded framework to
                reason about model uncertainty, but usually come with a
                prohibitive computational cost. In this paper we develop a new
                theoretical framework casting dropout training in deep neural
                networks (NNs) as approximate Bayesian inference in deep
                Gaussian processes. A direct result of this theory gives us
                tools to model uncertainty with dropout NNs -- extracting
                information from existing models that has been thrown away so
                far. This mitigates the problem of representing uncertainty in
                deep learning without sacrificing either computational
                complexity or test accuracy. We perform an extensive study of
                the properties of dropout's uncertainty. Various network
                architectures and non-linearities are assessed on tasks of
                regression and classification, using MNIST as an example. We
                show a considerable improvement in predictive log-likelihood
                and RMSE compared to existing state-of-the-art methods, and
                finish by using dropout's uncertainty in deep reinforcement
                learning.",
  publisher  = "jmlr.org",
  pages      = "1050--1059",
  month      =  jun,
  year       =  2016,
  keywords   = "Quantify Uncertainty;Supplemental Assurance;Assurances",
  language   = "en",
  conference = "International Conference on Machine Learning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gal2016-om,
  title     = "Uncertainty in deep learning",
  author    = "Gal, Yarin",
  abstract  = "Deep learning has attracted tremendous attention from
               researchers in various fields of information engineering such as
               AI, computer vision, and language processing [Kalchbrenner and
               Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but
               also from …",
  journal   = "University of Cambridge",
  publisher = "pdfs.semanticscholar.org",
  year      =  2016,
  keywords  = "Quantify Uncertainty;Supplemental Assurance;Assurances"
}

@INPROCEEDINGS{Zhu2015-kw,
  title     = "Machine Teaching: An Inverse Problem to Machine Learning and an
               Approach Toward Optimal Education",
  booktitle = "{AAAI}",
  author    = "Zhu, Xiaojin",
  abstract  = "I draw the reader's attention to machine teaching, the problem
               of finding an optimal training set given a machine learning
               algorithm and a target model. In addition to generating
               fascinating mathematical questions for computer scientists to
               ponder, machine teaching holds the promise of enhancing
               education and personnel training. The Socratic dialogue style
               aims to stimulate critical thinking.",
  publisher = "aaai.org",
  pages     = "4083--4087",
  year      =  2015,
  keywords  = "Supplemental Assurance;Reduce Complexity;Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Charikar2017-kr,
  title     = "Learning from Untrusted Data",
  booktitle = "Proceedings of the 49th Annual {ACM} {SIGACT} Symposium on
               Theory of Computing",
  author    = "Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory",
  abstract  = "The vast majority of theoretical results in machine learning and
               statistics assume that the training data is a reliable
               reflection of the phenomena to be learned. Similarly, most
               learning techniques used in practice are brittle to the presence
               of large amounts of biased or …",
  publisher = "ACM",
  pages     = "47--60",
  series    = "STOC 2017",
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "high-dimensional statistics, outlier removal, robust
               learning;Value Alignment;Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Soares2015-jc,
  title     = "Aligning Superintelligence with Human Interests: An Annotated
               Bibliography",
  author    = "Soares, N",
  abstract  = "How could superintelligent systems be aligned with the interests
               of humanity? This annotated bibliography compiles some recent
               research relevant to that question, and categorizes it into six
               topics:(1) realistic world models;(2) idealized decision
               theory;(3) logical uncertainty;(4) Vingean reflection;(5)
               corrigibility; and (6) value learning. Within each subject area,
               references are organized in an order amenable to learning the
               topic. These are by no means the only six topics relevant to the
               study of alignment, but this annotated …",
  journal   = "Intelligence",
  publisher = "Citeseer",
  year      =  2015,
  keywords  = "Value Alignment;Assurances"
}

@ARTICLE{Leike2017-du,
  title         = "{AI} Safety Gridworlds",
  author        = "Leike, Jan and Martic, Miljan and Krakovna, Victoria and
                   Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and
                   Orseau, Laurent and Legg, Shane",
  abstract      = "We present a suite of reinforcement learning environments
                   illustrating various safety properties of intelligent
                   agents. These problems include safe interruptibility,
                   avoiding side effects, absent supervisor, reward gaming,
                   safe exploration, as well as robustness to
                   self-modification, distributional shift, and adversaries. To
                   measure compliance with the intended safe behavior, we equip
                   each environment with a performance function that is hidden
                   from the agent. This allows us to categorize AI safety
                   problems into robustness and specification problems,
                   depending on whether the performance function corresponds to
                   the observed reward function. We evaluate A2C and Rainbow,
                   two recent deep reinforcement learning agents, on our
                   environments and show that they are not able to solve them
                   satisfactorily.",
  month         =  nov,
  year          =  2017,
  keywords      = "Value Alignment;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1711.09883"
}

@ARTICLE{Yudkowsky2001-hb,
  title    = "Creating friendly {AI} 1.0: The analysis and design of benevolent
              goal architectures",
  author   = "Yudkowsky, Eliezer",
  journal  = "The Singularity Institute, San Francisco, USA",
  year     =  2001,
  keywords = "Value Alignment;Assurances"
}

@MISC{Bensinger2014-ul,
  title        = "Stuart Russell: {AI} value alignment problem must be an
                  ``intrinsic part'' of the field's mainstream agenda",
  booktitle    = "lesswrong",
  author       = "Bensinger, Rob",
  abstract     = "Edge.org has recently been discussing ``the myth of AI''.
                  Unfortunately, although Superintelligence is cited in the
                  opening, most of the participants don't seem to have looked
                  into Bostrom's arguments. (Luke has written a **... (Read
                  More)",
  month        =  nov,
  year         =  2014,
  howpublished = "\url{https://www.lesswrong.com/posts/S95qCHBXtASmYyGSs/stuart-russell-ai-value-alignment-problem-must-be-an}",
  note         = "Accessed: 2018-6-20",
  keywords     = "Value Alignment;Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Gordon_Worley2018-xy,
  title        = "Formally Stating the {AI} Alignment Problem -- Map and
                  Territory",
  booktitle    = "Map and Territory",
  author       = "Gordon Worley, III, G",
  abstract     = "The development of smarter-than-human artificial intelligence
                  poses an existential and suffering risk to humanity. Given
                  that it is unlikely we can prevent and may not want to
                  prevent the development…",
  publisher    = "Map and Territory",
  month        =  feb,
  year         =  2018,
  howpublished = "\url{https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991}",
  note         = "Accessed: 2018-6-20",
  keywords     = "Value Alignment;Assurances"
}

@BOOK{Wenger2014-ld,
  title     = "Artificial Intelligence and Tutoring Systems: Computational and
               Cognitive Approaches to the Communication of Knowledge",
  author    = "Wenger, Etienne",
  abstract  = "Artificial Intelligence and Tutoring Systems: Computational and
               Cognitive Approaches to the Communication of Knowledge focuses
               on the cognitive approaches, methodologies, principles, and
               concepts involved in the communication of knowledge. The
               publication first elaborates on knowledge communication systems,
               basic issues, and tutorial dialogues. Concerns cover natural
               reasoning and tutorial dialogues, shift from local strategies to
               multiple mental models, domain knowledge, pedagogical knowledge,
               implicit versus explicit encoding of knowledge, knowledge
               communication, and practical and theoretical implications. The
               text then examines interactive simulations, existing CAI
               traditions, and learning environments. The manuscript elaborates
               on knowledge communication, didactics, and diagnosis. Topics
               include knowledge presentation and communication, pedagogical
               contexts, target levels of didactic operations, behavioral and
               epistemic diagnosis, and aspects of diagnostic experience. The
               publication is a dependable reference for researchers interested
               in the computational and cognitive approaches to the
               communication of knowledge.",
  publisher = "Morgan Kaufmann",
  month     =  may,
  year      =  2014,
  keywords  = "Assurances",
  language  = "en"
}

@MISC{Google2018-eb,
  title        = "Google Duplex: An {AI} System for Accomplishing {Real-World}
                  Tasks Over the Phone",
  booktitle    = "Google {AI} Blog",
  author       = "{Google}",
  abstract     = "Posted by Yaniv Leviathan, Principal Engineer and Yossi
                  Matias, Vice President, Engineering, Google A long-standing
                  goal of human-comput...",
  month        =  may,
  year         =  2018,
  howpublished = "\url{http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html}",
  note         = "Accessed: 2018-6-15",
  keywords     = "Integral Assurance;Human-like Behavior"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Van_Breemen2004-rz,
  title      = "Bringing Robots To Life: Applying Principles Of Animation To
                Robots",
  booktitle  = "Proceedings of Shaping {Human-Robot} Interaction workshop held
                at {CHI}",
  author     = "van Breemen, Ajn",
  abstract   = "The acceptance of user-interface robots as ``social friend''
                depends among others on the ability of the user to understand
                the robot's behavior− to understand what it is doing and
                thinking. Body gestures are a natural channel to communicate
                this. Traditionally, the control …",
  year       =  2004,
  keywords   = "Integral Assurance;Human-like Behavior",
  conference = "CHI"
}

@INPROCEEDINGS{Correa2009-hi,
  title     = "A framework for uncertainty-aware visual analytics",
  booktitle = "2009 {IEEE} Symposium on Visual Analytics Science and Technology",
  author    = "Correa, C D and Chan, Y H and Ma, K L",
  abstract  = "Visual analytics has become an important tool for gaining
               insight on large and complex collections of data. Numerous
               statistical tools and data transformations, such as projections,
               binning and clustering, have been coupled with visualization to
               help analysts understand data better and faster. However, data
               is inherently uncertain, due to error, noise or unreliable
               sources. When making decisions based on uncertain data, it is
               important to quantify and present to the analyst both the
               aggregated uncertainty of the results and the impact of the
               sources of that uncertainty. In this paper, we present a new
               framework to support uncertainty in the visual analytics
               process, through statistic methods such as uncertainty modeling,
               propagation and aggregation. We show that data transformations,
               such as regression, principal component analysis and k-means
               clustering, can be adapted to account for uncertainty. This
               framework leads to better visualizations that improve the
               decision-making process and help analysts gain insight on the
               analytic process itself.",
  publisher = "ieeexplore.ieee.org",
  pages     = "51--58",
  month     =  oct,
  year      =  2009,
  keywords  = "data visualisation;decision making;statistical analysis;data
               collections;data transformations;decision making;numerous
               statistical tools;uncertainty-aware visual analytics;Blogs;Data
               analysis;Data visualization;Decision making;Humans;Principal
               component analysis;Sampling methods;Statistical
               analysis;Uncertainty;Visual analytics;Data Transformations;Model
               Fitting;Principal Component Analysis;Uncertainty;Supplemental
               Assurance;vis\_dr"
}

@ARTICLE{Wu2012-qi,
  title    = "Visualizing Flow of Uncertainty through Analytical Processes",
  author   = "Wu, Yingcai and Yuan, Guo-Xun and Ma, Kwan-Liu",
  abstract = "Uncertainty can arise in any stage of a visual analytics process,
              especially in data-intensive applications with a sequence of data
              transformations. Additionally, throughout the process of
              multidimensional, multivariate data analysis, uncertainty due to
              data transformation and integration may split, merge, increase,
              or decrease. This dynamic characteristic along with other
              features of uncertainty pose a great challenge to effective
              uncertainty-aware visualization. This paper presents a new
              framework for modeling uncertainty and characterizing the
              evolution of the uncertainty information through analytical
              processes. Based on the framework, we have designed a visual
              metaphor called uncertainty flow to visually and intuitively
              summarize how uncertainty information propagates over the whole
              analysis pipeline. Our system allows analysts to interact with
              and analyze the uncertainty information at different levels of
              detail. Three experiments were conducted to demonstrate the
              effectiveness and intuitiveness of our design.",
  journal  = "IEEE Trans. Vis. Comput. Graph.",
  volume   =  18,
  number   =  12,
  pages    = "2526--2535",
  month    =  dec,
  year     =  2012,
  keywords = "Supplemental Assurance;vis\_dr",
  language = "en"
}

@ARTICLE{Sacha2017-hf,
  title     = "Visual Interaction with Dimensionality Reduction: A Structured
               Literature Analysis",
  author    = "Sacha, Dominik and Zhang, Leishi and Sedlmair, Michael and Lee,
               John A and Peltonen, Jaakko and Weiskopf, Daniel and North,
               Stephen C and Keim, Daniel A",
  abstract  = "Dimensionality Reduction (DR) is a core building block in
               visualizing multidimensional data. For DR techniques to be
               useful in exploratory data analysis, they need to be adapted to
               human needs and domain-specific problems, ideally,
               interactively, and on-the-fly. Many visual analytics systems
               have already demonstrated the benefits of tightly integrating DR
               with interactive visualizations. Nevertheless, a general,
               structured understanding of this integration is missing. To
               address this, we systematically studied the visual analytics and
               visualization literature to investigate how analysts interact
               with automatic DR techniques. The results reveal seven common
               interaction scenarios that are amenable to interactive control
               such as specifying algorithmic constraints, selecting relevant
               features, or choosing among several DR algorithms. We
               investigate specific implementations of visual analysis systems
               integrating DR, and analyze ways that other machine learning
               methods have been combined with DR. Summarizing the results in a
               ``human in the loop'' process model provides a general lens for
               the evaluation of visual interactive DR systems. We apply the
               proposed model to study and classify several systems previously
               described in the literature, and to derive future research
               opportunities.",
  journal   = "IEEE Trans. Vis. Comput. Graph.",
  publisher = "ieeexplore.ieee.org",
  volume    =  23,
  number    =  1,
  pages     = "241--250",
  month     =  jan,
  year      =  2017,
  keywords  = "Supplemental Assurance;vis\_dr",
  language  = "en"
}

@ARTICLE{Chen2018-xq,
  title         = "Confidence Scoring Using Whitebox Meta-models with Linear
                   Classifier Probes",
  author        = "Chen, Tongfei and Navr{\'a}til, Ji{\v r}{\'\i} and Iyengar,
                   Vijay and Shanmugam, Karthikeyan",
  abstract      = "We propose a confidence scoring mechanism for multi-layer
                   neural networks based on a paradigm of a base model and a
                   meta-model. The confidence score is learned by the
                   meta-model using features derived from the base model -- a
                   deep multi-layer neural network -- considered a whitebox. As
                   features, we investigate linear classifier probes inserted
                   between the various layers of the base model and trained
                   using each layer's intermediate activations. Experiments
                   show that this approach outperforms various baselines in a
                   filtering task, i.e., task of rejecting samples with low
                   confidence. Experimental results are presented using
                   CIFAR-10 and CIFAR-100 dataset with and without added noise
                   exploring various aspects of the method.",
  month         =  may,
  year          =  2018,
  keywords      = "Supplemental Assurance;Quantify Uncertainty;meh..",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1805.05396"
}

@ARTICLE{Choi2017-th,
  title         = "{Uncertainty-Aware} Learning from Demonstration using
                   Mixture Density Networks with {Sampling-Free} Variance
                   Modeling",
  author        = "Choi, Sungjoon and Lee, Kyungjae and Lim, Sungbin and Oh,
                   Songhwai",
  abstract      = "In this paper, we propose an uncertainty-aware learning from
                   demonstration method by presenting a novel uncertainty
                   estimation method utilizing a mixture density network
                   appropriate for modeling complex and noisy human behaviors.
                   The proposed uncertainty acquisition can be done with a
                   single forward path without Monte Carlo sampling and is
                   suitable for real-time robotics applications. The properties
                   of the proposed uncertainty measure are analyzed through
                   three different synthetic examples, absence of data, heavy
                   measurement noise, and composition of functions scenarios.
                   We show that each case can be distinguished using the
                   proposed uncertainty measure and presented an
                   uncertainty-aware learn- ing from demonstration method of an
                   autonomous driving using this property. The proposed
                   uncertainty-aware learning from demonstration method
                   outperforms other compared methods in terms of safety using
                   a complex real-world driving dataset.",
  month         =  sep,
  year          =  2017,
  keywords      = "Supplemental Assurance;Quantify Uncertainty",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1709.02249"
}

@ARTICLE{Kahn2017-vy,
  title         = "{Uncertainty-Aware} Reinforcement Learning for Collision
                   Avoidance",
  author        = "Kahn, Gregory and Villaflor, Adam and Pong, Vitchyr and
                   Abbeel, Pieter and Levine, Sergey",
  abstract      = "Reinforcement learning can enable complex, adaptive behavior
                   to be learned automatically for autonomous robotic
                   platforms. However, practical deployment of reinforcement
                   learning methods must contend with the fact that the
                   training process itself can be unsafe for the robot. In this
                   paper, we consider the specific case of a mobile robot
                   learning to navigate an a priori unknown environment while
                   avoiding collisions. In order to learn collision avoidance,
                   the robot must experience collisions at training time.
                   However, high-speed collisions, even at training time, could
                   damage the robot. A successful learning method must
                   therefore proceed cautiously, experiencing only low-speed
                   collisions until it gains confidence. To this end, we
                   present an uncertainty-aware model-based learning algorithm
                   that estimates the probability of collision together with a
                   statistical estimate of uncertainty. By formulating an
                   uncertainty-dependent cost function, we show that the
                   algorithm naturally chooses to proceed cautiously in
                   unfamiliar environments, and increases the velocity of the
                   robot in settings where it has high confidence. Our
                   predictive model is based on bootstrapped neural networks
                   using dropout, allowing it to process raw sensory inputs
                   from high-bandwidth sensors such as cameras. Our
                   experimental evaluation demonstrates that our method
                   effectively minimizes dangerous collisions at training time
                   in an obstacle avoidance task for a simulated and real-world
                   quadrotor, and a real-world RC car. Videos of the
                   experiments can be found at
                   https://sites.google.com/site/probcoll.",
  month         =  feb,
  year          =  2017,
  keywords      = "Supplemental Assurance;Quantify Uncertainty;Integral
                   Assurance;Value Alignment;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1702.01182"
}

@ARTICLE{Liu2017-xw,
  title     = "Towards better analysis of machine learning models: A visual
               analytics perspective",
  author    = "Liu, Shixia and Wang, Xiting and Liu, Mengchen and Zhu, Jun",
  abstract  = "Interactive model analysis, the process of understanding,
               diagnosing, and refining a machine learning model with the help
               of interactive visualization, is very important for users to
               efficiently solve real-world artificial intelligence and data
               mining problems. Dramatic advances in big data analytics have
               led to a wide variety of interactive model analysis tasks. In
               this paper, we present a comprehensive analysis and
               interpretation of this rapidly developing area. Specifically, we
               classify the relevant work into three categories: understanding,
               diagnosis, and refinement. Each category is exemplified by
               recent influential work. Possible future research opportunities
               are also explored and discussed.",
  journal   = "Visual Informatics",
  publisher = "Elsevier",
  volume    =  1,
  number    =  1,
  pages     = "48--56",
  month     =  mar,
  year      =  2017,
  keywords  = "Interactive model analysis; Interactive visualization; Machine
               learning; Understanding; Diagnosis; Refinement;Supplemental
               Assurance;vis\_dr"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kendall2017-ry,
  title     = "What Uncertainties Do We Need in Bayesian Deep Learning for
               Computer Vision?",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Kendall, Alex and Gal, Yarin",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  abstract  = "… Quantifying uncertainty in computer vision applications can be
               largely divided into regression set- tings such as depth
               regression, and classification settings such as semantic … In
               particular, we wish to quantify the performance of these
               uncertainty measurements and …",
  pages     = "5574--5584",
  year      =  2017,
  keywords  = "Supplemental Assurance;Quantify Uncertainty"
}

@INCOLLECTION{Yu2018-qw,
  title     = "Do {I} Trust a Machine? Differences in User Trust Based on
               System Performance",
  booktitle = "Human and Machine Learning: Visible, Explainable, Trustworthy
               and Transparent",
  author    = "Yu, Kun and Berkovsky, Shlomo and Conway, Dan and Taib, Ronnie
               and Zhou, Jianlong and Chen, Fang",
  editor    = "Zhou, Jianlong and Chen, Fang",
  abstract  = "Trust plays an important role in various user-facing systems and
               applications. It is particularly important in the context of
               decision support systems, where the system's output serves as
               one of the inputs for the users' decision making processes. In
               this chapter, we study the dynamics of explicit and implicit
               user trust in a simulated automated quality monitoring system,
               as a function of the system accuracy. We establish that users
               correctly perceive the accuracy of the system and adjust their
               trust accordingly. The results also show notable differences
               between two groups of users and indicate a possible threshold in
               the acceptance of the system. This important learning can be
               leveraged by designers of practical systems for sustaining the
               desired level of user trust.",
  publisher = "Springer International Publishing",
  pages     = "245--264",
  year      =  2018,
  address   = "Cham",
  keywords  = "Supplemental Assurance;User Observation;human\_study;Assurances"
}

@INCOLLECTION{Abdollahi2018-uw,
  title     = "Transparency in Fair Machine Learning: the Case of Explainable
               Recommender Systems",
  booktitle = "Human and Machine Learning: Visible, Explainable, Trustworthy
               and Transparent",
  author    = "Abdollahi, Behnoush and Nasraoui, Olfa",
  editor    = "Zhou, Jianlong and Chen, Fang",
  abstract  = "Machine Learning (ML) models are increasingly being used in many
               sectors, ranging from health and education to justice and
               criminal investigation. Therefore, building a fair and
               transparent model which conveys the reasoning behind its
               predictions is of great importance. This chapter discusses the
               role of explanation mechanisms in building fair machine learning
               models and explainable ML technique. We focus on the special
               case of recommender systems because they are a prominent example
               of a ML model that interacts directly with humans. This is in
               contrast to many other traditional decision making systems that
               interact with experts (e.g. in the health-care domain). In
               addition, we discuss the main sources of bias that can lead to
               biased and unfair models. We then review the taxonomy of
               explanation styles for recommender systems and review models
               that can provide explanations for their recommendations. We
               conclude by reviewing evaluation metrics for assessing the power
               ofExplainability explainability in recommender systems.",
  publisher = "Springer International Publishing",
  pages     = "21--35",
  year      =  2018,
  address   = "Cham",
  keywords  = "Supplemental Assurance;Reduce Complexity;explain;Assurances"
}

@INCOLLECTION{Khoa2018-gh,
  title     = "Structural Health Monitoring Using Machine Learning Techniques
               and Domain Knowledge Based Features",
  booktitle = "Human and Machine Learning: Visible, Explainable, Trustworthy
               and Transparent",
  author    = "Khoa, Nguyen Lu Dang and Makki Alamdari, Mehrisadat and
               Rakotoarivelo, Thierry and Anaissi, Ali and Wang, Yang",
  editor    = "Zhou, Jianlong and Chen, Fang",
  abstract  = "Structural Health Monitoring (SHM) is a condition-based
               maintenance technology using sensing systems. In SHM, the use of
               domain knowledge is essential: it motivates the use of machine
               learning approaches; it can be used to extract damage sensitive
               features and interpret the results by machine learning. This
               work focuses on two SHM problems: damage identification and
               substructure clustering. Our solutions to address them are based
               on machine learning techniques and robust feature extraction
               using domain knowledge. In the first problem, damage sensitive
               features were extracted using a frequency domain decomposition,
               followed by a robust one-class support vector machine for damage
               detection. In the second problem, a novel clustering technique
               and spectral moment feature were utilised for substructure
               grouping and anomaly detection. These methods were evaluated
               using data from lab-based structures and data collected from the
               Sydney Harbour Bridge. We obtained high damage detection
               accuracies and were able to assess damage severity. Furthermore,
               the clustering technique was able to group substructures of
               similar behaviour and detect spatial anomalies.",
  publisher = "Springer International Publishing",
  pages     = "409--435",
  year      =  2018,
  address   = "Cham",
  keywords  = "Integral Assurance;Value Alignment;TGDS;Assurances"
}

@INCOLLECTION{Zhang2018-no,
  title     = "Water Pipe Failure Prediction: A Machine Learning Approach
               Enhanced By Domain Knowledge",
  booktitle = "Human and Machine Learning: Visible, Explainable, Trustworthy
               and Transparent",
  author    = "Zhang, Bang and Guo, Ting and Zhang, Lelin and Lin, Peng and
               Wang, Yang and Zhou, Jianlong and Chen, Fang",
  editor    = "Zhou, Jianlong and Chen, Fang",
  abstract  = "Drinking water pipe and waste water pipe networks are valuable
               urban infrastructure assets that are responsible for reliable
               water resource distributions and waste water collection.
               However, due to fast growing demand and aging assets, water
               utilities find it increasingly difficult to efficiently maintain
               their pipe networks. Pipe failures - drinking water pipe breaks
               and waste water pipe blockages - can cause significant economic
               and social costs, and hence have become the primary challenge to
               water utilities. Identifying key influential factors, e.g.,
               pipes' physical attributes, environmental features, is critical
               for understanding pipe failure behaviours. The domain knowledge
               plays a significant role in this aspect. In this work, we
               propose a Bayesian nonparametric machine learning model with the
               support of domain knowledge for pipe failure prediction. It can
               forecast future high-risk pipes for physical condition
               assessment, thereby proactively preventing disastrous failures.
               Moreover, compared with traditional machine learning approaches,
               the proposed model considers domain expert knowledge and
               experience, which helps avoid the limit of traditional machine
               learning approaches - learning only from what it sees - and
               improves prediction performance.",
  publisher = "Springer International Publishing",
  pages     = "363--383",
  year      =  2018,
  address   = "Cham",
  keywords  = "Integral Assurance;Value Alignment;TGDS;Assurances"
}

@INPROCEEDINGS{Balbach2009-jw,
  title     = "Recent Developments in Algorithmic Teaching",
  booktitle = "Language and Automata Theory and Applications",
  author    = "Balbach, Frank J and Zeugmann, Thomas",
  abstract  = "The present paper surveys recent developments in algorithmic
               teaching. First, the traditional teaching dimension model is
               recalled.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1--18",
  year      =  2009,
  keywords  = "assurances;Assurances"
}

@ARTICLE{Huang2017-lk,
  title         = "Enabling Robots to Communicate their Objectives",
  author        = "Huang, Sandy H and Held, David and Abbeel, Pieter and
                   Dragan, Anca D",
  abstract      = "Our ultimate goal is to efficiently enable end-users to
                   correctly anticipate a robot's behavior in novel situations.
                   This behavior is often a direct result of the robot's
                   underlying objective function. Our insight is that end-users
                   need to have an accurate mental model of this objective
                   function in order to understand and predict what the robot
                   will do. While people naturally develop such a mental model
                   over time through observing the robot act, this
                   familiarization process may be lengthy. Our approach reduces
                   this time by having the robot model how people infer
                   objectives from observed behavior, and then selecting those
                   behaviors that are maximally informative. The problem of
                   computing a posterior over objectives from observed behavior
                   is known as Inverse Reinforcement Learning (IRL), and has
                   been applied to robots learning human objectives. We
                   consider the problem where the roles of human and robot are
                   swapped. Our main contribution is to recognize that unlike
                   robots, humans will not be \textbackslashemph\{exact\} in
                   their IRL inference. We thus introduce two factors to define
                   candidate approximate-inference models for human learning in
                   this setting, and analyze them in a user study in the
                   autonomous driving domain. We show that certain
                   approximate-inference models lead to the robot generating
                   example behaviors that better enable users to anticipate
                   what the robot will do in test situations. Our results also
                   suggest, however, that additional research is needed in
                   modeling how humans extrapolate from examples of robot
                   behavior.",
  month         =  feb,
  year          =  2017,
  keywords      = "Supplemental Assurance;Reduce Complexity",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1702.03465"
}

@ARTICLE{Dragan2014-gu,
  title     = "Integrating human observer inferences into robot motion planning",
  author    = "Dragan, Anca and Srinivasa, Siddhartha",
  abstract  = "Our goal is to enable robots to produce motion that is suitable
               for human--robot collaboration and co-existence. Most motion in
               robotics is purely functional, ideal when the robot is
               performing a task in isolation. In collaboration, however, the
               robot's motion has an observer, watching and interpreting the
               motion. In this work, we move beyond functional motion, and
               introduce the notion of an observer into motion planning, so
               that robots can generate motion that is mindful of how it will
               be interpreted by a human collaborator. We formalize
               predictability and legibility as properties of motion that
               naturally arise from the inferences in opposing directions that
               the observer makes, drawing on action interpretation theory in
               psychology. We propose models for these inferences based on the
               principle of rational action, and derive constrained functional
               trajectory optimization techniques for planning motion that is
               predictable or legible. Finally, we present experiments that
               test our work on novice users, and discuss the remaining
               challenges in enabling robots to generate such motion online in
               complex situations.",
  journal   = "Auton. Robots",
  publisher = "Springer US",
  volume    =  37,
  number    =  4,
  pages     = "351--368",
  month     =  dec,
  year      =  2014,
  keywords  = "NotRead",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Tellex2014-uc,
  title     = "Asking for Help Using Inverse Semantics",
  author    = "Tellex, Stefanie and Knepper, Ross and Li, Adrian and Rus,
               Daniela and Roy, Nicholas",
  abstract  = "Robots inevitably fail, often without the ability to recover
               autonomously. We demonstrate an approach for enabling a robot to
               recover from failures by communicating its need for specific
               help to a human partner using natural language. Our approach
               automatically detects failures, then generates targeted
               spoken-language requests for help such as ```Please give me the
               white table leg that is on the black table.'' Once the human
               partner has repaired the failure condition, the system resumes
               full autonomy. We present a novel inverse semantics algorithm
               for generating effective help requests. In contrast to forward
               semantic models that interpret natural language in terms of
               robot actions and perception, our inverse semantics algorithm
               generates requests by emulating the human's ability to interpret
               a request using the Generalized Grounding Graph (G³) framework.
               To assess the effectiveness of our approach, we present a
               corpus-based online evaluation, as well as an end-to-end user
               study, demonstrating that our approach increases the
               effectiveness of human interventions compared to static requests
               for help.",
  publisher = "Robotics: Science and Systems Foundation",
  month     =  jul,
  year      =  2014,
  keywords  = "Article;Integral Assurance;User Interaction"
}

@INPROCEEDINGS{Kaupp2008-yr,
  title     = "Measuring human-robot team effectiveness to determine an
               appropriate autonomy level",
  booktitle = "2008 {IEEE} International Conference on Robotics and Automation",
  author    = "Kaupp, T and Makarenko, A",
  abstract  = "This paper proposes a methodology to measure the effectiveness
               of a human-robot team as part of an adjustable autonomy system.
               The effectiveness measure is aimed at determining an appropriate
               autonomy level prior to the system's deployment. Two competing
               goals need to be traded off: maximising robot performance while
               minimising the amount of human input. The relative importance of
               the two goals depend on the mission priorities and constraints
               which are taken into account. The proposed methodology is
               applied to a human-robot communication system developed for
               task- oriented information exchange. The robot uses a decision-
               theoretic framework to act autonomously and to decide when to
               request input from human operators. The latter is achieved by
               computing the value-of-information an operator is able to
               provide which is compared to the cost of obtaining the
               information. For our system, the cost parameter represents the
               autonomy level to be determined. We demonstrate how an
               appropriate autonomy level can be found experimentally using a
               navigation task. In our experiment, the robot navigates through
               a set of simulated worlds with human input being generated by a
               software component. The results are used to find appropriate
               autonomy levels for three example missions and a subsequent user
               study.",
  publisher = "ieeexplore.ieee.org",
  pages     = "2146--2151",
  month     =  may,
  year      =  2008,
  keywords  = "man-machine systems;robots;decision-theoretic
               framework;human-robot communication system;software
               component;task-oriented information
               exchange;Anthropometry;Cognitive robotics;Communication
               systems;Costs;Human robot
               interaction;Measurement;Navigation;Robot sensing
               systems;Robotics and automation;Safety;Integral Assurance;User
               Interaction;Assurances"
}

@INPROCEEDINGS{Kaupp2005-te,
  title     = "Human sensor model for range observations",
  booktitle = "{IJCAI} Workshop Reasoning with Uncertainty in Robotics",
  author    = "Kaupp, Tobias and Makarenko, Alexei and Ramos, Fabio and
               Durrant-Whyte, Hugh",
  year      =  2005
}

@INPROCEEDINGS{Yanco2004-fh,
  title     = "Classifying human-robot interaction: an updated taxonomy",
  booktitle = "2004 {IEEE} International Conference on Systems, Man and
               Cybernetics ({IEEE} Cat. {No.04CH37583})",
  author    = "Yanco, H A and Drury, J",
  abstract  = "This paper extends taxonomy of human-robot interaction (HRI)
               introduced in 2002 to include additional categories as well as
               updates to the categories from the original taxonomy. New
               classifications include measures of the social nature of the
               task (human interaction roles and human-robot physical
               proximity), task type, and robot morphology.",
  publisher = "ieeexplore.ieee.org",
  volume    =  3,
  pages     = "2841--2846 vol.3",
  month     =  oct,
  year      =  2004,
  keywords  = "man-machine systems;robots;user interfaces;human interaction
               roles;human-robot interaction;human-robot physical
               proximity;robot morphology;updated taxonomy;Application
               software;Collaboration;Collaborative work;Computer
               science;Control systems;Human robot interaction;Robot sensing
               systems;Sensor systems;Taxonomy;Teleconferencing;Integral
               Assurance;User Interaction;Assurances"
}

@INPROCEEDINGS{Sweet2016-dw,
  title     = "Structured synthesis and compression of semantic human sensor
               models for Bayesian estimation",
  booktitle = "2016 American Control Conference ({ACC})",
  author    = "Sweet, N and Ahmed, N",
  abstract  = "We consider the problem of fusing human-generated semantic `soft
               sensor' data with conventional `hard sensor' data to augment
               Bayesian state estimators. This requires modeling semantic soft
               data via generalized continuous-to-discrete softmax likelihood
               functions, which can theoretically model semantic descriptions
               of any dynamic state space. This paper addresses two important
               related issues for deploying these models in practical
               applications. First, a general solution to the data-free
               likelihood synthesis problem is provided. This allows for easy
               embedding of contextual constraints and other relevant a priori
               information within generalized softmax models, without resorting
               to expensive non-convex optimization procedures for parameter
               estimation with sparse data. This result is then used to derive
               strategies for combining multiple semantic human observation
               models into `compressed' likelihood functions for fast batch
               data fusion. The proposed methods are demonstrated on a
               human-robot target search application.",
  pages     = "5479--5485",
  month     =  jul,
  year      =  2016,
  keywords  = "Bayes methods;human-robot interaction;parameter
               estimation;Bayesian estimation;Bayesian state
               estimators;compressed likelihood functions;contextual
               constraints;data-free likelihood synthesis problem;dynamic state
               space;expensive nonconvex optimization procedures;fast batch
               data fusion;generalized continuous-to-discrete softmax
               likelihood functions;generalized softmax models;hard sensor
               data;human-generated semantic soft sensor data;human-robot
               target search application;multiple semantic human observation
               models;parameter estimation;semantic descriptions;semantic human
               sensor models;semantic soft data;sparse data;structured
               synthesis;Adaptation models;Bayes methods;Data
               models;Probabilistic logic;Robot sensing
               systems;Semantics;Integral Assurance;User Interaction"
}

@INPROCEEDINGS{Admoni2016-db,
  title     = "Robot Nonverbal Behavior Improves Task Performance In Difficult
               Collaborations",
  booktitle = "The Eleventh {ACM/IEEE} International Conference on Human Robot
               Interaction",
  author    = "Admoni, Henny and Weng, Thomas and Hayes, Bradley and
               Scassellati, Brian",
  publisher = "IEEE Press",
  pages     = "51--58",
  series    = "HRI '16",
  year      =  2016,
  address   = "Piscataway, NJ, USA",
  keywords  = "collaborative hri, eye gaze, gesture, human-robot interaction,
               nonverbal behavior;Integral Assurance;Human-like
               Behavior;human\_study;Assurances"
}

@INPROCEEDINGS{Hayes2017-nt,
  title     = "Improving Robot Controller Transparency Through Autonomous
               Policy Explanation",
  booktitle = "Proceedings of the 2017 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "Hayes, Bradley and Shah, Julie A",
  publisher = "ACM",
  pages     = "303--312",
  series    = "HRI '17",
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "human-robot collaboration, human-robot interaction, human-robot
               teaming, interpretable machine
               learning;explain;transparency;assurance\_competence;Supplemental
               Assurance;Reduce Complexity;Assurances"
}

@ARTICLE{Fisac2018-at,
  title         = "Probabilistically Safe Robot Planning with
                   {Confidence-Based} Human Predictions",
  author        = "Fisac, Jaime F and Bajcsy, Andrea and Herbert, Sylvia L and
                   Fridovich-Keil, David and Wang, Steven and Tomlin, Claire J
                   and Dragan, Anca D",
  abstract      = "In order to safely operate around humans, robots can employ
                   predictive models of human motion. Unfortunately, these
                   models cannot capture the full complexity of human behavior
                   and necessarily introduce simplifying assumptions. As a
                   result, predictions may degrade whenever the observed human
                   behavior departs from the assumed structure, which can have
                   negative implications for safety. In this paper, we observe
                   that how ``rational'' human actions appear under a
                   particular model can be viewed as an indicator of that
                   model's ability to describe the human's current motion. By
                   reasoning about this model confidence in a real-time
                   Bayesian framework, we show that the robot can very quickly
                   modulate its predictions to become more uncertain when the
                   model performs poorly. Building on recent work in
                   provably-safe trajectory planning, we leverage these
                   confidence-aware human motion predictions to generate
                   assured autonomous robot motion. Our new analysis combines
                   worst-case tracking error guarantees for the physical robot
                   with probabilistic time-varying human predictions, yielding
                   a quantitative, probabilistic safety certificate. We
                   demonstrate our approach with a quadcopter navigating around
                   a human.",
  month         =  may,
  year          =  2018,
  keywords      = "Safety\_AI;Uncertainty;Integral Assurance;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1806.00109"
}

@BOOK{Kochenderfer2015-uu,
  title     = "Decision Making Under Uncertainty: Theory and Application",
  author    = "Kochenderfer, Mykel J",
  abstract  = "An introduction to decision making under uncertainty from a
               computational perspective, covering both theory and applications
               ranging from speech recognition to airborne collision
               avoidance.Many important problems involve decision making under
               uncertainty---that is, choosing actions based on often imperfect
               observations, with unknown outcomes. Designers of automated
               decision support systems must take into account the various
               sources of uncertainty while balancing the multiple objectives
               of the system. This book provides an introduction to the
               challenges of decision making under uncertainty from a
               computational perspective. It presents both the theory behind
               decision making models and algorithms and a collection of
               example applications that range from speech recognition to
               aircraft collision avoidance.Focusing on two methods for
               designing decision agents, planning and reinforcement learning,
               the book covers probabilistic models, introducing Bayesian
               networks as a graphical model that captures probabilistic
               relationships between variables; utility theory as a framework
               for understanding optimal decision making under uncertainty;
               Markov decision processes as a method for modeling sequential
               problems; model uncertainty; state uncertainty; and cooperative
               decision making involving multiple interacting agents. A series
               of applications shows how the theoretical concepts can be
               applied to systems for attribute-based person search, speech
               applications, collision avoidance, and unmanned aircraft
               persistent surveillance.Decision Making Under Uncertainty
               unifies research from different communities using consistent
               notation, and is accessible to students and researchers across
               engineering disciplines who have some prior exposure to
               probability theory and calculus. It can be used as a text for
               advanced undergraduate and graduate students in fields including
               computer science, aerospace and electrical engineering, and
               management science. It will also be a valuable professional
               reference for researchers in a variety of disciplines.",
  publisher = "MIT Press",
  month     =  jul,
  year      =  2015,
  keywords  = "Textbook;TextBooks",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Taylor2016-qt,
  title     = "Alignment for advanced machine learning systems",
  author    = "Taylor, Jessica and Yudkowsky, Eliezer and LaVictoire, Patrick
               and Critch, Andrew",
  abstract  = "We survey eight research areas organized around one question: As
               learning systems become increasingly intelligent and autonomous,
               what design principles can best ensure that their behavior is
               aligned with the interests of the operators? We focus on two
               major technical obstacles to AI alignment: the challenge of
               specifying the right kind of objective functions, and the
               challenge of designing AI systems that avoid unintended
               consequences and undesirable behavior even in cases where the
               objective function does not line up …",
  journal   = "Machine Intelligence Research Institute",
  publisher = "pdfs.semanticscholar.org",
  year      =  2016,
  keywords  = "Safety\_AI;AI;assurances;Integral Assurance;Value
               Alignment;Assurances"
}

@ARTICLE{noauthor_undated-qt,
  title    = "[{PDF]Artificial} Intelligence: Real Public Engagement - {RSA}",
  keywords = "trust\_public\_conversation;Assurances"
}

@MISC{Singh2013-jp,
  title        = "How to write survey or review papers and What sections should
                  be.",
  booktitle    = "{ResearchGate}",
  author       = "Singh, Sudhakar",
  abstract     = "Your toughest technical questions will likely get answered
                  within 48 hours on ResearchGate, the professional network for
                  scientists.",
  month        =  jun,
  year         =  2013,
  howpublished = "\url{https://www.researchgate.net/post/How_to_write_survey_or_review_papers_and_What_sections_should_be_mentioned_in_such_papers}",
  note         = "Accessed: 2018-5-17",
  keywords     = "WritingPapers"
}

@ARTICLE{Webster2002-ty,
  title     = "Analyzing the Past to Prepare for the Future: Writing a
               Literature Review",
  author    = "Webster, Jane and Watson, Richard T",
  journal   = "Miss. Q.",
  publisher = "Management Information Systems Research Center, University of
               Minnesota",
  volume    =  26,
  number    =  2,
  pages     = "xiii--xxiii",
  year      =  2002,
  keywords  = "WritingPapers"
}

@ARTICLE{B_Kitchenham2007-vw,
  title    = "Guidelines for performing Systematic Literature Reviews in
              Software Engineering",
  author   = "B. Kitchenham, S Charters",
  abstract = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
              Teregowda):",
  journal  = "citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.471",
  year     =  2007,
  keywords = "WritingPapers"
}

@ARTICLE{Lykourentzou2009-pb,
  title     = "Dropout prediction in e-learning courses through the combination
               of machine learning techniques",
  author    = "Lykourentzou, Ioanna and Giannoukos, Ioannis and Nikolopoulos,
               Vassilis and Mpardis, George and Loumos, Vassili",
  abstract  = "In this paper, a dropout prediction method for e-learning
               courses, based on three popular machine learning techniques and
               detailed student data, is proposed. The machine learning
               techniques used are feed-forward neural networks, support vector
               machines and probabilistic ensemble simplified fuzzy ARTMAP.
               Since a single technique may fail to accurately classify some
               e-learning students, whereas another may succeed, three decision
               schemes, which combine in different ways the results of the
               three machine learning techniques, were also tested. The method
               was examined in terms of overall accuracy, sensitivity and
               precision and its results were found to be significantly better
               than those reported in relevant literature.",
  journal   = "Comput. Educ.",
  publisher = "Elsevier",
  volume    =  53,
  number    =  3,
  pages     = "950--965",
  month     =  nov,
  year      =  2009,
  keywords  = "Distance education and telelearning; E-learning; Machine
               learning; Dropout prediction"
}

@INPROCEEDINGS{Huang2017-zt,
  title     = "Safety Verification of Deep Neural Networks",
  booktitle = "Computer Aided Verification",
  author    = "Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min",
  abstract  = "Deep neural networks have achieved impressive experimental
               results in image classification, but can surprisingly be
               unstable with respect to adversarial perturbations, that is,
               minimal changes to the input image that cause the network to
               misclassify it. With potential applications including perception
               modules and end-to-end controllers for self-driving cars, this
               raises concerns about their safety. We develop a novel automated
               verification framework for feed-forward multi-layer neural
               networks based on Satisfiability Modulo Theory (SMT). We focus
               on safety of image classification decisions with respect to
               image manipulations, such as scratches or changes to camera
               angle or lighting conditions that would result in the same class
               being assigned by a human, and define safety for an individual
               decision in terms of invariance of the classification within a
               small neighbourhood of the original image. We enable exhaustive
               search of the region by employing discretisation, and propagate
               the analysis layer by layer. Our method works directly with the
               network code and, in contrast to existing methods, can guarantee
               that adversarial examples, if they exist, are found for the
               given region and family of manipulations. If found, adversarial
               examples can be shown to human testers and/or used to fine-tune
               the network. We implement the techniques using Z3 and evaluate
               them on state-of-the-art networks, including regularised and
               deep learning networks. We also compare against existing
               techniques to search for adversarial examples and estimate
               network robustness.",
  publisher = "Springer International Publishing",
  pages     = "3--29",
  year      =  2017,
  keywords  = "Supplemental Assurance;Value Alignment"
}

@ARTICLE{Israelsen2018-es,
  title     = "Adaptive {Simulation-Based} Training of
               {Artificial-Intelligence} Decision Makers Using Bayesian
               Optimization",
  author    = "Israelsen, Brett and Ahmed, Nisar and Center, Kenneth and Green,
               Roderick and Bennett, Winston",
  abstract  = "This work studies how an artifical-intelligence-controlled
               dogfighting agent with tunable decision-making parameters can
               learn to optimize performance against an intelligent adversary,
               as measured by a stochastic objective function evaluated on
               simulated combat engagements. Gaussian process Bayesian
               optimization techniques are developed to automatically learn
               global Gaussian process surrogate models, which provide
               statistical performance predictions in both explored and
               unexplored areas of the parameter space. This allows a learning
               engine to sample full-combat simulations at parameter values
               that are most likely to optimize performance and provide highly
               informative data points for improving future predictions.
               However, standard Gaussian process Bayesian optimization methods
               do not provide a reliable surrogate model for the highly
               volatile objective functions found in aerial combat and thus do
               not reliably identify global maxima. These issues are addressed
               by novel repeat sampling and hybrid repeat/multipoint sampling
               techniques. Simulation studies show that hybrid
               repeat/multipoint sampling improves the accuracy of Gaussian
               process surrogate models, allowing artificial-intelligence
               decision makers to more accurately predict performance and
               efficiently tune parameters.",
  journal   = "Journal of Aerospace Information Systems",
  publisher = "American Institute of Aeronautics and Astronautics",
  volume    =  15,
  number    =  2,
  pages     = "38--56",
  month     =  jan,
  year      =  2018,
  keywords  = "My Papers;myPapers"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Buchanan2017-dz,
  title     = "Machine Learning for Policymakers",
  author    = "Buchanan, Ben and Miller, Taylor",
  abstract  = "Executive Summary Machine learning matters. If nothing else, the
               drumbeat of headlines in recent years offers proof of this. In
               fields as diverse as healthcare, transportation, policing, and
               warfighting, machine learning algorithms have already had a
               significant impact. They …",
  publisher = "belfercenter.org",
  month     =  jun,
  year      =  2017
}

@ARTICLE{Miller2006-uc,
  title    = "Why commutability matters",
  author   = "Miller, W Greg and Myers, Gary L and Rej, Robert",
  journal  = "Clin. Chem.",
  volume   =  52,
  number   =  4,
  pages    = "553--554",
  month    =  apr,
  year     =  2006,
  language = "en"
}

@ARTICLE{De_Visser2018-kd,
  title    = "From ``automation'' to ``autonomy'': The importance of trust
              repair in human-machine interaction",
  author   = "de Visser, Ewart J and Pak, Richard and Shaw, Tyler H",
  abstract = "Modern interactions with technology are increasingly moving away
              from simple human use of computers as tools to the establishment
              of human relationships with autonomous entities that carry out
              actions on our behalf. In a recent commentary, Peter Hancock
              (Hancock, 2017) issued a stark warning to the field of human
              factors that attention must be focused on the appropriate design
              of a new class of technology: highly autonomous systems. In this
              article, we heed the warning and propose a human-centered
              approach directly aimed at ensuring that future human-autonomy
              interactions remain focused on the user's needs and preferences.
              By adapting literature from industrial psychology, we propose a
              framework to infuse a unique human-like ability, building and
              actively repairing trust, into autonomous systems. We conclude by
              proposing a model to guide the design of future autonomy and a
              research agenda to explore current challenges in repairing trust
              between humans and autonomous systems. practitioner summary This
              paper is a call to practitioners to re-cast our connection to
              technology as akin to a relationship between two humans rather
              than between a human and their tools. To that end, designing
              autonomy with trust repair abilities will ensure future
              technology maintains and repairs relationships with their human
              partners.",
  journal  = "Ergonomics",
  pages    = "1--33",
  month    =  mar,
  year     =  2018,
  keywords = "Trust repair; automation; autonomy; human-machine teaming;
              humanness;Integral Assurance;Human-like Behavior",
  language = "en"
}

@ARTICLE{Maaten2008-jy,
  title    = "Visualizing Data using {t-SNE}",
  author   = "Maaten, Laurens van der and Hinton, Geoffrey",
  journal  = "J. Mach. Learn. Res.",
  volume   =  9,
  number   = "Nov",
  pages    = "2579--2605",
  year     =  2008,
  keywords = "acm\_review\_additions"
}

@ARTICLE{Ware2001-of,
  title    = "Interactive machine learning: letting users build classifiers",
  author   = "Ware, Malcolm and Frank, Eibe and Holmes, Geoffrey and Hall, Mark
              and Witten, Ian H",
  abstract = "According to standard procedure, building a classifier using
              machine learning is a fully automated process that follows the
              preparation of training data by a domain expert. In contrast,
              interactive machine learning engages users in actually generating
              the classifier themselves. This offers a natural way of
              integrating background knowledge into the modelling stage---as
              long as interactive tools can be designed that support efficient
              and effective communication. This paper shows that appropriate
              techniques can empower users to create models that compete with
              classifiers built by state-of-the-art learning algorithms. It
              demonstrates that users---even users who are not domain
              experts---can often construct good classifiers, without any help
              from a learning algorithm, using a simple two-dimensional visual
              interface. Experiments on real data demonstrate that, not
              surprisingly, success hinges on the domain: if a few attributes
              can support good predictions, users generate accurate
              classifiers, whereas domains with many high-order attribute
              interactions favour standard machine learning techniques. We also
              present an artificial example where domain knowledge allows an
              ``expert user'' to create a much more accurate model than
              automatic learning algorithms. These results indicate that our
              system has the potential to produce highly accurate classifiers
              in the hands of a domain expert who has a strong interest in the
              domain and therefore some insights into how to partition the
              data. Moreover, small expert-defined models offer the additional
              advantage that they will generally be more intelligible than
              those generated by automatic techniques.",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  55,
  number   =  3,
  pages    = "281--292",
  month    =  sep,
  year     =  2001,
  keywords = "interactive learning; classification; decision trees;
              visualization.;acm\_review\_additions"
}

@ARTICLE{Carter2017-ci,
  title    = "Using Artificial Intelligence to Augment Human Intelligence",
  author   = "Carter, Shan and Nielsen, Michael",
  journal  = "Distill",
  volume   =  2,
  number   =  12,
  month    =  dec,
  year     =  2017,
  keywords = "acm\_review\_additions"
}

@ARTICLE{Yosinski2015-me,
  title         = "Understanding Neural Networks Through Deep Visualization",
  author        = "Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs,
                   Thomas and Lipson, Hod",
  abstract      = "Recent years have produced great advances in training large,
                   deep neural networks (DNNs), including notable successes in
                   training convolutional neural networks (convnets) to
                   recognize natural images. However, our understanding of how
                   these models work, especially what computations they perform
                   at intermediate layers, has lagged behind. Progress in the
                   field will be further accelerated by the development of
                   better tools for visualizing and interpreting neural nets.
                   We introduce two such tools here. The first is a tool that
                   visualizes the activations produced on each layer of a
                   trained convnet as it processes an image or video (e.g. a
                   live webcam stream). We have found that looking at live
                   activations that change in response to user input helps
                   build valuable intuitions about how convnets work. The
                   second tool enables visualizing features at each layer of a
                   DNN via regularized optimization in image space. Because
                   previous versions of this idea produced less recognizable
                   images, here we introduce several new regularization methods
                   that combine to produce qualitatively clearer, more
                   interpretable visualizations. Both tools are open source and
                   work on a pre-trained convnet with minimal setup.",
  month         =  jun,
  year          =  2015,
  keywords      = "acm\_review\_additions",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1506.06579"
}

@ARTICLE{Ososky2013-ua,
  title     = "Building Appropriate Trust in {Human-Robot} Teams",
  author    = "Ososky, S and Schuster, D and Phillips, E and Jentsch, F G",
  abstract  = "Abstract Future robotic systems are expected to transition from
               tools to teammates, characterized by increasingly autonomous,
               intelligent robots interacting with humans in a more
               naturalistic manner, approaching a relationship more akin to
               human--human teamwork. Given the impact of trust observed in
               other systems, trust in the robot team member will likely be
               critical to effective and safe performance. Our thesis for this
               paper is that trust in a robot team member must be appropriately
               calibrated rather than simply",
  journal   = "AAAI Spring Symposium",
  publisher = "aaai.org",
  year      =  2013,
  keywords  = "very\_similar\_to\_mine;trust\_academic\_conversation;trust\_in\_technology;acm\_review\_additions;Assurances"
}

@ARTICLE{Huang_undated-ab,
  title    = "Establishing Appropriate Trust via Critical States",
  author   = "Huang, Sandy H and Bhatia, Kush and Abbeel, Pieter and Dragan,
              Anca D",
  keywords = "assurance\_explicit;acm\_review\_additions;Paper
              Reviews;Assurances"
}

@BOOK{Roberts1908-kf,
  title     = "Joseph Smith The {Prophet-Teacher}: A Discourse",
  author    = "Roberts, B H",
  publisher = "Deseret News",
  year      =  1908,
  keywords  = "Religious"
}

@INPROCEEDINGS{Pedersen2018-gg,
  title     = "Simulations and {Self-Driving} Cars: A Study of Trust and
               Consequences",
  booktitle = "Companion of the 2018 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "Pedersen, Bjarke Kristian Maigaard Kj{\ae}r and Andersen,
               Kamilla Egedal and K{\"o}slich, Simon and Weigelin, Bente
               Charlotte and Kuusinen, Kati",
  publisher = "ACM",
  pages     = "205--206",
  series    = "HRI '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "applied consequences, artificial intelligence, autonomous
               system, human-robot interaction, self-driving car, simulations,
               trust;NotRead"
}

@INPROCEEDINGS{De_Graaf2018-qi,
  title     = "Explainable Robotic Systems",
  booktitle = "Companion of the 2018 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "de Graaf, Maartje M A and Malle, Bertram F and Dragan, Anca and
               Ziemke, Tom",
  publisher = "ACM",
  pages     = "387--388",
  series    = "HRI '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "behavior explanation, explainable robotics, intentionality,
               theory of mind, transparency, trust calibration.;NotRead"
}

@INPROCEEDINGS{Kwon2018-xt,
  title     = "Expressing Robot Incapability",
  booktitle = "Proceedings of the 2018 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "Kwon, Minae and Huang, Sandy H and Dragan, Anca D",
  publisher = "ACM",
  pages     = "87--95",
  series    = "HRI '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "expressive robot motion, incapability, trajectory
               optimization;acm\_review\_additions;Integral
               Assurance;Human-like Behavior"
}

@INPROCEEDINGS{Javed2018-dq,
  title     = "{Robot-Assisted} {Socio-Emotional} Intervention Framework for
               Children with Autism Spectrum Disorder",
  booktitle = "Companion of the 2018 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "Javed, Hifza and Jeon, Myounghoon and Howard, Ayanna and Park,
               Chung Hyuk",
  publisher = "ACM",
  pages     = "131--132",
  series    = "HRI '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "autism spectrum disorder, emotion processing, emotion
               regulation, robot-assisted intervention, sensory overload,
               socio-emotional intervention;NotRead"
}

@INPROCEEDINGS{Shu2018-hi,
  title     = "Human Trust in Robot Capabilities Across Tasks",
  booktitle = "Companion of the 2018 {ACM/IEEE} International Conference on
               {Human-Robot} Interaction",
  author    = "Shu, Pan and Min, Chen and Bodala, Indu and Nikolaidis, Stefanos
               and Hsu, David and Soh, Harold",
  publisher = "ACM",
  pages     = "241--242",
  series    = "HRI '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "human robot interaction, trust;NotRead"
}

@INPROCEEDINGS{Reinhardt2017-yh,
  title     = "Dominance and movement cues of robot motion: A user study on
               trust and predictability",
  booktitle = "2017 {IEEE} International Conference on Systems, Man, and
               Cybernetics ({SMC})",
  author    = "Reinhardt, J and Pereira, A and Beckert, D and Bengler, K",
  abstract  = "We investigate the effect of dominant and submissive movement
               strategies and a movement cue in a human-robot cooperation
               scenario on perceived predictability and trust. Four different
               movement strategies in proximal cooperation between a robot
               manipulator and a human participant were tested in an experiment
               in which participants had to arrange small objects in a shared
               workspace working on the same product as the robot. The features
               of the robot motion were characterized by dominance or a
               movement cue. The robot modifies its motion in two ways
               resulting in four different movement strategies: either it stops
               when the human is in danger of collision (submissive) or not
               (dominant), and either it performs a backing-off movement cue or
               not. The participants evaluated the movement strategies in terms
               of trust and predictability in a questionnaire. We found that
               the submissive backing-off movement strategy significantly
               enhanced the users' trust compared to the dominant movement
               strategy without movement cue. Other strategies showed no
               significant differences in trust or predictability.",
  pages     = "1493--1498",
  month     =  oct,
  year      =  2017,
  keywords  = "human-robot interaction;manipulators;mobile
               robots;dominance;dominant movement strategies;human-robot
               cooperation scenario;movement cue;perceived predictability;robot
               manipulator;robot motion;submissive backing-off movement
               strategy;submissive movement strategies;trust;Collision
               avoidance;Robot kinematics;Robot motion;Robot sensing
               systems;Trajectory;Human Factors;Human-Machine Cooperation \&
               Systems;Robotic systems;NotRead"
}

@ARTICLE{Olah2018-rp,
  title    = "The Building Blocks of Interpretability",
  author   = "Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter,
              Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev,
              Alexander",
  journal  = "Distill",
  volume   =  3,
  number   =  3,
  month    =  mar,
  year     =  2018,
  keywords = "NotRead;acm\_review\_additions;Supplemental Assurance;Reduce
              Complexity;Assurances"
}

@ARTICLE{Olah2017-hc,
  title   = "Research Debt",
  author  = "Olah, Chris and Carter, Shan",
  journal = "Distill",
  volume  =  2,
  number  =  3,
  month   =  mar,
  year    =  2017
}

@ARTICLE{Curse_undated-kv,
  title  = "Workshop track - {ICLR} 2018",
  author = "Curse, Winner's"
}

@ARTICLE{McGuire2018-oo,
  title    = "Failure is Not an Option: Policy Learning for Adaptive Recovery
              in Space Operations",
  author   = "McGuire, S and Furlong, M and Heckman, C and Julier, S and
              Szafir, D J and Ahmed, N",
  abstract = "This paper considers the problem of how robots in long-term space
              operations can learn to choose appropriate sources of assistance
              to recover from failures. Current assistant selection methods for
              failure handling are based on manually specified static look up
              tables or policies, which are not responsive to dynamic
              environments or uncertainty in human performance. We describe a
              novel and highly flexible learning- based assistant selection
              framework that uses contextual multi- arm bandit algorithms. The
              contextual bandits exploit infor- mation from observed
              environment and assistant performance variables to efficiently
              learn selection policies under a wide set of uncertain operating
              conditions and unknown/dynamically constrained assistant
              capabilities. Proof of concept simulations of long-term
              human-robot interactions for space exploration are used to
              compare the performance of the contextual bandit against other
              state of the art assistant selection approaches. The contextual
              bandit outperforms conventional static policies and
              non-contextual learning approaches, and also demonstrates
              favorable robustness and scaling properties.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   = "PP",
  number   =  99,
  pages    = "1--1",
  year     =  2018,
  keywords = "Earth;Heuristic algorithms;Monitoring;Resource
              management;Robots;Space missions;Task analysis;Human Factors and
              Human-in-the-Loop;Learning and Adaptive Systems;Space Robotics
              and Automation"
}

@ARTICLE{noauthor_undated-aq,
  title = "[{PDF]A} history of Bayesian neural networks - Bayesian Deep
           Learning"
}

@ARTICLE{Mullachery2018-zp,
  title         = "Bayesian Neural Networks",
  author        = "Mullachery, Vikram and Khera, Aniruddh and Husain, Amir",
  abstract      = "This paper describes and discusses Bayesian Neural Network
                   (BNN). The paper showcases a few different applications of
                   them for classification and regression problems. BNNs are
                   comprised of a Probabilistic Model and a Neural Network. The
                   intent of such a design is to combine the strengths of
                   Neural Networks and Stochastic modeling. Neural Networks
                   exhibit continuous function approximator capabilities.
                   Stochastic models allow direct specification of a model with
                   known interaction between parameters to generate data.
                   During the prediction phase, stochastic models generate a
                   complete posterior distribution and produce probabilistic
                   guarantees on the predictions. Thus BNNs are a unique
                   combination of neural network and stochastic models with the
                   stochastic model forming the core of this integration. BNNs
                   can then produce probabilistic guarantees on it's
                   predictions and also generate the distribution of parameters
                   that it has learnt from the observations. That means, in the
                   parameter space, one can deduce the nature and shape of the
                   neural network's learnt parameters. These two
                   characteristics makes them highly attractive to
                   theoreticians as well as practitioners. Recently there has
                   been a lot of activity in this area, with the advent of
                   numerous probabilistic programming libraries such as: PyMC3,
                   Edward, Stan etc. Further this area is rapidly gaining
                   ground as a standard machine learning approach for numerous
                   problems",
  month         =  jan,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1801.07710"
}

@INCOLLECTION{Christiano2017-rm,
  title     = "Deep Reinforcement Learning from Human Preferences",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic,
               Miljan and Legg, Shane and Amodei, Dario",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "4302--4310",
  year      =  2017,
  keywords  = "acm\_review\_additions;Integral Assurance;Value Alignment"
}

@INCOLLECTION{Hadfield-Menell2017-tl,
  title     = "Inverse Reward Design",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and
               Russell, Stuart J and Dragan, Anca",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  abstract  = "Autonomous agents optimize the reward function we give them.
               What they don't know is how hard it is for us to design a reward
               function that actually captures what we want. When designing the
               reward, we might think of some specific training scenarios, and
               make sure that the reward will lead to the right behavior in
               those scenarios. Inevitably, agents encounter new scenarios
               (e.g., new types of terrain) where optimizing that same reward
               may lead to undesired behavior. Our insight is that reward
               functions are merely observations about what the designer
               actually wants, and that they should be interpreted in the
               context in which they were designed. We introduce inverse reward
               design (IRD) as the problem of inferring the true objective
               based on the designed reward and the training MDP. We introduce
               approximate methods for solving IRD problems, and use their
               solution to plan risk-averse behavior in test MDPs. Empirical
               results suggest that this approach can help alleviate negative
               side effects of misspecified reward functions and mitigate
               reward hacking.",
  publisher = "Curran Associates, Inc.",
  pages     = "6765--6774",
  year      =  2017
}

@ARTICLE{Hu2016-yn,
  title    = "{Real-Time} Sensing of Trust in {Human-Machine} Interactions",
  author   = "Hu, Wan-Lin and Akash, Kumar and Jain, Neera and Reid, Tahira",
  abstract = "Human trust in automation plays an important role in successful
              interactions between humans and machines. To design intelligent
              machines that can respond to changes in human trust, real-time
              sensing of trust level is needed. In this paper, we describe an
              empirical trust sensor model that maps psychophysiological
              measurements to human trust level. The use of psychophysiological
              measurements is motivated by their ability to capture a human's
              response in real time. An exhaustive feature set is considered,
              and a rigorous statistical approach is used to determine a
              reduced set of ten features. Multiple classification methods are
              considered for mapping the reduced feature set to the categorical
              trust level. The results show that psychophysiological
              measurements can be used to sense trust in real-time. Moreover, a
              mean accuracy of 71.57\% is achieved using a combination of
              classifiers to model trust level in each human subject. Future
              work will consider the effect of human demographics on feature
              selection and modeling.",
  volume   =  49,
  number   =  32,
  pages    = "48--53",
  month    =  dec,
  year     =  2016,
  keywords = "NotRead"
}

@ARTICLE{Zhu2018-rk,
  title         = "An Overview of Machine Teaching",
  author        = "Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and
                   Rafferty, Anna N",
  abstract      = "In this paper we try to organize machine teaching as a
                   coherent set of ideas. Each idea is presented as varying
                   along a dimension. The collection of dimensions then form
                   the problem space of machine teaching, such that existing
                   teaching problems can be characterized in this space. We
                   hope this organization allows us to gain deeper
                   understanding of individual teaching problems, discover
                   connections among them, and identify gaps in the field.",
  month         =  jan,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1801.05927"
}

@ARTICLE{Siegrist2000-jy,
  title    = "The influence of trust and perceptions of risks and benefits on
              the acceptance of gene technology",
  author   = "Siegrist, M",
  abstract = "A causal model explaining acceptance of gene technology was
              tested. It was hypothesized that trust in institutions using gene
              technology or using modified products has a positive impact on
              perceived benefit and a negative influence on perceived risk of
              this technology. Furthermore, perceived benefit and perceived
              risk determine acceptance of biotechnology. In other words, trust
              has an indirect influence on the acceptance of the technology.
              The postulated model was tested using structural equation
              modeling procedures and data from a random quota sample of 1001
              Swiss citizens between 18 and 74 years old. Results indicated
              that the proposed model fits the data very well. The same causal
              model explains females' and males' acceptance of gene technology.
              Gender differences were found for the latent variables trust,
              perceived benefit, and acceptance of gene technology. Females
              indicated more trust, perceived less benefit, and demonstrated
              less acceptance than did males. No significant difference was
              observed for perceived risk. The implications of the results are
              discussed.",
  journal  = "Risk Anal.",
  volume   =  20,
  number   =  2,
  pages    = "195--203",
  month    =  apr,
  year     =  2000,
  keywords = "Empirical Approach; Genetics and Reproduction;NotRead;Assurances",
  language = "en"
}

@BOOK{Various_undated-yx,
  title    = "A Compilation Containing The Lectures On Faith",
  author   = "{Various}",
  editor   = "Lundwall, N B",
  keywords = "Religious"
}

@BOOK{Roberts1907-mj,
  title     = "The Seventy's Course in Theology",
  author    = "Roberts, Elder B H",
  editor    = "Roberts, Elder B H",
  publisher = "The Deseret News",
  year      =  1907,
  keywords  = "Religious"
}

@ARTICLE{Chen2018-us,
  title         = "Planning with Trust for {Human-Robot} Collaboration",
  author        = "Chen, Min and Nikolaidis, Stefanos and Soh, Harold and Hsu,
                   David and Srinivasa, Siddhartha",
  abstract      = "Trust is essential for human-robot collaboration and user
                   adoption of autonomous systems, such as robot assistants.
                   This paper introduces a computational model which integrates
                   trust into robot decision-making. Specifically, we learn
                   from data a partially observable Markov decision process
                   (POMDP) with human trust as a latent variable. The
                   trust-POMDP model provides a principled approach for the
                   robot to (i) infer the trust of a human teammate through
                   interaction, (ii) reason about the effect of its own actions
                   on human behaviors, and (iii) choose actions that maximize
                   team performance over the long term. We validated the model
                   through human subject experiments on a table-clearing task
                   in simulation (201 participants) and with a real robot (20
                   participants). The results show that the trust-POMDP
                   improves human-robot team performance in this task. They
                   further suggest that maximizing trust in itself may not
                   improve team performance.",
  month         =  jan,
  year          =  2018,
  keywords      = "human\_study;assurance\_explicit;Integral Assurance;Value
                   Alignment;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1801.04099"
}

@ARTICLE{Innes2018-tr,
  title         = "Reasoning about Unforeseen Possibilities During Policy
                   Learning",
  author        = "Innes, Craig and Lascarides, Alex and Albrecht, Stefano V
                   and Ramamoorthy, Subramanian and Rosman, Benjamin",
  abstract      = "Methods for learning optimal policies in autonomous agents
                   often assume that the way the domain is conceptualised---its
                   possible states and actions and their causal structure---is
                   known in advance and does not change during learning. This
                   is an unrealistic assumption in many scenarios, because new
                   evidence can reveal important information about what is
                   possible, possibilities that the agent was not aware existed
                   prior to learning. We present a model of an agent which both
                   discovers and learns to exploit unforeseen possibilities
                   using two sources of evidence: direct interaction with the
                   world and communication with a domain expert. We use a
                   combination of probabilistic and symbolic reasoning to
                   estimate all components of the decision problem, including
                   its set of random variables and their causal dependencies.
                   Agent simulations show that the agent converges on optimal
                   polices even when it starts out unaware of factors that are
                   critical to behaving optimally.",
  month         =  jan,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1801.03331"
}

@ARTICLE{Khalastchi2018-xq,
  title     = "On Fault Detection and Diagnosis in Robotic Systems",
  author    = "Khalastchi, Eliahu and Kalech, Meir",
  journal   = "ACM Comput. Surv.",
  publisher = "ACM",
  volume    =  51,
  number    =  1,
  pages     = "9:1--9:24",
  month     =  jan,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "Fault detection, fault diagnosis, robots;NotRead;Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fogg2009-vb,
  title     = "Motivating, influencing, and persuading users: An introduction
               to captology",
  author    = "Fogg, B J and Cuellar, G and Danielson, D",
  abstract  = "Since the advent of modern computing in 1946, the uses of
               computing technology have expanded far beyond their initial role
               of performing complex calculations (Denning \& Metcalfe, 1997).
               Computers are not just for scientists any more; they are an
               integral part of workplaces and homes. The diffusion of
               computers has led to new uses for interactive technology;
               including the use of computers to change people's attitudes and
               behavior---in …",
  journal   = "Human Computer Interaction",
  publisher = "books.google.com",
  year      =  2009,
  keywords  = "assurance\_explicit;assurances;Assurances"
}

@INPROCEEDINGS{Fogg1998-zf,
  title     = "Persuasive Computers: Perspectives and Research Directions",
  booktitle = "Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems",
  author    = "Fogg, B J",
  publisher = "ACM Press/Addison-Wesley Publishing Co.",
  pages     = "225--232",
  series    = "CHI '98",
  year      =  1998,
  address   = "New York, NY, USA",
  keywords  = "captology, computers as persuasive technologies, computers as
               social actors, design methods, ethics, media,
               persuasion;NotRead;assurances;assurance\_explicit;acm\_review\_additions;Assurances"
}

@ARTICLE{Parker_A_Williams2017-qb,
  title    = "Measuring Actual Behaviors in {HCI} Research -- A call to Action
              and an Example",
  author   = "Parker A. Williams, University of Arizona and Jeffrey Jenkins,
              Brigham Young University and Joseph Valacich, University of
              Arizona and Michael D. Byrd, University of Arizona",
  abstract = "There have been repeated calls for studies in behavioral science
              and human-computer interaction (HCI) research to measure
              participants' actual behaviors. HCI research studies often use
              multiple constructs as perceived measures of behavior, which are
              captured using participants' self-reports on surveys. Response
              biases, however, are a widespread threat to the validity of
              self-report measures. To mitigate this threat to validity, we
              propose that studies in HCI measure actual behaviors in
              appropriate contexts rather than solely perceptions. We report an
              example of using movements that reflect both actual behavior and
              behavioral changes measured within a health care IS usage
              context, specifically the detection and alleviation of
              neuromuscular degenerative disease. We propose and test a method
              of monitoring mouse-cursor movements to detect hand tremors in
              real time when individuals are using websites. Our work suggests
              that analyzing hand movements as an actual (rather than
              perceptual) measure of usage could enrich other areas of IS
              research (e.g., technology acceptance, efficacy, fear, etc.), in
              which perceptions of states and behavior are measured post hoc to
              the interaction and subject to the threats of various forms of
              response bias.",
  journal  = "AIS Transactions on Human-Computer Interaction",
  volume   =  9,
  number   =  4,
  pages    = "339--352",
  year     =  2017,
  keywords = "trust\_related\_behavior;Assurances"
}

@ARTICLE{Haiqing_Li2017-ue,
  title    = "Information Technology Enabled Persuasion: An Experimental
              Investigation of the Role of Communication Channel, Strategy and
              Affect",
  author   = "Haiqing Li, City of Hope and Samir Chatterjee, Claremont Graduate
              University and Ozgur Turetken, Ryerson University",
  abstract = "With advances in information and communication technologies
              (ICT), organizations of various forms now deploy an increasing
              number of ICT-enabled persuasive systems in several domains.
              Traditional computer-mediated communication (CMC) theories mainly
              focus on the effectiveness of media in the
              synchronous/asynchronous spectrum for effectively matching medium
              with communication task. The contemporary communication
              environment is rich with asynchronous channels such as email,
              Web, and text messaging, which makes it important to go beyond
              synchronicity and determine the nuances among various
              asynchronous channels. No rigorous research has compared the
              effectiveness of these channels in the persuasive systems domain
              where organizations use technology to persuade users to modify
              their behavior in a direction that they mutually agree to be
              desirable. In this paper, we study the effectiveness of CMC and
              the strategy used to frame the persuasive message. We explore
              persuasive strategies of praising, reminding, suggesting, and
              rewarding for health behavior and promotion. We model user
              experience as a mediator between channel strategy combinations
              and persuasive effectiveness. Through controlled user studies, we
              compared sixteen combinations of communication channel and
              persuasive strategy with or without emoticons. We found that
              channel/strategy combinations affect persuasive effectiveness
              (mediated by user experience) in varying degrees. Our findings
              contribute to the body of CMC and persuasive system knowledge and
              have practical implications for online advertising, health
              promotion, and persuasive technology design.",
  journal  = "AIS Transactions on Human-Computer Interaction",
  volume   =  9,
  number   =  4,
  pages    = "281--300",
  year     =  2017,
  keywords = "assurances;assurance\_explicit;assurance\_method;assurance\_medium;Assurances"
}

@ARTICLE{Biglova2004-dr,
  title    = "Different Approaches to Risk Estimation in Portfolio Theory",
  author   = "Biglova, Almira and Ortobelli, Sergio and Rachev, Svetlozar T and
              Stoyanov, Stoyan",
  abstract = "Some new performance measures may be regarded as alternatives to
              the most popular criterion for portfolio optimization, the Sharpe
              ratio. Analysis of some allocation problems here takes into
              consideration portfolio selection models based. on different risk
              perceptions and sample paths of the final wealth process for each
              allocation problem. One new performance ratio seems to be
              suitable for some optimization problems, but we need a thorough
              classification of the set of performance measures that would be
              ideal for large classes of financial optimization problems.",
  journal  = "The Journal of Portfolio Management",
  volume   =  31,
  number   =  1,
  pages    = "103--112",
  month    =  sep,
  year     =  2004,
  keywords = "NotRead;SelfConfidence"
}

@INPROCEEDINGS{Farinelli2006-jj,
  title      = "Computational Asset Allocation Using {One-Sided} and
                {Two-Sided} Variability Measures",
  booktitle  = "Computational Science -- {ICCS} 2006",
  author     = "Farinelli, Simone and Rossello, Damiano and Tibiletti, Luisa",
  abstract   = "Excluding the assumption of normality in return distributions,
                a general reward-risk ratio suitable to compare portfolio
                returns with respect to a benchmark must includes asymmetrical
                information on both ``good'' volatility (above the benchmark)
                and ``bad'' volatility (below the benchmark), with different
                sensitivities. Including the Farinelli-Tibiletti ratio and few
                other indexes recently proposed by the literature, the class of
                one-sided variability measures achieves the goal. We
                investigate the forecasting ability of eleven alternatives
                ratios in portfolio optimization problems. We employ data from
                security markets to quantify the portfolio's overperformance
                with respect to a given benchmark.",
  publisher  = "Springer, Berlin, Heidelberg",
  pages      = "324--331",
  series     = "Lecture Notes in Computer Science",
  month      =  may,
  year       =  2006,
  keywords   = "NotRead;SelfConfidence",
  language   = "en",
  conference = "International Conference on Computational Science"
}

@ARTICLE{Israelsen2017-ym,
  title         = "``{Dave...I} can assure you...that it's going to be all
                   right...'' -- A definition, case for, and survey of
                   algorithmic assurances in human-autonomy trust relationships",
  author        = "Israelsen, Brett W and Ahmed, Nisar R",
  abstract      = "Those who design, use, and are otherwise affected by
                   advanced, technologies like artificially intelligent,
                   autonomous systems want to know that these systems will
                   perform correctly, understand the reasons behind their
                   actions, and know how to use them appropriately. In short:
                   they want to be able to trust such systems. Consequently,
                   designers have devised various kinds of assurances for
                   assessing trust. Typically, however, these assessments are
                   ad hoc, and have not been formally related to each other or
                   to formal trust models. This paper presents a survey of
                   algorithmic assurances that allow users to calibrate their
                   trust in autonomous artificially intelligent agents and use
                   such autonomous agents more appropriately. To this end
                   algorithmic assurances are first formally defined, and
                   classified, from the perspective of formally modeled trust
                   relationships. The survey is then performed using research
                   from related communities such as machine learning,
                   human-computer interaction, human-robot interaction,
                   e-commerce, and others. The literature for different classes
                   of assurances are identified with seven different levels of
                   integration for artificially intelligent agents; these
                   classes are useful for practitioners and system designers.
                   Recommendations and directions for future work are also
                   presented.",
  month         =  nov,
  year          =  2017,
  keywords      = "My Papers;myPapers;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1711.03846"
}

@ARTICLE{Gans2017-fh,
  title         = "{Self-Regulating} Artificial General Intelligence",
  author        = "Gans, Joshua S",
  abstract      = "Here we examine the paperclip apocalypse concern for
                   artificial general intelligence (or AGI) whereby a
                   superintelligent AI with a simple goal (ie., producing
                   paperclips) accumulates power so that all resources are
                   devoted towards that simple goal and are unavailable for any
                   other use. We provide conditions under which a paper
                   apocalypse can arise but also show that, under certain
                   architectures for recursive self-improvement of AIs, that a
                   paperclip AI may refrain from allowing power capabilities to
                   be developed. The reason is that such developments pose the
                   same control problem for the AI as they do for humans (over
                   AIs) and hence, threaten to deprive it of resources for its
                   primary goal.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1711.04309"
}

@ARTICLE{Weller2017-zx,
  title         = "Challenges for Transparency",
  author        = "Weller, Adrian",
  abstract      = "Transparency is often deemed critical to enable effective
                   real-world deployment of intelligent systems. Yet the
                   motivations for and benefits of different types of
                   transparency can vary significantly depending on context,
                   and objective measurement criteria are difficult to
                   identify. We provide a brief survey, suggesting challenges
                   and related concerns. We highlight and review settings where
                   transparency may cause harm, discussing connections across
                   privacy, multi-agent game theory, economics, fairness and
                   trust.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1708.01870"
}

@ARTICLE{Kumar2016-yw,
  title         = "Theory-guided Data Science: A New Paradigm for Scientific
                   Discovery",
  author        = "Karpatne, Anuj and Atluri, Gowtham and Faghmous, James and
                   Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop
                   and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin",
  abstract      = "Data science models, although successful in a number of
                   commercial domains, have had limited applicability in
                   scientific problems involving complex physical phenomena.
                   Theory-guided data science (TGDS) is an emerging paradigm
                   that aims to leverage the wealth of scientific knowledge for
                   improving the effectiveness of data science models in
                   enabling scientific discovery. The overarching vision of
                   TGDS is to introduce scientific consistency as an essential
                   component for learning generalizable models. Further, by
                   producing scientifically interpretable models, TGDS aims to
                   advance our scientific understanding by discovering novel
                   domain insights. Indeed, the paradigm of TGDS has started to
                   gain prominence in a number of scientific disciplines such
                   as turbulence modeling, material discovery, quantum
                   chemistry, bio-medical science, bio-marker discovery,
                   climate science, and hydrology. In this paper, we formally
                   conceptualize the paradigm of TGDS and present a taxonomy of
                   research themes in TGDS. We describe several approaches for
                   integrating domain knowledge in different research themes
                   using illustrative examples from different disciplines. We
                   also highlight some of the promising avenues of novel
                   research for realizing the full potential of theory-guided
                   data science.",
  month         =  dec,
  year          =  2016,
  keywords      = "TGDS;Integral Assurance;Interpretable Models;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1612.08544"
}

@INPROCEEDINGS{Nancy_Viola_Wuenderlich2017-dh,
  title     = "A Nice and Friendly Chat with a Bot: User Perceptions of
               {AI-Based} Service Agents",
  booktitle = "{ICIS} 2017 Proceedings",
  author    = "Nancy Viola Wuenderlich, Universitat Paderborn Fakultat Fur
               Wirtschaftswissenschaften and Stefanie Paluch, Rwth Aachen
               University and {Authors}",
  abstract  = "As more organizations establish artificial intelligence-based
               service agents to offer an automated customer dialogue it is
               crucial to understand how users perceive this new form of
               technology-mediated communication. AI-based service agents
               interact in a similar way as humans in human-to-human chat
               conversations, but instead of a live person on the other end, a
               chat bot steers the communication based on artificial
               intelligence and Natural Language Interaction. By combining
               qualitative and quantitative methods, we examine this context
               and explore the role of perceived authenticity and its impact on
               users' attitudinal and behavioral outcomes. Our results from the
               qualitative studies show that users infer the authenticity of
               the AI-based service agents based on two different categories of
               cues: (1) agent-related cues and (2) communication-related cues.
               We employ additional experimental studies to empirically test
               antecedents and consequences of authenticity perceptions in
               AI-based service encounters.",
  year      =  2017,
  keywords  = "NotRead;assurance\_explicit;assurances;Assurances"
}

@ARTICLE{Guestrin_undated-de,
  title  = "Distributed Planning in Hierarchical Factored {MDPs}",
  author = "Guestrin, Carlos and Dept, Computer Science"
}

@ARTICLE{Hauskrecht2013-og,
  title         = "Hierarchical Solution of Markov Decision Processes using
                   Macro-actions",
  author        = "Hauskrecht, Milos and Meuleau, Nicolas and Kaelbling, Leslie
                   Pack and Dean, Thomas L and Boutilier, Craig",
  abstract      = "We investigate the use of temporally abstract actions, or
                   macro-actions, in the solution of Markov decision processes.
                   Unlike current models that combine both primitive actions
                   and macro-actions and leave the state space unchanged, we
                   propose a hierarchical model (using an abstract MDP) that
                   works with macro-actions only, and that significantly
                   reduces the size of the state space. This is achieved by
                   treating macroactions as local policies that act in certain
                   regions of state space, and by restricting states in the
                   abstract MDP to those at the boundaries of regions. The
                   abstract MDP approximates the original and can be solved
                   more efficiently. We discuss several ways in which
                   macro-actions can be generated to ensure good solution
                   quality. Finally, we consider ways in which macro-actions
                   can be reused to solve multiple, related MDPs; and we show
                   that this can justify the computational overhead of
                   macro-action generation.",
  month         =  jan,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1301.7381"
}

@ARTICLE{Chipman2005-om,
  title     = "Interpretable dimension reduction",
  author    = "Chipman, Hugh A and Gu, Hong",
  abstract  = "Abstract The analysis of high-dimensional data often begins with
               the identification of lower dimensional subspaces. Principal
               component analysis is a dimension reduction technique that
               identifies linear combinations of variables along which most
               variation occurs or which best ?reconstruct? the original
               variables. For example, many temperature readings may be taken
               in a production process when in fact there are just a few
               underlying variables driving the process. A problem with
               principal components is that the linear combinations can seem
               quite arbitrary. To make them more interpretable, we introduce
               two classes of constraints. In the first, coefficients are
               constrained to equal a small number of values (homogeneity
               constraint). The second constraint attempts to set as many
               coefficients to zero as possible (sparsity constraint). The
               resultant interpretable directions are either calculated to be
               close to the original principal component directions, or
               calculated in a stepwise manner that may make the components
               more orthogonal. A small dataset on characteristics of cars is
               used to introduce the techniques. A more substantial data mining
               application is also given, illustrating the ability of the
               procedure to scale to a very large number of variables.",
  journal   = "J. Appl. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  32,
  number    =  9,
  pages     = "969--987",
  month     =  nov,
  year      =  2005,
  keywords  = "vis\_dr;interp\_models;Interpretable Models;Supplemental
               Assurance;Assurances"
}

@UNPUBLISHED{Adler2016-oi,
  title    = "The computations underlying human confidence reports are
              probabilistic, but not Bayesian",
  author   = "Adler, William T and Ma, Wei Ji",
  abstract = "Humans can meaningfully rate their confidence in a perceptual or
              cognitive decision. It is widely believed that these reports
              reflect the estimated probability that the decision is correct,
              but, upon closer look, this belief is a hypothesis rather than an
              established fact. In a pair of perceptual categorization tasks,
              we tested whether explicit confidence reports reflect the
              Bayesian posterior probability of being correct. This Bayesian
              hypothesis predicts that subjects take sensory uncertainty into
              account in a specific way in the computation of confidence
              ratings. We find that confidence reports are probabilistic:
              subjects take sensory uncertainty into account on a
              trial-to-trial basis. However, they do not do so in the way
              predicted by the Bayesian hypothesis. Instead, heuristic
              probabilistic models provide the best fit to human confidence
              ratings. This conclusion is robust to changes in the uncertainty
              manipulation, task, response modality, additional flexibility in
              the Bayesian model, and model comparison metric. To better
              understand the origins of the heuristic computation, we trained
              feedforward neural networks consisting of generic units with
              error feedback, mapped the output of the trained networks to
              confidence ratings, and fitted our behavioral models to the
              resulting synthetic datasets. We find that the synthetic
              confidence ratings are also best fit by heuristic probabilistic
              models. This suggests that implementational constraints cause
              explicit confidence reports to deviate from being Bayesian.",
  journal  = "bioRxiv",
  pages    = "093203",
  month    =  dec,
  year     =  2016,
  language = "en"
}

@INPROCEEDINGS{Tao2005-kh,
  title      = "Affective Computing: A Review",
  booktitle  = "Affective Computing and Intelligent Interaction",
  author     = "Tao, Jianhua and Tan, Tieniu",
  abstract   = "Affective computing is currently one of the most active
                research topics, furthermore, having increasingly intensive
                attention. This strong interest is driven by a wide spectrum of
                promising applications in many areas such as virtual reality,
                smart surveillance, perceptual interface, etc. Affective
                computing concerns multidisciplinary knowledge background such
                as psychology, cognitive, physiology and computer sciences. The
                paper is emphasized on the several issues involved implicitly
                in the whole interactive feedback loop. Various methods for
                each issue are discussed in order to examine the state of the
                art. Finally, some research challenges and future directions
                are also discussed.",
  publisher  = "Springer, Berlin, Heidelberg",
  pages      = "981--995",
  series     = "Lecture Notes in Computer Science",
  month      =  oct,
  year       =  2005,
  keywords   = "AI;ai\_general;Assurances",
  language   = "en",
  conference = "International Conference on Affective Computing and Intelligent
                Interaction"
}

@ARTICLE{Duch2007-oi,
  title    = "Intuition, Insight, Imagination and Creativity",
  author   = "Duch, W",
  abstract = "Can computers have intuition and insights, and be creative?
              Neurocognitive models inspired by the putative processes in the
              brain show that these mysterious features are a consequence of
              information processing in complex networks. Intuition is
              manifested in categorization based on evaluation of similarity,
              when decision borders are too complex to be reduced to logical
              rules. It is also manifested in heuristic reasoning based on
              partial observations, where network activity selects only those
              paths that may lead to solution, excluding all bad moves. Insight
              results from reasoning at the higher, non-verbal level of
              abstraction that comes from involvement of the right hemisphere
              networks forming large ``linguistic receptive fields.'' Three
              factors are essential for creativity in invention of novel words:
              knowledge of word morphology captured in network connections,
              imagination constrained by this knowledge, and filtering of
              results that selects the most interesting novel words. These
              principles have been implemented using a simple correlation-based
              algorithm for auto-associative memory. Results are surprisingly
              similar to those created by humans.",
  journal  = "IEEE Comput. Intell. Mag.",
  volume   =  2,
  number   =  3,
  pages    = "40--52",
  month    =  aug,
  year     =  2007,
  keywords = "cognition;human factors;linguistics;auto-associative
              memory;correlation-based algorithm;heuristic
              reasoning;intuition;linguistic receptive field;network
              connection;neurocognitive model;nonverbal level;putative
              process;word morphology;Artificial
              intelligence;Autobiographies;Competitive
              intelligence;Computational intelligence;Engines;Helium;Image
              analysis;Information processing;Machine
              intelligence;Machinery;AI;ai\_general;Assurances"
}

@ARTICLE{Calinescu2017-fh,
  title    = "Engineering Trustworthy {Self-Adaptive} Software with Dynamic
              Assurance Cases",
  author   = "Calinescu, R and Weyns, D and Gerasimou, S and Iftikhar, M U and
              Habli, I and Kelly, T",
  abstract = "Building on concepts drawn from control theory, self-adaptive
              software handles environmental and internal uncertainties by
              dynamically adjusting its architecture and parameters in response
              to events such as workload changes and component failures.
              Self-adaptive software is increasingly expected to meet strict
              functional and non-functional requirements in applications from
              areas as diverse as manufacturing, healthcare and finance. To
              address this need, we introduce a methodology for the systematic
              ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST).
              ENTRUST uses a combination of (1) design-time and runtime
              modelling and verification, and (2) industry-adopted assurance
              processes to develop trustworthy self-adaptive software and
              assurance cases arguing the suitability of the software for its
              intended application. To evaluate the effectiveness of our
              methodology, we present a tool-supported instance of ENTRUST and
              its use to develop proof-of-concept self-adaptive software for
              embedded and service-based systems from the oceanic monitoring
              and e-finance domains, respectively. The experimental results
              show that ENTRUST can be used to engineer self-adaptive software
              systems in different application domains and to generate dynamic
              assurance cases for these systems.",
  journal  = "IEEE Trans. Software Eng.",
  volume   = "PP",
  number   =  99,
  pages    = "1--1",
  year     =  2017,
  keywords = "Adaptive systems;Computer architecture;Control
              systems;Monitoring;Runtime;Software systems;Self-adaptive
              software systems;assurance cases;assurance evidence;software
              engineering methodology;NotRead;Assurances"
}

@INCOLLECTION{Humphrey2012-lr,
  title     = "Model Checking {UAV} Mission Plans",
  booktitle = "{AIAA} Modeling and Simulation Technologies Conference",
  author    = "Humphrey, Laura",
  publisher = "American Institute of Aeronautics and Astronautics",
  series    = "Guidance, Navigation, and Control and Co-located Conferences",
  month     =  aug,
  year      =  2012,
  keywords  = "NotRead;Assurances"
}

@ARTICLE{Korpan2017-sj,
  title         = "{WHY}: Natural Explanations from a Robot Navigator",
  author        = "Korpan, Raj and Epstein, Susan L and Aroor, Anoop and Dekel,
                   Gil",
  abstract      = "Effective collaboration between a robot and a person
                   requires natural communication. When a robot travels with a
                   human companion, the robot should be able to explain its
                   navigation behavior in natural language. This paper explains
                   how a cognitively-based, autonomous robot navigation system
                   produces informative, intuitive explanations for its
                   decisions. Language generation here is based upon the
                   robot's commonsense, its qualitative reasoning, and its
                   learned spatial model. This approach produces natural
                   explanations in real time for a robot as it navigates in a
                   large, complex indoor environment.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1709.09741"
}

@ARTICLE{Fox2017-uw,
  title         = "Explainable Planning",
  author        = "Fox, Maria and Long, Derek and Magazzeni, Daniele",
  abstract      = "As AI is increasingly being adopted into application
                   solutions, the challenge of supporting interaction with
                   humans is becoming more apparent. Partly this is to support
                   integrated working styles, in which humans and intelligent
                   systems cooperate in problem-solving, but also it is a
                   necessary step in the process of building trust as humans
                   migrate greater responsibility to such systems. The
                   challenge is to find effective ways to communicate the
                   foundations of AI-driven behaviour, when the algorithms that
                   drive it are far from transparent to humans. In this paper
                   we consider the opportunities that arise in AI planning,
                   exploiting the model-based representations that form a
                   familiar and common basis for communication with users,
                   while acknowledging the gap between planning algorithms and
                   human problem-solving.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1709.10256"
}

@ARTICLE{Lala2017-ir,
  title         = "Detection of social signals for recognizing engagement in
                   human-robot interaction",
  author        = "Lala, Divesh and Inoue, Koji and Milhorat, Pierrick and
                   Kawahara, Tatsuya",
  abstract      = "Detection of engagement during a conversation is an
                   important function of human-robot interaction. The level of
                   user engagement can influence the dialogue strategy of the
                   robot. Our motivation in this work is to detect several
                   behaviors which will be used as social signal inputs for a
                   real-time engagement recognition model. These behaviors are
                   nodding, laughter, verbal backchannels and eye gaze. We
                   describe models of these behaviors which have been learned
                   from a large corpus of human-robot interactions with the
                   android robot ERICA. Input data to the models comes from a
                   Kinect sensor and a microphone array. Using our engagement
                   recognition model, we can achieve reasonable performance
                   using the inputs from automatic social signal detection,
                   compared to using manual annotation as input.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "1709.10257"
}

@ARTICLE{Chatterjee2017-jv,
  title         = "Sensor Synthesis for {POMDPs} with Reachability Objectives",
  author        = "Chatterjee, Krishnendu and Chmelik, Martin and Topcu, Ufuk",
  abstract      = "Partially observable Markov decision processes (POMDPs) are
                   widely used in probabilistic planning problems in which an
                   agent interacts with an environment using noisy and
                   imprecise sensors. We study a setting in which the sensors
                   are only partially defined and the goal is to synthesize
                   ``weakest'' additional sensors, such that in the resulting
                   POMDP, there is a small-memory policy for the agent that
                   almost-surely (with probability~1) satisfies a reachability
                   objective. We show that the problem is NP-complete, and
                   present a symbolic algorithm by encoding the problem into
                   SAT instances. We illustrate trade-offs between the amount
                   of memory of the policy and the number of additional sensors
                   on a simple example. We have implemented our approach and
                   consider three classical POMDP examples from the literature,
                   and show that in all the examples the number of sensors can
                   be significantly decreased (as compared to the existing
                   solutions in the literature) without increasing the
                   complexity of the policies.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1710.00675"
}

@ARTICLE{Doran2017-pg,
  title         = "What Does Explainable {AI} Really Mean? A New
                   Conceptualization of Perspectives",
  author        = "Doran, Derek and Schulz, Sarah and Besold, Tarek R",
  abstract      = "We characterize three notions of explainable AI that cut
                   across research fields: opaque systems that offer no insight
                   into its algo- rithmic mechanisms; interpretable systems
                   where users can mathemat- ically analyze its algorithmic
                   mechanisms; and comprehensible systems that emit symbols
                   enabling user-driven explanations of how a conclusion is
                   reached. The paper is motivated by a corpus analysis of
                   NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing
                   differences in how work on explainable AI is positioned in
                   various fields. We close by introducing a fourth notion:
                   truly explainable systems, where automated reasoning is
                   central to output crafted explanations without requiring
                   human post processing as final step of the generative
                   process.",
  month         =  oct,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1710.00794"
}

@ARTICLE{Vilamala2017-ba,
  title         = "Deep Convolutional Neural Networks for Interpretable
                   Analysis of {EEG} Sleep Stage Scoring",
  author        = "Vilamala, Albert and Madsen, Kristoffer H and Hansen, Lars K",
  abstract      = "Sleep studies are important for diagnosing sleep disorders
                   such as insomnia, narcolepsy or sleep apnea. They rely on
                   manual scoring of sleep stages from raw polisomnography
                   signals, which is a tedious visual task requiring the
                   workload of highly trained professionals. Consequently,
                   research efforts to purse for an automatic stage scoring
                   based on machine learning techniques have been carried out
                   over the last years. In this work, we resort to multitaper
                   spectral analysis to create visually interpretable images of
                   sleep patterns from EEG signals as inputs to a deep
                   convolutional network trained to solve visual recognition
                   tasks. As a working example of transfer learning, a system
                   able to accurately classify sleep stages in new unseen
                   patients is presented. Evaluations in a widely-used publicly
                   available dataset favourably compare to state-of-the-art
                   results, while providing a framework for visual
                   interpretation of outcomes.",
  month         =  oct,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1710.00633"
}

@ARTICLE{Wang2017-oh,
  title    = "A Bayesian Framework for Learning Rule Sets for Interpretable
              Classification",
  author   = "Wang, Tong and Rudin, Cynthia and Doshi-Velez, Finale and Liu,
              Yimin and Klampfl, Erica and MacNeille, Perry",
  journal  = "J. Mach. Learn. Res.",
  volume   =  18,
  number   =  70,
  pages    = "1--37",
  year     =  2017,
  keywords = "NotRead;Assurances"
}

@MISC{noauthor_undated-dc,
  title        = "Drones | Homeland Security News Wire",
  howpublished = "\url{http://www.homelandsecuritynewswire.com/dr20140116-autonomous-drones-to-help-in-search-and-rescue-disaster-relief}",
  note         = "Accessed: 2017-10-2",
  keywords     = "SelfConfidence/Experiment"
}

@ARTICLE{Y_Silva2014-eq,
  title     = "A methodology for determining operational priorities for
               prevention and suppression of wildland fires",
  author    = "y Silva, Francisco Rodr{\'\i}guez and Mart{\'\i}nez, Juan
               Ram{\'o}n Molina and Gonz{\'a}lez-Cab{\'a}n, Armando",
  abstract  = "Traditional uses of the forest (timber, forage) have been giving
               way to other uses more in demand (recreation, ecosystem
               services). An observable consequence of this process of forest
               land use conversion is an increase in more difficult and extreme
               wildfires. Wildland forest management and protection program
               budgets are limited, and managers are requesting help in finding
               ways to objectively assign their limited protection resources
               ...",
  journal   = "Int. J. Wildland Fire",
  publisher = "CSIRO",
  volume    =  23,
  number    =  4,
  pages     = "544--554",
  year      =  2014,
  keywords  = "SelfConfidence/Experiment"
}

@ARTICLE{noauthor_undated-qs,
  title    = "[{PDF]Rethinking} Wildfire Suppression With Swarm Robotics",
  keywords = "SelfConfidence/Experiment"
}

@ARTICLE{Allison2016-cx,
  title    = "Airborne Optical and Thermal Remote Sensing for Wildfire
              Detection and Monitoring",
  author   = "Allison, Robert S and Johnston, Joshua M and Craig, Gregory and
              Jennings, Sion",
  abstract = "For decades detection and monitoring of forest and other wildland
              fires has relied heavily on aircraft (and satellites). Technical
              advances and improved affordability of both sensors and sensor
              platforms promise to revolutionize the way aircraft detect,
              monitor and help suppress wildfires. Sensor systems like
              hyperspectral cameras, image intensifiers and thermal cameras
              that have previously been limited in use due to cost or
              technology considerations are now becoming widely available and
              affordable. Similarly, new airborne sensor platforms,
              particularly small, unmanned aircraft or drones, are enabling new
              applications for airborne fire sensing. In this review we outline
              the state of the art in direct, semi-automated and automated fire
              detection from both manned and unmanned aerial platforms. We
              discuss the operational constraints and opportunities provided by
              these sensor systems including a discussion of the objective
              evaluation of these systems in a realistic context.",
  journal  = "Sensors",
  volume   =  16,
  number   =  8,
  month    =  aug,
  year     =  2016,
  keywords = "airborne sensors; detection patrols; fire detection; fire
              monitoring; fire spotting; unmanned aerial vehicles;
              wildfire;SelfConfidence/Experiment",
  language = "en"
}

@ARTICLE{Fulcher2017-xv,
  title         = "Feature-based time-series analysis",
  author        = "Fulcher, Ben D",
  abstract      = "This work presents an introduction to feature-based
                   time-series analysis. The time series as a data type is
                   first described, along with an overview of the
                   interdisciplinary time-series analysis literature. I then
                   summarize the range of feature-based representations for
                   time series that have been developed to aid interpretable
                   insights into time-series structure. Particular emphasis is
                   given to emerging research that facilitates wide comparison
                   of feature-based representations that allow us to understand
                   the properties of a time-series dataset that make it suited
                   to a particular feature-based representation or analysis
                   algorithm. The future of time-series analysis is likely to
                   embrace approaches that exploit machine learning methods to
                   partially automate human learning to aid understanding of
                   the complex dynamical patterns in the time series we measure
                   from the world.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1709.08055"
}

@ARTICLE{Korpan2017-qo,
  title         = "{WHY}: Natural Explanations from a Robot Navigator",
  author        = "Korpan, Raj and Epstein, Susan L and Aroor, Anoop and Dekel,
                   Gil",
  abstract      = "Effective collaboration between a robot and a person
                   requires natural communication. When a robot travels with a
                   human companion, the robot should be able to explain its
                   navigation behavior in natural language. This paper explains
                   how a cognitively-based, autonomous robot navigation system
                   produces informative, intuitive explanations for its
                   decisions. Language generation here is based upon the
                   robot's commonsense, its qualitative reasoning, and its
                   learned spatial model. This approach produces natural
                   explanations in real time for a robot as it navigates in a
                   large, complex indoor environment.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1709.09741"
}

@ARTICLE{Mc_Gregor_undated-bu,
  title    = "Visualizing {High-Dimensional} {MDPs} with {Model-Free} Monte
              Carlo",
  author   = "Mc Gregor, Sean and Houtman, Rachel",
  keywords = "NotRead"
}

@ARTICLE{noauthor_undated-mo,
  title    = "Improved prediction accuracy for disease risk mapping using
              Gaussian Process stacked generalisation",
  author   = "Bhatt, Samir and Cameron, Ewan and Flaxman, Seth R and Weiss,
              Daniel J and Smith, David L and Gething, Peter W",
  abstract = "Maps of infectious disease---charting spatial variations in the
              force of infection, degree of endemicity, and the burden on human
              health---provide an essential evidence base to support planning
              towards global health targets. Contemporary disease mapping
              efforts have embraced statistical modelling approaches to
              properly acknowledge uncertainties in both the available
              measurements and their spatial interpolation. The most common
              such approach is that of Gaussian process regression, a
              mathematical framework comprised of two components: a mean
              function harnessing the predictive power of multiple independent
              variables, and a covariance function yielding spatio-temporal
              shrinkage against residual variation from the mean. Though many
              techniques have been developed to improve the flexibility and
              fitting of the covariance function, models for the mean function
              have typically been restricted to simple linear terms. For
              infectious diseases, known to be driven by complex interactions
              between environmental and socio-economic factors, improved
              modelling of the mean function can greatly boost predictive
              power. Here we present an ensemble approach based on stacked
              generalisation that allows for multiple, non-linear algorithmic
              mean functions to be jointly embedded within the Gaussian process
              framework. We apply this method to mapping Plasmodium falciparum
              prevalence data in Sub-Saharan Africa and show that the
              generalised ensemble approach markedly out-performs any
              individual method.",
  journal  = "arXiv [stat.AP]",
  month    =  dec,
  year     =  2016,
  keywords = "NotRead"
}

@ARTICLE{Abid2017-qm,
  title         = "Contrastive Principal Component Analysis",
  author        = "Abid, Abubakar and Bagaria, Vivek K and Zhang, Martin J and
                   Zou, James",
  abstract      = "We present a new technique called contrastive principal
                   component analysis (cPCA) that is designed to discover
                   low-dimensional structure that is unique to a dataset, or
                   enriched in one dataset relative to other data. The
                   technique is a generalization of standard PCA, for the
                   setting where multiple datasets are available -- e.g. a
                   treatment and a control group, or a mixed versus a
                   homogeneous population -- and the goal is to explore
                   patterns that are specific to one of the datasets. We
                   conduct a wide variety of experiments in which cPCA
                   identifies important dataset-specific patterns that are
                   missed by PCA, demonstrating that it is useful for many
                   applications: subgroup discovery, visualizing trends,
                   feature selection, denoising, and data-dependent
                   standardization. We provide geometrical interpretations of
                   cPCA and show that it satisfies desirable theoretical
                   guarantees. We also extend cPCA to nonlinear settings in the
                   form of kernel cPCA. We have released our code as a python
                   package and documentation is on Github.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1709.06716"
}

@ARTICLE{Chatzilygeroudis2017-fl,
  title         = "Using Parameterized {Black-Box} Priors to Scale Up
                   {Model-Based} Policy Search for Robotics",
  author        = "Chatzilygeroudis, Konstantinos and Mouret, Jean-Baptiste",
  abstract      = "The most data-efficient algorithms for reinforcement
                   learning in robotics are model-based policy search
                   algorithms, which alternate between learning a dynamical
                   model of the robot and optimizing a policy to maximize the
                   expected return given the model and its uncertainties. Among
                   the few proposed approaches, the recently introduced
                   Black-DROPS algorithm exploits a black-box optimization
                   algorithm to achieve both high data-efficiency and good
                   computation times when several cores are used; nevertheless,
                   like all model-based policy search approaches, Black-DROPS
                   does not scale to high dimensional state/action spaces. In
                   this paper, we introduce a new model learning procedure in
                   Black-DROPS that leverages parameterized black-box priors to
                   (1) scale up to high-dimensional systems, and (2) be robust
                   to large inaccuracies of the prior information. We
                   demonstrate the effectiveness of our approach with the
                   ``pendubot'' swing-up task in simulation and with a physical
                   hexapod robot (48D state space, 18D action space) that has
                   to walk forward as fast as possible. The results show that
                   our new algorithm is more data-efficient than previous
                   model-based policy search algorithms (with and without
                   priors) and that it can allow a physical 6-legged robot to
                   learn new gaits in only 16 to 30 seconds of interaction
                   time.",
  month         =  sep,
  year          =  2017,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1709.06917"
}

@ARTICLE{Paiva2017-so,
  title     = "Empathy in Virtual Agents and Robots: A Survey",
  author    = "Paiva, Ana and Leite, Iolanda and Boukricha, Hana and Wachsmuth,
               Ipke",
  journal   = "ACM Trans. Interact. Intell. Syst.",
  publisher = "ACM",
  volume    =  7,
  number    =  3,
  pages     = "11:1--11:40",
  month     =  sep,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "affective computing, empathy, human-computer interaction,
               human-robot interaction, social robots, virtual agents;NotRead"
}

@ARTICLE{Atkinson2013-oc,
  title     = "Autonomous agents and human interpersonal trust: Can we engineer
               a human-machine social interface for trust?",
  author    = "Atkinson, D J and Clark, M H",
  abstract  = "Abstract There is a recognized need to employ autonomous agents
               in domains that are not amenable to conventional automation
               and/or which humans find difficult, dangerous, or undesirable to
               perform. These include time-critical and mission-critical
               applications in health, defense, transportation, and industry,
               where the consequences of failure can be catastrophic. A
               prerequisite for such applications is the establishment of
               wellcalibrated ...",
  journal   = "AAAI Spring Symposium: Trust and Autonomous",
  publisher = "aaai.org",
  year      =  2013,
  keywords  = "NotRead"
}

@TECHREPORT{Israelsen2012-ov,
  title       = "Proprietary technical report on development of novel sensing
                 technique",
  author      = "Israelsen, B W and {et al}",
  institution = "Corning Inc.",
  month       =  nov,
  year        =  2012,
  address     = "275 River Street, Oneonta NY 13820",
  keywords    = "My Papers"
}

@BOOK{Israelsen2009-vi,
  title     = "Documentation of the {APCO}, Inc. Nonlinear System
               Identification ({NLI}) software",
  author    = "Israelsen, B W",
  publisher = "APCO Inc.",
  pages     = "44",
  edition   =  1,
  month     =  nov,
  year      =  2009,
  address   = "2120 N. Redwood Rd. North Salt Lake UT 84054",
  keywords  = "My Papers"
}

@ARTICLE{Israelsen2017-db,
  title         = "``I can assure you [$\ldots$] that it's going to be all
                   right'' -- A definition, case for, and survey of algorithmic
                   assurances in human-autonomy trust relationships",
  author        = "Israelsen, Brett W",
  abstract      = "As technology become more advanced, those who design, use
                   and are otherwise affected by it want to know that it will
                   perform correctly, and understand why it does what it does,
                   and how to use it appropriately. In essence they want to be
                   able to trust the systems that are being designed. In this
                   survey we present assurances that are the method by which
                   users can understand how to trust this technology. Trust
                   between humans and autonomy is reviewed, and the
                   implications for the design of assurances are highlighted. A
                   survey of research that has been performed with respect to
                   assurances is presented, and several key ideas are extracted
                   in order to refine the definition of assurances. Several
                   directions for future research are identified and discussed.",
  month         =  aug,
  year          =  2017,
  keywords      = "My Papers",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1708.00495"
}

@ARTICLE{Goodrich2008-kz,
  title     = "{Human--Robot} Interaction: A Survey",
  author    = "Goodrich, Michael A and Schultz, Alan C",
  journal   = "Foundations and Trends\textregistered{} in Human--Computer
               Interaction",
  publisher = "Now Publishers",
  volume    =  1,
  number    =  3,
  pages     = "203--275",
  year      =  2008,
  keywords  = "Human-Robot Interaction"
}

@ARTICLE{Lazar2007-zf,
  title     = "Understanding Web Credibility: A Synthesis of the Research
               Literature",
  author    = "Lazar, Jonathan and Meiselwitz, Gabriele and Feng, Jinjuan",
  journal   = "Foundations and Trends\textregistered{} in Human--Computer
               Interaction",
  publisher = "Now Publishers",
  volume    =  1,
  number    =  2,
  pages     = "139--202",
  year      =  2007,
  keywords  = "Perception and the user interface; Perception and the user
               interface; Web design"
}

@ARTICLE{Lasota2017-ot,
  title     = "A Survey of Methods for Safe {Human-Robot} Interaction",
  author    = "Lasota, Przemyslaw A and Fong, Terrence and Shah, Julie A",
  journal   = "Foundations and Trends\textregistered{} in Robotics",
  publisher = "Now Publishers",
  volume    =  5,
  number    =  4,
  pages     = "261--349",
  year      =  2017,
  keywords  = "Human-Robot Interaction: Robot Safety; Human-Robot Interaction:
               Physical Robot Interaction; Planning; Robot Control"
}

@BOOK{Parry_undated-yo,
  title     = "Isaiah in the Book of Mormon",
  editor    = "Parry, Donald W and Welch, John H",
  publisher = "Foundation for Ancient Research and Mormon Studies, 1998",
  keywords  = "Religious"
}

@ARTICLE{Van_der_Wilk2017-jz,
  title         = "Convolutional Gaussian Processes",
  author        = "van der Wilk, Mark and Rasmussen, Carl Edward and Hensman,
                   James",
  abstract      = "We present a practical way of introducing convolutional
                   structure into Gaussian processes, making them more suited
                   to high-dimensional inputs like images. The main
                   contribution of our work is the construction of an
                   inter-domain inducing point approximation that is
                   well-tailored to the convolutional kernel. This allows us to
                   gain the generalisation benefit of a convolutional kernel,
                   together with fast but accurate posterior inference. We
                   investigate several variations of the convolutional kernel,
                   and apply it to MNIST and CIFAR-10, which have both been
                   known to be challenging for Gaussian processes. We also show
                   how the marginal likelihood can be used to find an optimal
                   weighting between convolutional and RBF kernels to further
                   improve performance. We hope that this illustration of the
                   usefulness of a marginal likelihood will help automate
                   discovering architectures in larger models.",
  month         =  sep,
  year          =  2017,
  keywords      = "GPs;NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1709.01894"
}

@ARTICLE{Chandrasekaran2017-sd,
  title         = "It Takes Two to Tango: Towards Theory of {AI's} Mind",
  author        = "Chandrasekaran, Arjun and Yadav, Deshraj and Chattopadhyay,
                   Prithvijit and Prabhu, Viraj and Parikh, Devi",
  abstract      = "Theory of Mind is the ability to attribute mental states
                   (beliefs, intents, knowledge, perspectives, etc.) to others
                   and recognize that these mental states may differ from one's
                   own. Theory of Mind is critical to effective communication
                   and to teams demonstrating higher collective performance. To
                   effectively leverage the progress in Artificial Intelligence
                   (AI) to make our lives more productive, it is important for
                   humans and AI to work well together in a team.
                   Traditionally, there has been much emphasis on research to
                   make AI more accurate, and (to a lesser extent) on having it
                   better understand human intentions, tendencies, beliefs, and
                   contexts. The latter involves making AI more human-like and
                   having it develop a theory of our minds. In this work, we
                   argue that for human-AI teams to be effective, humans must
                   also develop a theory of AI's mind - get to know its
                   strengths, weaknesses, beliefs, and quirks. We instantiate
                   these ideas within the domain of Visual Question Answering
                   (VQA). We find that using just a few examples(50), lay
                   people can be trained to better predict responses and
                   oncoming failures of a complex VQA model. Surprisingly, we
                   find that having access to the model's internal states - its
                   confidence in its top-k predictions, explicit or implicit
                   attention maps which highlight regions in the image (and
                   words in the question) the model is looking at (and
                   listening to) while answering a question about an image - do
                   not help people better predict its behavior",
  month         =  apr,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1704.00717"
}

@ARTICLE{Chakraborti2017-uy,
  title         = "{AI} Challenges in {Human-Robot} Cognitive Teaming",
  author        = "Chakraborti, Tathagata and Kambhampati, Subbarao and
                   Scheutz, Matthias and Zhang, Yu",
  abstract      = "Among the many anticipated roles for robots in the future is
                   that of being a human teammate. Aside from all the
                   technological hurdles that have to be overcome with respect
                   to hardware and control to make robots fit to work with
                   humans, the added complication here is that humans have many
                   conscious and subconscious expectations of their teammates -
                   indeed, we argue that teaming is mostly a cognitive rather
                   than physical coordination activity. This introduces new
                   challenges for the AI and robotics community and requires
                   fundamental changes to the traditional approach to the
                   design of autonomy. With this in mind, we propose an update
                   to the classical view of the intelligent agent architecture,
                   highlighting the requirements for mental modeling of the
                   human in the deliberative process of the autonomous agent.
                   In this article, we outline briefly the recent efforts of
                   ours, and others in the community, towards developing
                   cognitive teammates along these guidelines.",
  month         =  jul,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1707.04775"
}

@ARTICLE{Miller2017-pd,
  title         = "Explanation in Artificial Intelligence: Insights from the
                   Social Sciences",
  author        = "Miller, Tim",
  abstract      = "There has been a recent resurgence in the area of
                   explainable artificial intelligence as researchers and
                   practitioners seek to provide more transparency to their
                   algorithms. Much of this research is focused on explicitly
                   explaining decisions or actions to a human observer, and it
                   should not be controversial to say that, if these techniques
                   are to succeed, the explanations they generate should have a
                   structure that humans accept. However, it is fair to say
                   that most work in explainable artificial intelligence uses
                   only the researchers' intuition of what constitutes a `good'
                   explanation. There exists vast and valuable bodies of
                   research in philosophy, psychology, and cognitive science of
                   how people define, generate, select, evaluate, and present
                   explanations. This paper argues that the field of
                   explainable artificial intelligence should build on this
                   existing research, and reviews relevant papers from
                   philosophy, cognitive psychology/science, and social
                   psychology, which study these topics. It draws out some
                   important findings, and discusses ways that these can be
                   infused with work on explainable artificial intelligence.",
  month         =  jun,
  year          =  2017,
  keywords      = "NotRead;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1706.07269"
}

@ARTICLE{noauthor_undated-yd,
  title    = "On the Safety of Machine Learning: {Cyber-Physical} Systems,
              Decision Sciences, and Data Products",
  author   = "Varshney, Kush R and Alemzadeh, Homa",
  abstract = "Machine learning algorithms increasingly influence our decisions
              and interact with us in all parts of our daily lives. Therefore,
              just as we consider the safety of power plants, highways, and a
              variety of other engineered socio-technical systems, we must also
              take into account the safety of systems involving machine
              learning. Heretofore, the definition of safety has not been
              formalized in a machine learning context. In this paper, we do so
              by defining machine learning safety in terms of risk, epistemic
              uncertainty, and the harm incurred by unwanted outcomes. We then
              use this definition to examine safety in all sorts of
              applications in cyber-physical systems, decision sciences, and
              data products. We find that the foundational principle of modern
              statistical machine learning, empirical risk minimization, is not
              always a sufficient objective. Finally, we discuss how four
              different categories of strategies for achieving safety in
              engineering, including inherently safe design, safety reserves,
              safe fail, and procedural safeguards can be mapped to a machine
              learning context. We then discuss example techniques that can be
              adopted in each category, such as considering interpretability
              and causality of predictive models, objective functions beyond
              expected prediction accuracy, human involvement for labeling
              difficult or rare examples, and user experience design of
              software and open data.",
  journal  = "arXiv [cs.CY]",
  month    =  oct,
  year     =  2016,
  keywords = "risk\_safety\_nonstationary;Safety\_AI;NotRead;Assurances"
}

@MISC{Gunning2016-kb,
  title        = "Explainable Artificial Intelligence",
  author       = "Gunning, David",
  year         =  2016,
  howpublished = "\url{https://www.darpa.mil/program/explainable-artificial-intelligence}",
  note         = "Accessed: 2017-9-1",
  keywords     = "Assurances"
}

@MISC{Neema2017-bb,
  title        = "Assured Autonomy",
  author       = "Neema, Sandeep",
  year         =  2017,
  howpublished = "\url{https://www.darpa.mil/program/assured-autonomy}",
  note         = "Accessed: 2017-9-1",
  keywords     = "Assurances"
}

@TECHREPORT{Raman2013-mz,
  title       = "Sorry Dave, I'm Afraid {I} Can't Do That: Explaining
                 Unachievable Robot Tasks Using Natural Language",
  author      = "Raman, Vasumathi and Lignos, Constantine and Finucane, Cameron
                 and Lee, Kenton C and Marcus, Mitch and Kress-Gazit, Hadas",
  institution = "University of Pennsylvania Philadelphia United States",
  year        =  2013,
  keywords    = "V\&V;assurance\_implicit;trust\_informal\_treatment;Integral
                 Assurance;Value Alignment;Assurances"
}

@INPROCEEDINGS{Szafir2015-iy,
  title     = "Communicating Directionality in Flying Robots",
  booktitle = "Proceedings of the Tenth Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Szafir, Daniel and Mutlu, Bilge and Fong, Terry",
  publisher = "ACM",
  pages     = "19--26",
  series    = "HRI '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "design, free-flyers, micro air vehicles (MAVs), robot
               intent;Integral Assurance;Human-like Behavior;Assurances"
}

@ARTICLE{Gunning2017-ih,
  title     = "Explainable artificial intelligence (xai)",
  author    = "Gunning, David",
  abstract  = "Dramatic success in machine learning has led to an explosion of
               AI applications. Researchers have developed new AI capabilities
               for a wide variety of tasks. Continued advances promise to
               produce autonomous systems that will perceive, learn, decide,
               and act",
  journal   = "Defense Advanced Research Projects Agency (DARPA), nd Web",
  publisher = "cc.gatech.edu",
  year      =  2017,
  keywords  = "explain;Assurances"
}

@ARTICLE{Wagner2016-ck,
  title     = "National Legislation within the Framework of the {GDPR}",
  author    = "Wagner, Julian and Benecke, Alexander",
  abstract  = "After several years of deliberation, the European Union has
               adopted the General Data Protection Regulation. Considering that
               the Regulation is directly applicable in all Member States after
               coming into effect, the General Data Protection Regulation (
               GDPR ) will have an",
  journal   = "Eur. Data Prot. L. Rev.",
  publisher = "HeinOnline",
  volume    =  2,
  pages     = "353",
  year      =  2016,
  keywords  = "NotRead;trust\_public\_conversation;Assurances"
}

@ARTICLE{Tankard2016-rk,
  title    = "What the {GDPR} means for businesses",
  author   = "Tankard, Colin",
  abstract = "The long-awaited General Data Protection Regulation (GDPR) of the
              EU was provisionally agreed in December 2015.1 The final details
              are still being ironed out, but publication of the final version
              of the regulation is expected around July 2016.2 There will then
              be a two-year waiting period until every organisation that does
              business in, or with, the EU must comply with the regulation.
              Since it is a regulation, not a directive, compliance is
              mandatory, without the need for each member state to ratify it
              into its own legislation. The GDPR expands the scope of data
              protection so that it applies to anyone or any organisation that
              collects and processes information related to EU citizens, no
              matter where they are based or where the data is stored. Colin
              Tankard of Digital Pathways examines what effect the new
              regulation is likely to have on organisations.",
  journal  = "Network Security",
  volume   =  2016,
  number   =  6,
  pages    = "5--8",
  month    =  jun,
  year     =  2016,
  keywords = "NotRead;trust\_corporate\_conversation;Assurances"
}

@ARTICLE{Lakkaraju2016-gb,
  title    = "Interpretable Decision Sets: A Joint Framework for Description
              and Prediction",
  author   = "Lakkaraju, Himabindu and Bach, Stephen H and Jure, Leskovec",
  abstract = "One of the most important obstacles to deploying predictive
              models is the fact that humans do not understand and trust them.
              Knowing which variables are important in a model's prediction and
              how they are combined can be very powerful in helping people
              understand and trust automatic decision making systems. Here we
              propose interpretable decision sets, a framework for building
              predictive models that are highly accurate, yet also highly
              interpretable. Decision sets are sets of independent if-then
              rules. Because each rule can be applied independently, decision
              sets are simple, concise, and easily interpretable. We formalize
              decision set learning through an objective function that
              simultaneously optimizes accuracy and interpretability of the
              rules. In particular, our approach learns short, accurate, and
              non-overlapping rules that cover the whole feature space and pay
              attention to small but important classes. Moreover, we prove that
              our objective is a non-monotone submodular function, which we
              efficiently optimize to find a near-optimal set of rules.
              Experiments show that interpretable decision sets are as accurate
              at classification as state-of-the-art machine learning
              techniques. They are also three times smaller on average than
              rule-based models learned by other methods. Finally, results of a
              user study show that people are able to answer multiple-choice
              questions about the decision boundaries of interpretable decision
              sets and write descriptions of classes based on them faster and
              more accurately than with other rule-based models that were
              designed for interpretability. Overall, our framework provides a
              new approach to interpretable machine learning that balances
              accuracy, interpretability, and computational efficiency.",
  journal  = "KDD",
  volume   =  2016,
  pages    = "1675--1684",
  month    =  aug,
  year     =  2016,
  language = "en"
}

@ARTICLE{Phillips2017-ho,
  title         = "Interpretable Active Learning",
  author        = "Phillips, Richard L and Chang, Kyu Hyun and Friedler,
                   Sorelle A",
  abstract      = "Active learning has long been a topic of study in machine
                   learning. However, as increasingly complex and opaque models
                   have become standard practice, the process of active
                   learning, too, has become more opaque. There has been little
                   investigation into interpreting what specific trends and
                   patterns an active learning strategy may be exploring. This
                   work expands on the Local Interpretable Model-agnostic
                   Explanations framework (LIME) to provide explanations for
                   active learning recommendations. We demonstrate how LIME can
                   be used to generate locally faithful explanations for an
                   active learning strategy, and how these explanations can be
                   used to understand how different models and datasets explore
                   a problem space over time. In order to quantify the
                   per-subgroup differences in how an active learning strategy
                   queries spatial regions, we introduce a notion of
                   uncertainty bias (based on disparate impact) to measure the
                   discrepancy in the confidence for a model's predictions
                   between one subgroup and another. Using the uncertainty bias
                   measure, we show that our query explanations accurately
                   reflect the subgroup focus of the active learning queries,
                   allowing for an interpretable explanation of what is being
                   learned as points with similar sources of uncertainty have
                   their uncertainty bias resolved. We demonstrate that this
                   technique can be applied to track uncertainty bias over
                   user-defined clusters or automatically generated clusters
                   based on the source of uncertainty.",
  month         =  jul,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1708.00049"
}

@INPROCEEDINGS{McAllister2017-zn,
  title     = "Concrete problems for autonomous vehicle safety: Advantages of
               Bayesian deep learning",
  booktitle = "International Joint Conference on Artificial Intelligence
               ({IJCAI})",
  author    = "McAllister, Rowan and Gal, Yarin and Kendall, Alex and van der
               Wilk, Mark and Shah, Amar and Cipolla, Roberto and Weller,
               Adrian",
  year      =  2017,
  keywords  = "NotRead;reading\_list;Reading List"
}

@ARTICLE{Acquisti2017-hf,
  title     = "Nudges for Privacy and Security: Understanding and Assisting
               {Users\&Rsquo}; Choices Online",
  author    = "Acquisti, Alessandro and Adjerid, Idris and Balebako, Rebecca
               and Brandimarte, Laura and Cranor, Lorrie Faith and Komanduri,
               Saranga and Leon, Pedro Giovanni and Sadeh, Norman and Schaub,
               Florian and Sleeper, Manya and Wang, Yang and Wilson, Shomir",
  journal   = "ACM Comput. Surv.",
  publisher = "ACM",
  volume    =  50,
  number    =  3,
  pages     = "44:1--44:41",
  month     =  aug,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "Privacy, behavioral economics, nudge, security, soft
               paternalism;NotRead;reading\_list;Reading List;COHRINT Reading
               List -- Brett"
}

@ARTICLE{Gerris2010-gz,
  title     = "{Big-Five} personality factors and interpersonal trust in
               established marriages",
  author    = "Gerris, Jan R M and Delsing, Marc J M H and Oud, Johan H L",
  abstract  = "The present study investigated the effects of husbands' and
               wives' Big-Five personality characteristics on their own (i.e.,
               actor effects) as well as on their partner's (i.e., partner
               effects) degree of trust in one another. Data were collected
               from 288 husband?wife couples with at least two adolescent
               children. Both self-reported and partner-reported personality
               characteristics were analyzed using two complementary
               approaches: hierarchical regression analyses and Actor-Partner
               Interdependence Model (APIM) analyses. Results were dependent on
               the type of personality rating used. Actor effects and partner
               effects were inflated when using self-reports and
               partner-reports, respectively. Controlling for shared rater
               variance, our findings suggest that trust should be conceived as
               a property of the dyad, rather than as an individual
               characteristic of the dyad members. Conscientiousness,
               extraversion, and openness emerged as the most important
               predictors of dyadic trust. Husbands' and wives' personality
               characteristics contributed equally strongly to dyadic trust.",
  journal   = "Fam. Sci.",
  publisher = "Routledge",
  volume    =  1,
  number    =  1,
  pages     = "48--62",
  month     =  feb,
  year      =  2010
}

@BOOK{Van_der_Pool2003-ed,
  title     = "The Apostolic Bible Polyglot -- Old Testament",
  publisher = "The Apostolic Press",
  edition   = "alpha",
  year      =  2003,
  keywords  = "Religious"
}

@BOOK{Van_der_Pool2003-li,
  title     = "The Apostolic Bible Polyglot -- New Testament",
  publisher = "The Apostolic Press",
  edition   = "alpha",
  year      =  2003,
  keywords  = "Religious"
}

@ARTICLE{Zhang2017-tn,
  title         = "{MDNet}: A Semantically and Visually Interpretable Medical
                   Image Diagnosis Network",
  author        = "Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and McGough,
                   Mason and Yang, Lin",
  abstract      = "The inability to interpret the model prediction in
                   semantically and visually meaningful ways is a well-known
                   shortcoming of most existing computer-aided diagnosis
                   methods. In this paper, we propose MDNet to establish a
                   direct multimodal mapping between medical images and
                   diagnostic reports that can read images, generate diagnostic
                   reports, retrieve images by symptom descriptions, and
                   visualize attention, to provide justifications of the
                   network diagnosis process. MDNet includes an image model and
                   a language model. The image model is proposed to enhance
                   multi-scale feature ensembles and utilization efficiency.
                   The language model, integrated with our improved attention
                   mechanism, aims to read and explore discriminative image
                   feature descriptions from reports to learn a direct mapping
                   from sentence words to image pixels. The overall network is
                   trained end-to-end by using our developed optimization
                   strategy. Based on a pathology bladder cancer images and its
                   diagnostic reports (BCIDR) dataset, we conduct sufficient
                   experiments to demonstrate that MDNet outperforms
                   comparative baselines. The proposed image model obtains
                   state-of-the-art performance on two CIFAR datasets as well.",
  month         =  jul,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1707.02485"
}

@ARTICLE{Raissi2017-pi,
  title         = "Hidden Physics Models: Machine Learning of Nonlinear Partial
                   Differential Equations",
  author        = "Raissi, Maziar and Karniadakis, George Em",
  abstract      = "We introduce the concept of hidden physics models, which are
                   essentially data-efficient learning machines capable of
                   leveraging the underlying laws of physics, expressed by time
                   dependent and nonlinear partial differential equations, to
                   extract patterns from high-dimensional data generated from
                   experiments. The proposed technology is applied to the
                   problem of learning, system identification, or data-driven
                   discovery of partial differential equations. The framework
                   relies on Gaussian processes, a powerful tool for
                   probabilistic inference over functions, to strike a balance
                   between model complexity and data fit. The effectiveness of
                   the proposed approach is demonstrated through a variety of
                   canonical problems, spanning a number of scientific domains,
                   including the Navier-Stokes,
                   Schr\textbackslash``\{o\}dinger, Kuramoto-Sivashinsky, and
                   fractional equations. The methodology provides a promising
                   new direction for capitalizing on the long-standing
                   developments of classical methods in applied mathematics and
                   mathematical physics to design learning machines with the
                   ability to learn in complex domains without requiring large
                   quantities of data.",
  month         =  aug,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1708.00588"
}

@ARTICLE{Koh2017-mf,
  title         = "Understanding Black-box Predictions via Influence Functions",
  author        = "Koh, Pang Wei and Liang, Percy",
  abstract      = "How can we explain the predictions of a black-box model? In
                   this paper, we use influence functions -- a classic
                   technique from robust statistics -- to trace a model's
                   prediction through the learning algorithm and back to its
                   training data, thereby identifying training points most
                   responsible for a given prediction. To scale up influence
                   functions to modern machine learning settings, we develop a
                   simple, efficient implementation that requires only oracle
                   access to gradients and Hessian-vector products. We show
                   that even on non-convex and non-differentiable models where
                   the theory breaks down, approximations to influence
                   functions can still provide valuable information. On linear
                   models and convolutional neural networks, we demonstrate
                   that influence functions are useful for multiple purposes:
                   understanding model behavior, debugging models, detecting
                   dataset errors, and even creating visually-indistinguishable
                   training-set attacks.",
  month         =  mar,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1703.04730"
}

@ARTICLE{Lakkaraju2017-uq,
  title         = "Interpretable \& Explorable Approximations of Black Box
                   Models",
  author        = "Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and
                   Leskovec, Jure",
  abstract      = "We propose Black Box Explanations through Transparent
                   Approximations (BETA), a novel model agnostic framework for
                   explaining the behavior of any black-box classifier by
                   simultaneously optimizing for fidelity to the original model
                   and interpretability of the explanation. To this end, we
                   develop a novel objective function which allows us to learn
                   (with optimality guarantees), a small number of compact
                   decision sets each of which explains the behavior of the
                   black box model in unambiguous, well-defined regions of
                   feature space. Furthermore, our framework also is capable of
                   accepting user input when generating these approximations,
                   thus allowing users to interactively explore how the
                   black-box model behaves in different subspaces that are of
                   interest to the user. To the best of our knowledge, this is
                   the first approach which can produce global explanations of
                   the behavior of any given black box model through joint
                   optimization of unambiguity, fidelity, and interpretability,
                   while also allowing users to explore model behavior based on
                   their preferences. Experimental evaluation with real-world
                   datasets and user studies demonstrates that our approach can
                   generate highly compact, easy-to-understand, yet accurate
                   approximations of various kinds of predictive models
                   compared to state-of-the-art baselines.",
  month         =  jul,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1707.01154"
}

@ARTICLE{Montavon2017-rd,
  title         = "Methods for Interpreting and Understanding Deep Neural
                   Networks",
  author        = "Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller,
                   Klaus-Robert",
  abstract      = "This paper provides an entry point to the problem of
                   interpreting a deep neural network model and explaining its
                   predictions. It is based on a tutorial given at ICASSP 2017.
                   It introduces some recently proposed techniques of
                   interpretation, along with theory, tricks and
                   recommendations, to make most efficient use of these
                   techniques on real data. It also discusses a number of
                   practical applications.",
  month         =  jun,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.07979"
}

@ARTICLE{Tolomei2017-cp,
  title         = "Interpretable Predictions of Tree-based Ensembles via
                   Actionable Feature Tweaking",
  author        = "Tolomei, Gabriele and Silvestri, Fabrizio and Haines, Andrew
                   and Lalmas, Mounia",
  abstract      = "Machine-learned models are often described as ``black
                   boxes''. In many real-world applications however, models may
                   have to sacrifice predictive power in favour of
                   human-interpretability. When this is the case, feature
                   engineering becomes a crucial task, which requires
                   significant and time-consuming human effort. Whilst some
                   features are inherently static, representing properties that
                   cannot be influenced (e.g., the age of an individual),
                   others capture characteristics that could be adjusted (e.g.,
                   the daily amount of carbohydrates taken). Nonetheless, once
                   a model is learned from the data, each prediction it makes
                   on new instances is irreversible - assuming every instance
                   to be a static point located in the chosen feature space.
                   There are many circumstances however where it is important
                   to understand (i) why a model outputs a certain prediction
                   on a given instance, (ii) which adjustable features of that
                   instance should be modified, and finally (iii) how to alter
                   such a prediction when the mutated instance is input back to
                   the model. In this paper, we present a technique that
                   exploits the internals of a tree-based ensemble classifier
                   to offer recommendations for transforming true negative
                   instances into positively predicted ones. We demonstrate the
                   validity of our approach using an online advertising
                   application. First, we design a Random Forest classifier
                   that effectively separates between two types of ads: low
                   (negative) and high (positive) quality ads (instances).
                   Then, we introduce an algorithm that provides
                   recommendations that aim to transform a low quality ad
                   (negative instance) into a high quality one (positive
                   instance). Finally, we evaluate our approach on a subset of
                   the active inventory of a large ad network, Yahoo Gemini.",
  month         =  jun,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1706.06691"
}

@ARTICLE{Ross2017-tp,
  title         = "Right for the Right Reasons: Training Differentiable Models
                   by Constraining their Explanations",
  author        = "Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez,
                   Finale",
  abstract      = "Neural networks are among the most accurate supervised
                   learning methods in use today, but their opacity makes them
                   difficult to trust in critical applications, especially when
                   conditions in training differ from those in test. Recent
                   work on explanations for black-box models has produced tools
                   (e.g. LIME) to show the implicit rules behind predictions,
                   which can help us identify when models are right for the
                   wrong reasons. However, these methods do not scale to
                   explaining entire datasets and cannot correct the problems
                   they reveal. We introduce a method for efficiently
                   explaining and regularizing differentiable models by
                   examining and selectively penalizing their input gradients,
                   which provide a normal to the decision boundary. We apply
                   these penalties both based on expert annotation and in an
                   unsupervised fashion that encourages diverse models with
                   qualitatively different decision boundaries for the same
                   classification problem. On multiple datasets, we show our
                   approach generates faithful explanations and models that
                   generalize much better when conditions differ between
                   training and test.",
  month         =  mar,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1703.03717"
}

@ARTICLE{Doshi-Velez2017-xy,
  title         = "Towards A Rigorous Science of Interpretable Machine Learning",
  author        = "Doshi-Velez, Finale and Kim, Been",
  abstract      = "As machine learning systems become ubiquitous, there has
                   been a surge of interest in interpretable machine learning:
                   systems that provide explanation for their outputs. These
                   explanations are often used to qualitatively assess other
                   criteria such as safety or non-discrimination. However,
                   despite the interest in interpretability, there is very
                   little consensus on what interpretable machine learning is
                   and how it should be measured. In this position paper, we
                   first define interpretability and describe when
                   interpretability is needed (and when it is not). Next, we
                   suggest a taxonomy for rigorous evaluation and expose open
                   questions towards a more rigorous science of interpretable
                   machine learning.",
  month         =  feb,
  year          =  2017,
  keywords      = "reading\_list;Interpretable Models;Integral
                   Assurance;Reading List;Assurances;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1702.08608"
}

@ARTICLE{noauthor_undated-hr,
  title    = "Programs as {Black-Box} Explanations",
  author   = "Singh, Sameer and Ribeiro, Marco Tulio and Guestrin, Carlos",
  abstract = "Recent work in model-agnostic explanations of black-box machine
              learning has demonstrated that interpretability of complex models
              does not have to come at the cost of accuracy or model
              flexibility. However, it is not clear what kind of explanations,
              such as linear models, decision trees, and rule lists, are the
              appropriate family to consider, and different tasks and models
              may benefit from different kinds of explanations. Instead of
              picking a single family of representations, in this work we
              propose to use ``programs'' as model-agnostic explanations. We
              show that small programs can be expressive yet intuitive as
              explanations, and generalize over a number of existing
              interpretable families. We propose a prototype program induction
              method based on simulated annealing that approximates the local
              behavior of black-box classifiers around a specific prediction
              using random perturbations. Finally, we present preliminary
              application on small datasets and show that the generated
              explanations are intuitive and accurate for a number of
              classifiers.",
  journal  = "arXiv [stat.ML]",
  month    =  nov,
  year     =  2016,
  keywords = "NotRead;reading\_list;Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{noauthor_undated-uv,
  title    = "{CASSL}: Curriculum Accelerated {Self-Supervised} Learning",
  author   = "Murali, Adithyavairavan and Pinto, Lerrel and Gandhi, Dhiraj and
              Gupta, Abhinav",
  abstract = "Recent self-supervised learning approaches focus on using a few
              thousand data points to learn policies for high-level,
              low-dimensional action spaces. However, scaling this framework
              for high-dimensional control require either scaling up the data
              collection efforts or using a clever sampling strategy for
              training. We present a novel approach - Curriculum Accelerated
              Self-Supervised Learning (CASSL) - to train policies that map
              visual information to high-level, higher- dimensional action
              spaces. CASSL orders the sampling of training data based on
              control dimensions: the learning and sampling are focused on few
              control parameters before other parameters. The right curriculum
              for learning is suggested by variance-based global sensitivity
              analysis of the control space. We apply our CASSL framework to
              learning how to grasp using an adaptive, underactuated
              multi-fingered gripper, a challenging system to control. Our
              experimental results indicate that CASSL provides significant
              improvement and generalization compared to baseline methods such
              as staged curriculum learning (8\% increase) and complete
              end-to-end learning with random exploration (14\% improvement)
              tested on a set of novel objects.",
  journal  = "arXiv [cs.RO]",
  month    =  aug,
  year     =  2017,
  keywords = "NotRead;reading\_list;Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Welke2017-xz,
  title         = "Brain Responses During {Robot-Error} Observation",
  author        = "Welke, Dominik and Behncke, Joos and Hader, Marina and
                   Schirrmeister, Robin Tibor and Sch{\"o}nau, Andreas and
                   E{\ss}mann, Boris and M{\"u}ller, Oliver and Burgard,
                   Wolfram and Ball, Tonio",
  abstract      = "Brain-controlled robots are a promising new type of
                   assistive device for severely impaired persons. Little is
                   however known about how to optimize the interaction of
                   humans and brain-controlled robots. Information about the
                   human's perceived correctness of robot performance might
                   provide a useful teaching signal for adaptive control
                   algorithms and thus help enhancing robot control. Here, we
                   studied whether watching robots perform erroneous vs.
                   correct action elicits differential brain responses that can
                   be decoded from single trials of electroencephalographic
                   (EEG) recordings, and whether brain activity during
                   human-robot interaction is modulated by the robot's visual
                   similarity to a human. To address these topics, we designed
                   two experiments. In experiment I, participants watched a
                   robot arm pour liquid into a cup. The robot performed the
                   action either erroneously or correctly, i.e. it either
                   spilled some liquid or not. In experiment II, participants
                   observed two different types of robots, humanoid and
                   non-humanoid, grabbing a ball. The robots either managed to
                   grab the ball or not. We recorded high-resolution EEG during
                   the observation tasks in both experiments to train a Filter
                   Bank Common Spatial Pattern (FBCSP) pipeline on the
                   multivariate EEG signal and decode for the correctness of
                   the observed action, and for the type of the observed robot.
                   Our findings show that it was possible to decode both
                   correctness and robot type for the majority of participants
                   significantly, although often just slightly, above chance
                   level. Our findings suggest that non-invasive recordings of
                   brain responses elicited when observing robots indeed
                   contain decodable information about the correctness of the
                   robot's action and the type of observed robot.",
  month         =  aug,
  year          =  2017,
  keywords      = "NotRead;reading\_list;Reading List;COHRINT Reading List --
                   Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "1708.01465"
}

@ARTICLE{Robinette2017-qg,
  title    = "Effect of Robot Performance on {Human-Robot} Trust in
              {Time-Critical} Situations",
  author   = "Robinette, P and Howard, A M and Wagner, A R",
  abstract = "Robots have the potential to save lives in high-risk situations,
              such as emergency evacuations. To realize this potential, we must
              understand how factors such as the robot's performance, the
              riskiness of the situation, and the evacuee's motivation
              influence his or her decision to follow a robot. In this paper,
              we developed a set of experiments that tasked individuals with
              navigating a virtual maze using different methods to simulate an
              evacuation. Participants chose whether or not to use the robot
              for guidance in each of two separate navigation rounds. The robot
              performed poorly in two of the three conditions. The
              participant's decision to use the robot and self-reported trust
              in the robot served as dependent measures. A 53\% drop in
              self-reported trust was found when the robot performs poorly.
              Self-reports of trust were strongly correlated with the decision
              to use the robot for guidance ( $\phi (90) = + 0.745$). We
              conclude that a mistake made by a robot will cause a person to
              have a significantly lower level of trust in it in later
              interactions.",
  journal  = "IEEE Transactions on Human-Machine Systems",
  volume   =  47,
  number   =  4,
  pages    = "425--436",
  month    =  aug,
  year     =  2017,
  keywords = "Atmospheric measurements;Educational robots;Navigation;Particle
              measurements;Reliability;Time factors;Cooperative
              systems;emergency guidance robot;human--robot
              interaction;human--robot trust;Reading List;COHRINT Reading List
              -- Brett"
}

@ARTICLE{Scheggi2017-hm,
  title    = "Cooperative Navigation for Mixed Human \#x2013;Robot Teams Using
              Haptic Feedback",
  author   = "Scheggi, S and Aggravi, M and Prattichizzo, D",
  abstract = "In this paper, we present a novel cooperative navigation control
              for human--robot teams. Assuming that a human wants to reach a
              final location in a large environment with the help of a mobile
              robot, the robot must steer the human from the initial to the
              target position. The challenges posed by cooperative human--robot
              navigation are typically addressed by using haptic feedback via
              physical interaction. In contrast with that, in this paper, we
              describe a different approach, in which the human--robot
              interaction is achieved via wearable vibrotactile armbands. In
              the proposed work, the subject is free to decide her/his own
              pace. A warning vibrational signal is generated by the haptic
              armbands when a large deviation with respect to the desired pose
              is detected by the robot. The proposed method has been evaluated
              in a large indoor environment, where 15 blindfolded human
              subjects were asked to follow the haptic cues provided by the
              robot. The participants had to reach a target area, while
              avoiding static and dynamic obstacles. Experimental results
              revealed that the blindfolded subjects were able to avoid the
              obstacles and safely reach the target in all of the performed
              trials. A comparison is provided between the results obtained
              with blindfolded users and experiments performed with sighted
              people.",
  journal  = "IEEE Transactions on Human-Machine Systems",
  volume   =  47,
  number   =  4,
  pages    = "462--473",
  month    =  aug,
  year     =  2017,
  keywords = "Haptic interfaces;Legged locomotion;Navigation;Robot sensing
              systems;Visualization;Autonomous vehicles;formation
              control;haptic feedback;human body tracking;human--robot
              interaction;human--robot team;psychophysics"
}

@ARTICLE{Tripp2011-rx,
  title     = "Degrees of Humanness in Technology: What Type of Trust Matters?",
  author    = "Tripp, J and McKnight, D H and Lankton, N K",
  abstract  = "The literature regarding trust in information technology (IT)
               contexts shows compelling evidence that trust predicts
               technology usage intention. This literature has typically
               examined trust in the e-commerce environment, focusing on trust
               in an e-vendor or online",
  journal   = "AMCIS",
  publisher = "works.bepress.com",
  year      =  2011,
  keywords  = "
               trust\_in\_technology;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_implicit;in\_paper;Integral
               Assurance;Human-like Behavior;Assurances"
}

@ARTICLE{Hassabis2017-bn,
  title    = "{Neuroscience-Inspired} Artificial Intelligence",
  author   = "Hassabis, Demis and Kumaran, Dharshan and Summerfield,
              Christopher and Botvinick, Matthew",
  abstract = "The fields of neuroscience and artificial intelligence (AI) have
              a long and intertwined history. In more recent times, however,
              communication and collaboration between the two fields has become
              less commonplace. In this article, we argue that better
              understanding biological brains could play a vital role in
              building intelligent machines. We survey historical interactions
              between the AI and neuroscience fields and emphasize current
              advances in AI that have been inspired by the study of neural
              computation in humans and other animals. We conclude by
              highlighting shared themes that may be key for advancing future
              research in both fields.",
  journal  = "Neuron",
  volume   =  95,
  number   =  2,
  pages    = "245--258",
  month    =  jul,
  year     =  2017,
  keywords = "artificial intelligence; brain; cognition; learning; neural
              network",
  language = "en"
}

@MISC{Krauss_undated-jm,
  title        = "Bikes May Have To Talk To {Self-Driving} Cars For Safety's
                  Sake",
  booktitle    = "{NPR.org}",
  author       = "Krauss, Margaret J",
  abstract     = "Autonomous vehicles have gotten pretty good at detecting
                  other cars and pedestrians. But ``seeing'' bikes and
                  predicting what they'll do is still a challenge. The answer
                  may lie in bikes themselves.",
  howpublished = "\url{http://www.npr.org/sections/alltechconsidered/2017/07/24/537746346/bikes-may-have-to-talk-to-self-driving-cars-for-safetys-sake?utm_medium=RSS&utm_campaign=technology}",
  note         = "Accessed: 2017-7-25",
  keywords     = "assurances;Assurances"
}

@ARTICLE{Israelsen2017-pv,
  title         = "Adaptive Simulation-based Training of {AI} Decision-makers
                   using Bayesian Optimization",
  author        = "Israelsen, Brett W and Ahmed, Nisar and Center, Kenneth and
                   Green, Roderick and Bennett, Jr, Winston",
  abstract      = "This work studies how an AI-controlled dog-fighting agent
                   with tunable decision-making parameters can learn to
                   optimize performance against an intelligent adversary, as
                   measured by a stochastic objective function evaluated on
                   simulated combat engagements. Gaussian process Bayesian
                   optimization (GPBO) techniques are developed to
                   automatically learn global Gaussian Process (GP) surrogate
                   models, which provide statistical performance predictions in
                   both explored and unexplored areas of the parameter space.
                   This allows a learning engine to sample full-combat
                   simulations at parameter values that are most likely to
                   optimize performance and also provide highly informative
                   data points for improving future predictions. However,
                   standard GPBO methods do not provide a reliable surrogate
                   model for the highly volatile objective functions found in
                   aerial combat, and thus do not reliably identify global
                   maxima. These issues are addressed by novel Repeat Sampling
                   (RS) and Hybrid Repeat/Multi-point Sampling (HRMS)
                   techniques. Simulation studies show that HRMS improves the
                   accuracy of GP surrogate models, allowing AI decision-makers
                   to more accurately predict performance and efficiently tune
                   parameters.",
  month         =  mar,
  year          =  2017,
  keywords      = "My Papers;myPapers",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1703.09310"
}

@BOOK{Bostrom2014-fz,
  title     = "Superintelligence: Paths, Dangers, Strategies",
  author    = "Bostrom, Nick",
  abstract  = "The human brain has some capabilities that the brains of other
               animals lack. It is to these distinctive capabilities that our
               species owes its dominant position. Other animals have stronger
               muscles or sharper claws, but we have cleverer brains. If
               machine brains one day come to surpass human brains in general
               intelligence, then this new superintelligence could become very
               powerful. As the fate of the gorillas now depends more on us
               humans than on the gorillas themselves, so the fate of our
               species then would come to depend on the actions of the machine
               superintelligence. But we have one advantage: we get to make the
               first move. Will it be possible to construct a seed AI or
               otherwise to engineer initial conditions so as to make an
               intelligence explosion survivable? How could one achieve a
               controlled detonation? To get closer to an answer to this
               question, we must make our way through a fascinating landscape
               of topics and considerations. Read the book and learn about
               oracles, genies, singletons; about boxing methods, tripwires,
               and mind crime; about humanity's cosmic endowment and
               differential technological development; indirect normativity,
               instrumental convergence, whole brain emulation and technology
               couplings; Malthusian economics and dystopian evolution;
               artificial intelligence, and biological cognitive enhancement,
               and collective intelligence. This profoundly ambitious and
               original book picks its way carefully through a vast tract of
               forbiddingly difficult intellectual terrain. Yet the writing is
               so lucid that it somehow makes it all seem easy. After an
               utterly engrossing journey that takes us to the frontiers of
               thinking about the human condition and the future of intelligent
               life, we find in Nick Bostrom's work nothing less than a
               reconceptualization of the essential task of our time.",
  publisher = "OUP Oxford",
  month     =  jul,
  year      =  2014,
  keywords  = "Integral Assurance;Value Alignment;Assurances",
  language  = "en"
}

@ARTICLE{Kress-Gazit2009-wf,
  title    = "{Temporal-Logic-Based} Reactive Mission and Motion Planning",
  author   = "Kress-Gazit, H and Fainekos, G E and Pappas, G J",
  abstract = "This paper provides a framework to automatically generate a
              hybrid controller that guarantees that the robot can achieve its
              task when a robot model, a class of admissible environments, and
              a high-level task or behavior for the robot are provided. The
              desired task specifications, which are expressed in a fragment of
              linear temporal logic (LTL), can capture complex robot behaviors
              such as search and rescue, coverage, and collision avoidance. In
              addition, our framework explicitly captures sensor specifications
              that depend on the environment with which the robot is
              interacting, which results in a novel paradigm for sensor-based
              temporal-logic-motion planning. As one robot is part of the
              environment of another robot, our sensor-based framework very
              naturally captures multirobot specifications in a decentralized
              manner. Our computational approach is based on first creating
              discrete controllers satisfying specific LTL formulas. If
              feasible, the discrete controller is then used to guide the
              sensor-based composition of continuous controllers, which results
              in a hybrid controller satisfying the high-level specification
              but only if the environment is admissible.",
  journal  = "IEEE Trans. Rob.",
  volume   =  25,
  number   =  6,
  pages    = "1370--1381",
  month    =  dec,
  year     =  2009,
  keywords = "decentralised control;discrete systems;path
              planning;robots;temporal logic;discrete controllers;linear
              temporal logic;multirobot specifications;robot model;sensor-based
              temporal-logic-motion planning;task
              specifications;temporal-logic-based reactive mission;Controller
              synthesis;hybrid control;motion planning;sensor-based
              planning;temporal
              logic;assurance\_implicit;trust\_informal\_treatment;Assurances"
}

@INPROCEEDINGS{Chang2017-kl,
  title     = "Revolt: Collaborative Crowdsourcing for Labeling Machine
               Learning Datasets",
  booktitle = "Proceedings of the 2017 Conference on Human Factors in Computing
               Systems",
  author    = "Chang, Joseph Chee and Amershi, Saleema and Kamar, Ece",
  abstract  = "Abstract Crowdsourcing provides a scalable and efficient way to
               construct labeled datasets for training machine learning
               systems. However, creating comprehensive label guidelines for
               crowdworkers is often prohibitive even for seemingly simple
               concepts.",
  publisher = "ACM",
  pages     = "2334--2346",
  series    = "CHI '17",
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "collaboration, crowdsourcing, machine learning,
               real-time;Integral Assurance;Value Alignment;Assurances"
}

@INPROCEEDINGS{Kamar2012-gw,
  title     = "Combining Human and Machine Intelligence in Large-scale
               Crowdsourcing",
  booktitle = "Proceedings of the 11th International Conference on Autonomous
               Agents and Multiagent Systems - Volume 1",
  author    = "Kamar, Ece and Hacker, Severin and Horvitz, Eric",
  abstract  = "Abstract We show how machine learning and inference can be
               harnessed to leverage the complementary strengths of humans and
               computational agents to solve crowdsourcing tasks. We construct
               a set of Bayesian predictive models from data and describe how
               the models operate within an overall crowd-sourcing architecture
               that combines the efforts of people and machine vision on the
               task of classifying celestial bodies defined within a ...",
  publisher = "International Foundation for Autonomous Agents and Multiagent
               Systems",
  pages     = "467--474",
  series    = "AAMAS '12",
  year      =  2012,
  address   = "Richland, SC",
  keywords  = "complementary computing, consensus tasks, crowdsourcing,
               decision-theoretic reasoning;Assurances"
}

@ARTICLE{Endsley1995-ie,
  title     = "Toward a Theory of Situation Awareness in Dynamic Systems",
  author    = "Endsley, Mica R",
  abstract  = "This paper presents a theoretical model of situation awareness
               based on its role in dynamic human decision making in a variety
               of domains. Situation awareness is presented as a predominant
               concern in system operation, based on a descriptive view of
               decision making. The relationship between situation awareness
               and numerous individual and environmental factors is explored.
               Among these factors, attention and working memory are presented
               as critical factors limiting operators from acquiring and
               interpreting information from the environment to form situation
               awareness, and mental models and goal-directed behavior are
               hypothesized as important mechanisms for overcoming these
               limits. The impact of design features, workload, stress, system
               complexity, and automation on operator situation awareness is
               addressed, and a taxonomy of errors in situation awareness is
               introduced, based on the model presented. The model is used to
               generate design implications for enhancing operator situation
               awareness and future directions for situation awareness
               research.",
  journal   = "Hum. Factors",
  publisher = "SAGE Publications Inc",
  volume    =  37,
  number    =  1,
  pages     = "32--64",
  month     =  mar,
  year      =  1995,
  keywords  = "Assurances"
}

@INPROCEEDINGS{Kingston2012-va,
  title     = "Intruder tracking using {UAV} teams and ground sensor networks",
  booktitle = "German Aviation and Aerospace Congress ({DLRK} 2012)",
  author    = "Kingston, D",
  year      =  2012
}

@ARTICLE{Kuhn1997-qc,
  title    = "Communicating Uncertainty: Framing Effects on Responses to Vague
              Probabilities",
  author   = "Kuhn, Kristine M",
  abstract = "Most real-world risky decisions are based on imprecise
              probabilities. Although people generally demonstrate vagueness
              aversion, behaving as if vaguely specified probabilities are
              worse than comparable precisely specified probabilities,
              vagueness seeking also occurs. Previous explanations of vagueness
              preferences have been based on individual differences and
              regressive beliefs about extreme probabilities, and little
              research has examined the effect of changes in the way vagueness
              is communicated to the decision maker. The present study
              demonstrates that gain/loss framing, moderated by the
              operationalization of vagueness, influences how people respond to
              vagueness about a probability estimate. Subjects read scenarios
              describing consumer purchases, organizational marketing
              decisions, and medical treatments, and expressed preference
              between options having either precisely or vaguely described
              probabilities. Vagueness was operationalized either as a range of
              possible values or as verbal qualification of a single point
              estimate. Negative framing was associated with greater preference
              for vague prospects, unless vagueness was described by a
              numerical range with the higher value presented first, indicating
              a substantial primacy order effect. A second experiment
              demonstrates that negative framing led people to make more
              favorable inferences about the likelihoods of vague
              probabilities.",
  journal  = "Organ. Behav. Hum. Decis. Process.",
  volume   =  71,
  number   =  1,
  pages    = "55--83",
  month    =  jul,
  year     =  1997,
  keywords = "Supplemental Assurance;Reduce Complexity;Assurances"
}

@ARTICLE{Montavon2017-qu,
  title    = "Explaining nonlinear classification decisions with deep Taylor
              decomposition",
  author   = "Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder,
              Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert",
  abstract = "Abstract Nonlinear methods such as Deep Neural Networks (DNNs)
              are the gold standard for various challenging machine learning
              problems such as image recognition. Although these methods
              perform impressively well, they have a significant disadvantage,
              the lack of transparency, limiting the interpretability of the
              solution and thus the scope of application in practice.
              Especially DNNs act as black boxes due to their multilayer
              nonlinear structure. In this paper we introduce a novel
              methodology for interpreting generic multilayer neural networks
              by decomposing the network classification decision into
              contributions of its input elements. Although our focus is on
              image classification, the method is applicable to a broad set of
              input data, learning tasks and network architectures. Our method
              called deep Taylor decomposition efficiently utilizes the
              structure of the network by backpropagating the explanations from
              the output to the input layer. We evaluate the proposed method
              empirically on the MNIST and ILSVRC data sets.",
  journal  = "Pattern Recognit.",
  volume   =  65,
  pages    = "211--222",
  year     =  2017,
  keywords = "Deep neural networks; Heatmapping; Taylor decomposition;
              Relevance propagation; Image recognition;explain;Assurances"
}

@INPROCEEDINGS{Wang2015-ww,
  title      = "Falling Rule Lists",
  booktitle  = "Artificial Intelligence and Statistics",
  author     = "Wang, Fulton and Rudin, Cynthia",
  abstract   = "Falling rule lists are classification models consisting of an
                ordered list of if-then rules, where (i) the order of rules
                determines which example should be classified by each rule, and
                (ii) the estimated probability of success decreases
                monotonically down the list. These kinds of rule lists are
                inspired by healthcare applications where patients would be
                stratified into risk sets and the highest at-risk patients
                should be considered first. We provide a Bayesian framework for
                learning falling rule lists that does not rely on traditional
                greedy decision tree learning methods.",
  pages      = "1013--1022",
  month      =  feb,
  year       =  2015,
  keywords   = "trust\_informal\_treatment;assurance\_explicit;interp\_models;Assurances",
  language   = "en",
  conference = "Artificial Intelligence and Statistics"
}

@INPROCEEDINGS{Szafir2014-ok,
  title     = "Communication of Intent in Assistive Free Flyers",
  booktitle = "Proceedings of the 2014 {ACM/IEEE} International Conference on
               Human-robot Interaction",
  author    = "Szafir, Daniel and Mutlu, Bilge and Fong, Terrence",
  publisher = "ACM",
  pages     = "358--365",
  series    = "HRI '14",
  year      =  2014,
  address   = "New York, NY, USA",
  keywords  = "assistive free-flyer (aff), design, flying robot, human factors,
               intent, micro air vehicle (mav), motion, usability;Integral
               Assurance;Human-like Behavior;Assurances"
}

@ARTICLE{Lacher2014-yc,
  title     = "Autonomy, trust, and transportation",
  author    = "Lacher, A and Grabowski, R and Cook, S",
  abstract  = "Abstract Automation in transportation (rail, air, road, etc.) is
               becoming increasingly complex and interconnected. Ensuring that
               these sophisticated non-deterministic software systems can be
               trusted and remain resilient is a community concern. As
               technology evolves, systems",
  journal   = "2014 AAAI Spring Symposium Series",
  publisher = "aaai.org",
  year      =  2014,
  keywords  = "trust\_formal\_treatment;Assurances"
}

@ARTICLE{Yamagishi2005-cp,
  title     = "Separating trust from cooperation in a dynamic relationship
               prisoner's dilemma with variable dependence",
  author    = "Yamagishi, Toshio and Kanazawa, Satoshi and Mashima, Rie and
               Terai, Shigeru",
  journal   = "Ration. Soc.",
  publisher = "Sage Publications",
  volume    =  17,
  number    =  3,
  pages     = "275--308",
  year      =  2005,
  keywords  = "NotRead;Assurances"
}

@INPROCEEDINGS{Ghosh2016-dl,
  title      = "Trusted Machine Learning for Probabilistic Models",
  author     = "Ghosh, Shalini and Lincoln, Patrick and Tiwari, Ashish and Zhu,
                Xiaojin",
  year       =  2016,
  keywords   = "
                trust\_informal\_treatment;assurance\_explicit;model\_check;Integral
                Assurance;Value Alignment;Assurances",
  conference = "In ICML Workshop on Reliable Machine Learning In the Wild
                (WildML)"
}

@ARTICLE{Wickens1999-la,
  title     = "Unreliable Automated Attention Cueing for {Air-Ground} Targeting
               and Traffic Maneuvering",
  author    = "Wickens, Christopher D and Conejo, Rena and Gempler, Keith",
  abstract  = "We report two experiments in which pilots' attention is
               occasionally directed to inappropriate or inaccurate locations
               in space, replicating the effects of imperfect automation. A
               general taxonomy of human performance costs in these situations
               is presented. In Experiment 1, pilots are engaged in an
               air-ground targeting scenario. Target cueing, based upon
               semi-reliable sensor information, sometimes directs attention
               away from the true target. Yet pilots follow such guidance, even
               knowing its unreliability, a result of the difficulty of the
               unaided task. In Experiment 2, pilots in a free flight
               simulation are engaged in a series of traffic conflict avoidance
               maneuvers, using a cockpit display of traffic information
               (CDTI). On rare trials the CDTI knowledge of the traffic
               intruder's intentions, reflected in a predictor symbol, is
               unreliable and does not correspond with the actual aircraft
               behavior. Yet pilots' avoidance behavior is governed by the
               predictor symbol, and a display manipulation that calls
               attention to the inaccuracy of the predictor does little to
               influence pilots' reliance upon the predictor symbol although it
               does reduce visual workload. The data are interpreted in terms
               of appropriate trust calibration.",
  journal   = "Proc. Hum. Fact. Ergon. Soc. Annu. Meet.",
  publisher = "SAGE Publications",
  volume    =  43,
  number    =  1,
  pages     = "21--25",
  month     =  sep,
  year      =  1999,
  keywords  = "
               human\_study;trust\_formal\_treatment;assurance\_implicit;assurance\_structural;assurance\_normality;in\_paper;Supplemental
               Assurance;User Observation;Assurances"
}

@MISC{Benioff2016-tc,
  title        = "The {AI} revolution is coming fast. But without a revolution
                  in trust, it will fail",
  booktitle    = "World Economic Forum",
  author       = "Benioff, Mark",
  abstract     = "Before we can benefit from any of the latest innovations and
                  technologies, we need a revolution in trust.",
  month        =  aug,
  year         =  2016,
  howpublished = "\url{https://www.weforum.org/agenda/2016/08/the-digital-revolution-is-here-but-without-a-revolution-in-trust-it-will-fail/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Rudnitsky2017-in,
  title        = "How Can We Trust {AI} Companies? -- Contextual Wisdom --- The
                  Official Neura Blog -- Medium",
  booktitle    = "Medium",
  author       = "Rudnitsky, Fade",
  abstract     = "When building a company or a product, a lot of times you
                  happen to be at a crossroads, where you are tested against
                  your integrity and your commitment to become a company which
                  people can trust. If…",
  publisher    = "Contextual Wisdom --- The Official Neura Blog",
  month        =  may,
  year         =  2017,
  howpublished = "\url{https://medium.com/the-official-neura-blog/how-can-we-trust-ai-companies-28a6ac635e45}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

@MISC{Moody2017-vd,
  title        = "How {CHROs} can handle emerging {AI} and build company trust",
  booktitle    = "{HR} Dive",
  author       = "Moody, Kathryn",
  abstract     = "Artificial intelligence is on its way. Before full adoption,
                  CHROs can brace their departments for change.",
  month        =  mar,
  year         =  2017,
  howpublished = "\url{http://www.hrdive.com/news/how-chros-can-handle-emerging-ai-and-build-company-trust/438602/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

@MISC{Spectrum2016-jv,
  title        = "Can We Trust Robots?",
  booktitle    = "{IEEE} Spectrum: Technology, Engineering, and Science News",
  author       = "Spectrum, Ieee",
  abstract     = "Robots will soon have the power of life and death over human
                  beings. Are they ready? Are we?",
  publisher    = "IEEE Spectrum",
  month        =  may,
  year         =  2016,
  howpublished = "\url{http://spectrum.ieee.org/robotics/artificial-intelligence/can-we-trust-robots}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@MISC{Danks2017-sb,
  title        = "Can we trust self-driving cars?",
  booktitle    = "Salon",
  author       = "Danks, David",
  abstract     = "Trust is complex and varied, and so are the technologies in
                  question",
  month        =  jan,
  year         =  2017,
  howpublished = "\url{http://www.salon.com/2017/01/08/can-we-trust-self-driving-cars_partner/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Cassel2017-tn,
  title        = "The Big Question from 2016: Can We Trust Our Technologies?",
  booktitle    = "The New Stack",
  author       = "Cassel, David",
  abstract     = "As a new year begins, it's a good time to cast one last,
                  lingering look back at 2016. I've called this the ultimate
                  big data question: what just happened over the last 12
                  months? If every moment is its own container filled with
                  memories, here's an attempt to tease out some meaning from
                  that great …",
  month        =  jan,
  year         =  2017,
  howpublished = "\url{https://thenewstack.io/big-question-2016-can-trust-technologies/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@MISC{Cranz2017-yh,
  title        = "Why We Don't Fully Trust Technology",
  booktitle    = "Gizmodo Australia",
  author       = "Cranz, Alex and Elderkin, Beth",
  abstract     = "We've all been there. The presenter is about to begin, but
                  then disaster strikes: the computer technology fails. Perhaps
                  the computer has fallen asleep,...",
  month        =  jun,
  year         =  2017,
  howpublished = "\url{https://www.gizmodo.com.au/2015/10/why-we-dont-fully-trust-technology/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@MISC{DeSteno2014-cq,
  title        = "Can You Trust Technology?",
  booktitle    = "{HuffPost}",
  author       = "DeSteno, David",
  publisher    = "HuffPost",
  month        =  jan,
  year         =  2014,
  howpublished = "\url{http://www.huffingtonpost.com/david-desteno/can-you-trust-technology_b_4683614.html}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@PHDTHESIS{Ganjali2016-pb,
  title    = "Efficient Reinforcement Learning with Bayesian Optimization",
  author   = "Ganjali, Danyan",
  editor   = "Sideris, Athanasios",
  abstract = "A probabilistic reinforcement learning algorithm is presented for
              finding control policies in continuous state and action spaces
              without a prior knowledge of the dynamics. The objective of this
              algorithm is to learn from minimal amount of interaction with the
              environment in order to maximize a notion of reward, i.e. a
              numerical measure of the quality of the resulting state
              trajectories. Experience from the interactions are used to
              construct a set of probabilistic Gaussian process (GP) models
              that predict the resulting state trajectories and the reward from
              executing a policy on the system. These predictions are used with
              a technique known as Bayesian optimization to search for policies
              that promise higher rewards. As more experience is gathered,
              predictions are made with more confidence and the search for
              better policies relies less on new interactions with the
              environment. The computational demand of a GP makes it eventually
              impractical to use as the number of observations from interacting
              with the environment increase. Moreover, using a single GP to
              model different regions that may exhibit disparate behaviors can
              produce unsatisfactory representations and predictions. One way
              of mitigating these issues is by partitioning the observation
              points into different regions each represented by a local GP.
              With the sequential arrival of the observation points from new
              experiences, it is necessary to have an adaptive clustering
              method that can partition the data into an appropriate number of
              regions. This led to the development of EM+ algorithm presented
              in the second part of this work, which is an extension to the
              Expectation Maximization (EM) for the Gaussian mixture models,
              that assumes no prior knowledge of the number of components.
              Lastly, an application of the EM+ algorithm to filtering problems
              is presented. We propose a filtering algorithm that combines the
              advantages of the well-known particle filter and the mixture of
              Gaussian filter, while avoiding their issues.",
  year     =  2016,
  school   = "UC Irvine",
  keywords = "ReinforcementLearning;BayesOpt"
}

@MISC{Foley2017-qj,
  title        = "A pioneering computer scientist wants algorithms to be
                  regulated like cars, banks, and drugs",
  booktitle    = "Quartz",
  author       = "Foley, Katherine Ellen",
  abstract     = "It's convenient when Facebook can tag your friends in photos
                  for you, and it's fun when Snapchat can apply a filter to
                  your face. Both are examples of algorithms that have been
                  trained to recognize eyes, noses, and mouths with consistent
                  accuracy. When these programs are wrong---like when Facebook
                  mistakes you for your sibling or...",
  publisher    = "Quartz",
  month        =  jun,
  year         =  2017,
  howpublished = "\url{https://qz.com/998131/}",
  note         = "Accessed: 2017-6-5",
  keywords     = "trust\_popular\_media;trust\_academic\_conversation;Assurances"
}

@ARTICLE{Lacave2004-gq,
  title     = "A review of explanation methods for heuristic expert systems",
  author    = "Lacave, Carmen and Diez, Francisco J",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge Univ Press",
  volume    =  19,
  number    =  02,
  pages     = "133--146",
  year      =  2004
}

@BOOK{Halpern2017-zl,
  title     = "Reasoning about Uncertainty",
  author    = "Halpern, Joseph Y",
  abstract  = "In order to deal with uncertainty intelligently, we need to be
               able to represent it and reason about it. In this book, Joseph
               Halpern examines formal ways of representing uncertainty and
               considers various logics for reasoning about it. While the ideas
               presented are formalized in terms of definitions and theorems,
               the emphasis is on the philosophy of representing and reasoning
               about uncertainty. Halpern surveys possible formal systems for
               representing uncertainty, including probability measures,
               possibility measures, and plausibility measures; considers the
               updating of beliefs based on changing information and the
               relation to Bayes' theorem; and discusses qualitative,
               quantitative, and plausibilistic Bayesian networks.This second
               edition has been updated to reflect Halpern's recent research.
               New material includes a consideration of weighted probability
               measures and how they can be used in decision making; analyses
               of the Doomsday argument and the Sleeping Beauty problem;
               modeling games with imperfect recall using the runs-and-systems
               approach; a discussion of complexity-theoretic considerations;
               the application of first-order conditional logic to security.
               Reasoning about Uncertainty is accessible and relevant to
               researchers and students in many fields, including computer
               science, artificial intelligence, economics (particularly game
               theory), mathematics, philosophy, and statistics.",
  publisher = "MIT Press",
  month     =  apr,
  year      =  2017,
  keywords  = "Assurances",
  language  = "en"
}

@ARTICLE{Reeves1997-ad,
  title    = "The Media Equation: How People Treat Computers, Television, and
              New Media Like Real People and Places",
  author   = "Reeves, B and Nass, C",
  journal  = "Comput. Math. Appl.",
  volume   =  5,
  number   =  33,
  pages    = "128",
  year     =  1997,
  keywords = "NotRead;Assurances",
  language = "en"
}

@ARTICLE{Thatcher2011-si,
  title    = "The Role of Trust in Postadoption {IT} Exploration: An Empirical
              Examination of Knowledge Management Systems",
  author   = "Thatcher, J B and McKnight, D H and Baker, E W and Arsal, R E and
              Roberts, N H",
  abstract = "In this study, we examine trust in information technology's (IT)
              relationship with postadoption exploration of knowledge
              management systems (KMS). We introduce and distinguish between
              trust in IT and trust in IT support staff as object-specific
              beliefs that influence technology's infusion into organizations.
              We suggest that these object-specific beliefs' influence on
              intention to explore KMS is mediated by behavioral beliefs about
              IT (e.g., perceived usefulness and perceived ease of use). To
              test the model, we completed two studies. Study 1 examined users'
              perceptions of a knowledge portal. Study 2 examined IT
              professionals' perceptions of KMS. Across studies, our analysis
              suggests that trust in IT exerts direct effects on behavioral
              beliefs leading to intentions to explore KMS. Further, when
              compared to trust in IT support, we found that trust in IT played
              a more central role in shaping behavioral beliefs leading to
              exploration of IT. Implications for research and practice are
              offered.",
  journal  = "IEEE Trans. Eng. Manage.",
  volume   =  58,
  number   =  1,
  pages    = "56--70",
  month    =  feb,
  year     =  2011,
  keywords = "belief maintenance;information technology;knowledge
              management;organisational aspects;security of data;behavioral
              beliefs;information technology;knowledge management
              system;knowledge portal;professional perception;technology
              infusion;Intention to explore information technology
              (IT);knowledge management system (KMS);technology
              diffusion;technology infusion;trust;trust in
              technology;NotRead;Assurances"
}

@ARTICLE{Lippert2008-dv,
  title     = "Assessing post-adoption utilisation of an information technology
               within a supply chain management context",
  author    = "Lippert, Susan K",
  abstract  = "Information Technology (IT) has generated profound effects on
               Supply Chain Management (SCM) activities related to
               problem-solving, information sharing, and cost reduction
               initiatives. The influences of individual-level antecedents on
               post-adoption utilisation of a specialised IT within an SCM
               context were examined. Data were collected from 272 first-tier
               supply chain members of the second largest US automotive
               service-parts logistics operation using a new supply chain
               technology. Twelve hypotheses were tested through a structural
               equation model. The results suggest that in supply chains where
               usage is mandated, individual-level determinants can increase
               utilisation. Study implications and suggestions for future
               research are discussed.",
  journal   = "Int. J. Inf. Technol. Manage.",
  publisher = "Inderscience Publishers",
  volume    =  7,
  number    =  1,
  pages     = "36--59",
  month     =  jan,
  year      =  2008,
  keywords  = "NotRead;Assurances"
}

@ARTICLE{2003-lu,
  title     = "Consumer Acceptance of Electronic Commerce: Integrating Trust
               and Risk with the Technology Acceptance Model",
  author    = "{Paul A Pavlou}",
  abstract  = "This paper aims to predict consumer acceptance of e-commerce by
               proposing a set of key drivers for engaging consumers in on-line
               transactions. The primary constructs for capturing consumer
               acceptance of e-commerce are intention to transact and on-line
               transaction behavior. Following the theory of reasoned action
               (TRA) as applied to a technology-driven environment, technology
               acceptance model (TAM) variables (perceived usefulness and ease
               of use) are posited as key drivers of e-commerce acceptance. The
               practical utility of TAM stems from the fact that e-commerce is
               technology-driven. The proposed model integrates trust and
               perceived risk, which are incorporated given the implicit
               uncertainty of the e-commerce environment. The proposed
               integration of the hypothesized independent variables is
               justified by placing all the variables under the nomological TRA
               structure and proposing their interrelationships. The resulting
               research model is tested using data from two empirical studies.
               The first, exploratory study comprises three experiential
               scenarios with 103 students. The second, confirmatory study uses
               a sample of 155 on-line consumers. Both studies strongly support
               the e-commerce acceptance model by validating the proposed
               hypotheses. The paper discusses the implications for e-commerce
               theory, research, and practice, and makes several suggestions
               for future research.",
  journal   = "International Journal of Electronic Commerce",
  publisher = "Routledge",
  volume    =  7,
  number    =  3,
  pages     = "101--134",
  month     =  apr,
  year      =  2003,
  keywords  = "NotRead;Assurances"
}

@MISC{Orlikowski2006-eg,
  title     = "Desperately seeking the {`IT'in} {IT} research: a call to
               theorizing the {IT} artifact",
  author    = "Orlikowski, Wanda J and Iacono, C Suzanne",
  journal   = "Information systems: The state of the field",
  publisher = "Chichester, UK: John Wiley \& Sons Ltd",
  pages     = "19--42",
  year      =  2006,
  keywords  = "NotRead;Assurances"
}

@ARTICLE{Luhmann1982-jx,
  title     = "Trust and Power",
  author    = "Luhmann, Niklas",
  journal   = "Stud. Sov. Thought",
  publisher = "Springer",
  volume    =  23,
  number    =  3,
  pages     = "266--270",
  year      =  1982,
  keywords  = "trust\_definition;distrust\_definition;Assurances"
}

@INCOLLECTION{McKnight2001-hm,
  title     = "Trust and Distrust Definitions: One Bite at a Time",
  booktitle = "Trust in Cyber-societies",
  author    = "McKnight, D H and Chervany, Norman L",
  abstract  = "Researchers have remarked and recoiled at the literature
               confusion regarding the meanings of trust and distrust. The
               problem involves both the proliferation of narrow
               intra-disciplinary research definitions of trust and the
               multiple meanings the word trust possesses in everyday use. To
               enable trust researchers to more easily compare empirical
               results, we define a cohesive set of conceptual and measurable
               constructs that captures the essence of trust and distrust
               definitions across several disciplines. This chapter defines
               disposition to trust (and -distrust) constructs from psychology
               and economics, institution-based trust (and -distrust)
               constructs from sociology, and trusting/distrusting beliefs,
               trusting/distrusting intentions, and trust/distrust-related
               behavior constructs from social psychology and other
               disciplines. Distrust concepts are defined as separate and
               opposite from trust concepts. We conclude by discussing the
               importance of viewing trust and distrust as separate,
               simultaneously operating concepts.",
  publisher = "Springer, Berlin, Heidelberg",
  pages     = "27--54",
  year      =  2001,
  keywords  = "distrust\_definition;trust\_definition;Assurances",
  language  = "en"
}

@ARTICLE{Mcknight1996-ng,
  title    = "The Meanings of Trust",
  author   = "Mcknight, D H and Chervany, N L",
  abstract = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
              Teregowda): Our trust conceptualizations have benefited from
              discussions with Ellen Berscheid and Larry Cummings of the
              University of Minnesota. The authors also thank three anonymous
              reviewers from the Organizational Behavior division of the 1996
              meeting of the Academy of Management for their comments on an
              earlier version of this paper. THE MEANINGS OF TRUST What does
              the word `trust ' mean? Scholars continue to express concern
              regarding their collective lack of consensus about trust's
              meaning. Conceptual confusion on trust makes comparing one trust
              study to another problematic. To facilitate cumulative trust
              research, the authors propose two kinds of trust typologies: (a)
              a classification system for types of trust, and (b) definitions
              of six related trust types that form a model. Some of the model's
              implications for management are also outlined. 2 THE MEANINGS OF
              TRUST ``...trust is a term with many meanings. '' (Williamson,
              1993: 453) ``Trust is itself a term for a clustering of
              perceptions. '' (White, 1992: 174) Scholars and practitioners
              widely acknowledge trust's importance. Trust makes cooperative
              endeavors happen (e.g., Arrow, 1974; Deutsch, 1973; Gambetta,
              1988). Trust is a key to positive interpersonal relationships in",
  year     =  1996,
  keywords = "trust\_definition;business;Assurances"
}

@ARTICLE{Phillips2006-ix,
  title    = "A Model of User Distrust of Information Systems",
  author   = "Phillips, Brandis and McKnight, D H",
  journal  = "MWAIS 2006 Proceedings",
  pages    = "4",
  year     =  2006,
  keywords = "trust\_in\_technology;NotRead;Assurances"
}

@INCOLLECTION{McKnight2007-qw,
  title     = "An Extended Trust Building Model: Comparing Experiential and
               {Non-Experiential} Factors",
  booktitle = "Emerging Information Resources Management and Technologies",
  author    = "McKnight, D H and Chervany, Norman L",
  abstract  = "An Extended Trust Building Model: Comparing Experiential and
               Non-Experiential Factors: 10.4018/978-1-59904-286-2.ch008: This
               study examines a model of factors influencing system
               troubleshooter trust in their supervisors, contrasting
               experiential and nonexperiential factors.",
  publisher = "IGI Global",
  pages     = "176--199",
  year      =  2007,
  keywords  = "Normal Accident Theory; computer security; terrorism; Y2K;
               computer privacy;human\_study;trust\_in\_technology;Assurances",
  language  = "en"
}

@ARTICLE{Lankton2008-ct,
  title    = "Do People Trust Facebook as a Technology or as a`` Person''?
              Distinguishing Technology Trust from Interpersonal Trust",
  author   = "Lankton, Nancy K and McKnight, D Harrison",
  journal  = "AMCIS 2008 Proceedings",
  pages    = "375",
  year     =  2008,
  keywords = "trust\_in\_technology;very\_similar\_to\_mine;trust\_formal\_treatment;human\_study;assurance\_implicit;in\_paper;Assurances"
}

@ARTICLE{Mcknight2011-gv,
  title     = "Trust in a Specific Technology: An Investigation of Its
               Components and Measures",
  author    = "Mcknight, D Harrison and Carter, Michelle and Thatcher, Jason
               Bennett and Clay, Paul F",
  journal   = "ACM Trans. Manage. Inf. Syst.",
  publisher = "ACM",
  volume    =  2,
  number    =  2,
  pages     = "12:1--12:25",
  month     =  jul,
  year      =  2011,
  address   = "New York, NY, USA",
  keywords  = "Trust, construct development, trust in
               technology;trust\_in\_technology;very\_similar\_to\_mine;assurances;human\_study;trust\_formal\_treatment;assurance\_implicit;in\_paper;Assurances"
}

@ARTICLE{McKnight2006-ce,
  title     = "Reflections on an initial trust-building model",
  author    = "McKnight, D Harrison and Chervany, Norman L",
  journal   = "Handbook of trust research",
  publisher = "Edward Elgar Cheltenham, UK",
  pages     = "29--51",
  year      =  2006,
  keywords  = "trust\_definition;distrust\_definition;e-commerce;business;Assurances"
}

@ARTICLE{McKnight1998-ty,
  title    = "Initial Trust Formation in New Organizational Relationships",
  author   = "McKnight, D Harrison and Cummings, Larry L and Chervany, Norman L",
  abstract = "Arguably, the most critical time frame for organizational
              participants to develop trust is at the beginning of their
              relationship. Using primarily a cognitive approach, we address
              factors and processes that enable two organizational parties to
              form relatively high trust initially. We propose a model of
              specific relationships among several trust-related constructs and
              two cognitive processes. The model helps explain the paradoxical
              finding of high initial trust levels in new organizational
              relationships.",
  journal  = "Acad. Manage. Rev.",
  volume   =  23,
  number   =  3,
  pages    = "473--490",
  month    =  jul,
  year     =  1998,
  keywords = "trust\_definition;trust\_model;business;Assurances"
}

@ARTICLE{McKnight2004-vv,
  title     = "Dispositional Trust And Distrust Distinctions in Predicting
               High- and {Low-Risk} Internet Expert Advice Site Perceptions",
  author    = "McKnight, D H and Kacmar, Charles J and Choudhury, Vivek",
  abstract  = "This study examines whether some types of dispositional
               trust/distrust concepts are better than other types at inducing
               consumers to trust a Web advice provider. We propose and test a
               model in which dispositional trust and distrust concepts are
               given separate roles. This unique approach is based on the
               growing, but untested theoretical consensus that trust and
               distrust are separate concepts that co-exist yet differ in terms
               of their emotional makeup. While trust concepts tend to be calm
               and collected, distrust concepts embody significant levels of
               fear and insecurity. Based on this difference, we propose that
               dispositional distrust concepts will be better predictors of
               high-risk Internet legal advice site perceptions, while the
               corresponding trust concepts will be better predictors of
               low-risk Internet legal advice site perceptions. As proposed,
               the study finds that dispositional trust better predicts
               low-risk perceptions, while dispositional distrust better
               predicts high-risk perceptions. For e-commerce advice site
               research, the findings of this article suggest that perhaps
               scholars should Not only study dispositional trust but also
               dispositional distrust.",
  journal   = "e-Service Journal",
  publisher = "Indiana University Press",
  volume    =  3,
  number    =  2,
  pages     = "35--55",
  year      =  2004,
  keywords  = "trust\_definition;trust\_model;e-commerce;human\_study;distrust\_definition;Assurances"
}

@ARTICLE{McKnight2001-gz,
  title    = "While trust is cool and collected, distrust is fiery and
              frenzied: A model of distrust concepts",
  author   = "McKnight, D Harrison and Chervany, Norman",
  journal  = "Amcis 2001 Proceedings",
  pages    = "171",
  year     =  2001,
  keywords = "trust\_definition;trust\_model;e-commerce;Assurances"
}

@ARTICLE{Feltz_undated-yn,
  title    = "{Self'-Confidence} and Sports Performance",
  author   = "Feltz, Deborah L",
  keywords = "self-confidence;sport;ai\_general;assurances;Assurances"
}

@ARTICLE{Rempel1985-sg,
  title     = "Trust in close relationships",
  author    = "Rempel, John K and Holmes, John G and Zanna, Mark P",
  abstract  = "Tested a theoretical model of interpersonal trust in close
               relationships with 47 dating, cohabiting, or married couples
               (mean ages were 31 yrs for males and 29 yrs for females). The
               validity of the model's 3 dimensions of trust---predictability,
               dependability, and faith---was examined. Ss completed scales
               designed to measure liking and loving, trust, and motivation for
               maintaining the relationship. An analysis of the instrument
               measuring trust was consistent with the notion that the
               predictability, dependability, and faith components represent
               distinct and coherent dimensions. The perception of intrinsic
               motives in a partner emerged as a dimension, as did instrumental
               and extrinsic motives. As expected, love and happiness were
               closely tied to feelings of faith and the attribution of
               intrinsic motivation to both self and partner. Women appeared to
               have more integrated, complex views of their relationships than
               men: All 3 forms of trust were strongly related, and
               attributions of instrumental motives in their partners seemed to
               be self-affirming. There was a tendency for Ss to view their own
               motives as less self-centered and more exclusively intrinsic
               than their partner's motives. (25 ref) (PsycINFO Database Record
               (c) 2016 APA, all rights reserved)",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "American Psychological Association",
  volume    =  49,
  number    =  1,
  pages     = "95",
  month     =  jul,
  year      =  1985,
  keywords  = "validity of predictability \& dependability \& faith as
               dimensions of model of interpersonal trust, dating vs cohabiting
               vs married couples;NotRead;Assurances",
  language  = "en"
}

@BOOK{Wiener1964-ex,
  title     = "God and golem",
  author    = "Wiener, Norbert",
  publisher = "Massachusetts Institute of Technology",
  year      =  1964,
  keywords  = "NotRead;Textbook;TextBooks"
}

@ARTICLE{Tribe1972-zp,
  title     = "Technology assessment and the fourth discontinuity: The limits
               of instrumental rationality",
  author    = "Tribe, Laurence H",
  journal   = "South. Calif. Law Rev.",
  publisher = "HeinOnline",
  volume    =  46,
  pages     = "617",
  year      =  1972
}

@INCOLLECTION{Crum2004-xy,
  title     = "Certification Challenges for Autonomous Flight Control Systems",
  booktitle = "{AIAA} Guidance, Navigation, and Control Conference and Exhibit",
  author    = "Crum, Vincent and Homan, David and Bortner, Raymond",
  publisher = "American Institute of Aeronautics and Astronautics",
  series    = "Guidance, Navigation, and Control and Co-located Conferences",
  month     =  aug,
  year      =  2004,
  keywords  = "V\&V;Assurances"
}

@TECHREPORT{Huang2016-vd,
  title       = "Reasoning about cognitive trust in stochastic multiagent
                 systems",
  author      = "Huang, Xiaowei and Kwiatkowska, Marta",
  institution = "Technical Report CS-RR-16-02, Department of Computer Science,
                 University of Oxford",
  year        =  2016,
  keywords    = "Assurances"
}

@ARTICLE{Patrick2002-ga,
  title     = "Building trustworthy software agents",
  author    = "Patrick, A S",
  abstract  = "As agents become more active and sophisticated, the implications
               of their actions become more serious. With today's GUIs, user
               and software errors can often be easily fixed or undone. An
               agent performing actions on behalf of a user could make errors
               that are very difficult to ``undo'', and, depending on the
               agent's complexity, it might not be clear what went wrong.
               Moreover, for agents to operate effectively and truly act on
               their users' behalf, they might need confidential or sensitive
               information. This includes financial details and personal
               contact information. Thus, along with the excitement about
               agents and what they can do, there is concern about the
               resulting security and privacy issues. It is not enough to
               assume that well-designed software agents will provide the
               security and privacy users need; assurances and assumptions
               about security and privacy need to be made explicit. This
               article proposes a model of the factors that determine agent
               acceptance, based on earlier work on user attitudes toward
               e-commerce transactions, in which feelings of trust and
               perceptions of risk combine in opposite directions to determine
               a user's final acceptance of an agent technology.",
  journal   = "IEEE Internet Comput.",
  publisher = "ieeexplore.ieee.org",
  volume    =  6,
  number    =  6,
  pages     = "46--53",
  month     =  nov,
  year      =  2002,
  keywords  = "data privacy;electronic commerce;knowledge engineering;security
               of data;software agents;confidential information;e-commerce
               transactions;privacy;risk perceptions;security;trust;trustworthy
               software agent building;user attitudes;Computer errors;Computer
               interfaces;Computer networks;Distributed computing;Graphical
               user interfaces;Information security;Intelligent
               agent;Mice;Privacy;Software agents;NotRead;Assurances"
}

@ARTICLE{Cheshire2011-zx,
  title     = "Online trust, trustworthiness, or assurance?",
  author    = "Cheshire, Coye",
  abstract  = "Every day, individuals around the world retrieve, share, and
               exchange information on the Internet. We interact online to
               share personal information, find answers to questions, make
               financial transactions, play social games, and maintain
               professional and personal relationships. Sometimes our online
               interactions take place between two or more humans. In other
               cases, we rely on computers to manage information on our behalf.
               In each scenario, risk and uncertainty are essential for
               determining possible actions and outcomes. This essay highlights
               common deficiencies in our understanding of key concepts such as
               trust, trustworthiness, cooperation, and assurance in online
               environments. Empirical evidence from experimental work in
               computer-mediated environments underscores the promises and
               perils of overreliance on security and assurance structures as
               replacements for interpersonal trust. These conceptual
               distinctions are critical because the future shape of the
               Internet will depend on whether we build assurance structures to
               limit and control ambiguity or allow trust to emerge in the
               presence of risk and uncertainty.",
  journal   = "Daedalus",
  publisher = "MIT Press",
  volume    =  140,
  number    =  4,
  pages     = "49--58",
  year      =  2011,
  keywords  = "NotRead;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Aitken2016-fb,
  title     = "Assurances and machine self-confidence for enhanced trust in
               autonomous systems",
  booktitle = "{RSS} 2016 Workshop on Social Trust in Autonomous Systems",
  author    = "Aitken, Matthew and Ahmed, Nisar and Lawrence, Dale and Argrow,
               Brian and Frew, Eric",
  abstract  = "This work investigates a model-based approach to understanding
               how user trust evolves in systems consisting of a supervising
               user and an autonomous agent. This model consists of a
               multivariate model for user trust , and a feedback connection
               between user and agent.",
  publisher = "qav.comlab.ox.ac.uk",
  year      =  2016,
  keywords  = "self-confidence;Assurances"
}

@INPROCEEDINGS{Ahmed2016-wz,
  title     = "Collaborative autonomous sensing with Bayesians in the loop",
  booktitle = "{SPIE} Security + Defence",
  author    = "Ahmed, Nisar",
  abstract  = "abstract There is a strong push to develop intelligent unmanned
               autonomy that complements human reasoning for applications as
               diverse as wilderness search and rescue, military surveillance,
               and robotic space exploration. More than just replacing",
  publisher = "International Society for Optics and Photonics",
  pages     = "99860B--99860B--15",
  month     =  oct,
  year      =  2016,
  keywords  = "Algorithms; Defense and security; Information fusion;
               Interfaces; Machine learning; Modeling; Robotics; Sensors;
               Surveillance; Robots; Search and rescue"
}

@ARTICLE{Nahavandi2017-mo,
  title     = "Trusted Autonomy Between Humans and Robots: Toward
               {Human-on-the-Loop} in Robotics and Autonomous Systems",
  author    = "Nahavandi, S",
  abstract  = "Systems that can change their behavior in response to unexpected
               conditions and events during operation are known as autonomous
               [1]. Autonomy refers to the capability of a machine to perform a
               task, or part of it, with no-or substantially reduced-human
               intervention. Over the years, autonomous systems have appeared
               and sometimes dominated various aspects of human daily
               activities, such as in robot-controlled operations.",
  journal   = "IEEE Systems, Man, and Cybernetics Magazine",
  publisher = "ieeexplore.ieee.org",
  volume    =  3,
  number    =  1,
  pages     = "10--17",
  month     =  jan,
  year      =  2017,
  keywords  = "Autonomous systems;Cybernetics;Decision making;Man-machine
               systems;Robots;Service robots;Weapons"
}

@ARTICLE{Dassonville1996-hu,
  title    = "Trust between man and machine in a teleoperation system",
  author   = "Dassonville, I and Jolly, D and Desodt, A M",
  abstract = "The work we present deals with the trust of man in a
              teleoperation system. Trust is important because it is linked to
              stress which modifies human reliability. We are trying to
              quantify trust. In this paper, we'll present the theory of trust
              in relationships, and its extension for a man-machine system.
              Then, we explain the links between trust and human reliability.
              Then, we introduce our experimental process and the first results
              concerning selfconfidence.",
  journal  = "Reliab. Eng. Syst. Saf.",
  volume   =  53,
  number   =  3,
  pages    = "319--325",
  month    =  sep,
  year     =  1996
}

@TECHREPORT{Sheridan1984-kx,
  title       = "Research and modeling of supervisory control behavior. Report
                 of a workshop",
  author      = "Sheridan, Thomas B and Hennessy, Robert T",
  institution = "DTIC Document",
  year        =  1984,
  keywords    = "automation;assurances;visionary\_paper;trust\_formal\_treatment;assurance\_implicit;Assurances"
}

@ARTICLE{Lewis1985-pr,
  title     = "Trust as a Social Reality",
  author    = "Lewis, J David and Weigert, Andrew",
  abstract  = "Although trust is an underdeveloped concept in sociology,
               promising theoretical formulations are available in the recent
               work of Luhmann and Barber. This sociological version
               complements the psychological and attitudinal conceptualizations
               of experimental and survey researchers. Trust is seen to include
               both emotional and cognitive dimensions and to function as a
               deep assumption underwriting social order. Contemporary examples
               such as lying, family exchange, monetary attitudes, and
               litigation illustrate the centrality of trust as a sociological
               reality.",
  journal   = "Soc. Forces",
  publisher = "Oxford University Press",
  volume    =  63,
  number    =  4,
  pages     = "967--985",
  month     =  jun,
  year      =  1985,
  keywords  = "trust\_definition;NotRead;Assurances"
}

@ARTICLE{Wachter2016-kz,
  title    = "Why a Right to Explanation of Automated {Decision-Making} Does
              Not Exist in the General Data Protection Regulation",
  author   = "Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano",
  abstract = "Since approval of the EU General Data Protection Regulation
              (GDPR) in 2016, it has been widely and repeatedly claimed that
              the GDPR will legally mandate a `righ",
  month    =  dec,
  year     =  2016,
  keywords = "artificial intelligence, algorithms, automated decision-making,
              data protection, General Data Protection Regulation, right to
              explanation, right of
              access;NotRead;trust\_popular\_media;Assurances"
}

@MISC{Lipton2017-zw,
  title        = "{NYU} Law's Algorithms and Explanations",
  booktitle    = "Approximately Correct",
  author       = "Lipton, Zachary",
  month        =  aug,
  year         =  2017,
  howpublished = "\url{http://approximatelycorrect.com/2017/05/08/nyu-laws-algorithms-and-explanations/}",
  note         = "Accessed: 2017-8-5",
  keywords     = "decision\_support;ai\_reasoning;assurances;NotRead;Assurances"
}

@INPROCEEDINGS{Azzopardi2003-bg,
  title     = "Investigating the Relationship Between Language Model Perplexity
               and {IR} Precision-recall Measures",
  booktitle = "Proceedings of the 26th Annual International {ACM} {SIGIR}
               Conference on Research and Development in Informaion Retrieval",
  author    = "Azzopardi, Leif and Girolami, Mark and van Risjbergen, Keith",
  publisher = "ACM",
  pages     = "369--370",
  series    = "SIGIR '03",
  year      =  2003,
  address   = "New York, NY, USA",
  keywords  = "language model;NotRead"
}

@ARTICLE{Heinrich2008-nf,
  title    = "Parameter estimation for text analysis",
  author   = "Heinrich, Gregor",
  journal  = "University of Leipzig, Tech. Rep",
  year     =  2008,
  keywords = "NotRead"
}

@INPROCEEDINGS{Zhang2014-he,
  title     = "Predicting failures of vision systems",
  booktitle = "Proceedings of the {IEEE} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Zhang, Peng and Wang, Jiuling and Farhadi, Ali and Hebert,
               Martial and Parikh, Devi",
  pages     = "3566--3573",
  year      =  2014,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;in\_presentation;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Churchill2015-ei,
  title     = "Know your limits: Embedding localiser performance models in
               teach and repeat maps",
  booktitle = "2015 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Churchill, W and Tong, Chi Hay and Gur{\u a}u, C and Posner, I
               and Newman, P",
  abstract  = "This paper is about building maps which not only contain the
               traditional information useful for localising - such as point
               features - but also embeds a spatial model of expected localiser
               performance. This often overlooked second-order information
               provides vital context when it comes to map use and planning.
               Our motivation here is to improve the performance of the popular
               Teach and Repeat paradigm [1] which has been shown to enable
               truly large-scale field operation. When using the taught route
               for localisation, it is often assumed the robot is following
               exactly, or is sufficiently close to, the original path,
               enabling successful localisation. However, what happens if it is
               not possible, or not desirable to exactly follow the mapped
               path? How far off the beaten track can the robot travel before
               it gets lost? We present an approach for assessing this
               localisation area around a taught route, which we refer to as
               the localisation envelope. Using a combination of physical
               sampling and a Gaussian Process model, we are able to accurately
               predict the localisation performance at unseen points.",
  pages     = "4238--4244",
  month     =  may,
  year      =  2015,
  keywords  = "Gaussian processes;path planning;robots;sampling
               methods;Gaussian process model;localisation envelope;localiser
               performance models;map building;physical sampling;point
               features;robot;second-order information;teach and repeat
               paradigm;Gaussian processes;Planning;Robots;Splines
               (mathematics);Trajectory;Uncertainty;Visualization;trust\_informal\_treatment;assurance\_explicit;perf\_prediction;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Tellex2012-hn,
  title     = "Toward Information Theoretic {Human-Robot} Dialog",
  booktitle = "Robotics: Science and Systems",
  author    = "Tellex, Stefanie and Thaker, Pratiksha and Deits, Robin and
               Kollar, Thomas and Roy, Nicholas",
  volume    =  2,
  pages     = "3",
  year      =  2012,
  keywords  = "ai\_learning;ai\_interaction;assurance\_competence;assurance\_predictability;robotics;trust\_informal\_treatment;assurance\_implicit;Assurances"
}

@INPROCEEDINGS{Paul2011-vr,
  title     = "Self help: Seeking out perplexing images for ever improving
               navigation",
  booktitle = "2011 {IEEE} International Conference on Robotics and Automation",
  author    = "Paul, R and Newman, P",
  abstract  = "This paper is a demonstration of how a robot can, through
               introspection and then targeted data retrieval, improve its own
               performance. It is a step in the direction of lifelong learning
               and adaptation and is motivated by the desire to build robots
               that have plastic competencies which are not baked in. They
               should react to and benefit from use. We consider a particular
               instantiation of this problem in the context of place
               recognition. Based on a topic based probabilistic model of
               images, we use a measure of perplexity to evaluate how well a
               working set of background images explain the robot's online view
               of the world. Offline, the robot then searches an external
               resource to seek out additional background images that bolster
               its ability to localise in its environment when used next. In
               this way the robot adapts and improves performance through use.",
  pages     = "445--451",
  month     =  may,
  year      =  2011,
  keywords  = "SLAM (robots);image retrieval;mobile robots;path
               planning;probability;robot vision;FAB-MAP algorithm;data
               retrieval;image probabilistic model;introspection;lifelong
               learning;navigation improvement;perplexing image seeking
               out;place recognition;self help;Biological system
               modeling;Convergence;Databases;Mathematical
               model;Redundancy;Robots;Visualization;image\_classification;assurance\_competence;assurance\_normality;ai\_perception;ai\_knowledge\_rep;ai\_learning;assurance\_implicit;trust\_informal\_treatment;active\_learning;in\_paper;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@ARTICLE{Freund1997-ig,
  title     = "Selective Sampling Using the Query by Committee Algorithm",
  author    = "Freund, Yoav and Sebastian Seung, H and Shamir, Eli and Tishby,
               Naftali",
  abstract  = "We analyze the ``query by committee'' algorithm, a method for
               filtering informative queries from a random stream of inputs. We
               show that if the two-member committee algorithm achieves
               information gain with positive lower bound, then the prediction
               error decreases exponentially with the number of queries. We
               show that, in particular, this exponential decrease holds for
               query learning of perceptrons.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  28,
  number    = "2-3",
  pages     = "133--168",
  month     =  aug,
  year      =  1997,
  keywords  = "NotRead;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Weiyu_Zhang_undated-jx,
  title           = "Power {SVM}: Generalization with exemplar classification
                     uncertainty",
  booktitle       = "2012 {IEEE} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "{Weiyu Zhang} and Yu, S X and {Shang-Hua Teng}",
  publisher       = "IEEE",
  pages           = "2144--2151",
  keywords        = "meh..;Assurances",
  conference      = "2012 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)"
}

@ARTICLE{Tong2001-di,
  title    = "Support Vector Machine Active Learning with Applications to Text
              Classification",
  author   = "Tong, Simon and Koller, Daphne",
  journal  = "J. Mach. Learn. Res.",
  volume   =  2,
  number   = "Nov",
  pages    = "45--66",
  year     =  2001,
  keywords = "classification;meh..;Assurances"
}

@INPROCEEDINGS{Holub2008-pe,
  title     = "Entropy-based active learning for object recognition",
  booktitle = "2008 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition Workshops",
  author    = "Holub, A and Perona, P and Burl, M C",
  abstract  = "Most methods for learning object categories require large
               amounts of labeled training data. However, obtaining such data
               can be a difficult and time-consuming endeavor. We have
               developed a novel, entropy-based ldquoactive learningrdquo
               approach which makes significant progress towards this problem.
               The main idea is to sequentially acquire labeled data by
               presenting an oracle (the user) with unlabeled images that will
               be particularly informative when labeled. Active learning
               adaptively prioritizes the order in which the training examples
               are acquired, which, as shown by our experiments, can
               significantly reduce the overall number of training examples
               required to reach near-optimal performance. At first glance this
               may seem counter-intuitive: how can the algorithm know whether a
               group of unlabeled images will be informative, when, by
               definition, there is no label directly associated with any of
               the images? Our approach is based on choosing an image to label
               that maximizes the expected amount of information we gain about
               the set of unlabeled images. The technique is demonstrated in
               several contexts, including improving the efficiency of Web
               image-search queries and open-world visual learning by an
               autonomous agent. Experiments on a large set of 140 visual
               object categories taken directly from text-based Web image
               searches show that our technique can provide large improvements
               (up to 10 x reduction in the number of training examples needed)
               over baseline techniques.",
  pages     = "1--8",
  month     =  jun,
  year      =  2008,
  keywords  = "entropy;learning (artificial intelligence);object
               recognition;Web image-search queries;autonomous
               agent;entropy-based active learning;object categories;object
               recognition;open-world visual learning;text-based Web
               image;unlabeled images;Autonomous
               agents;Entropy;Histograms;Image
               sampling;Labeling;Laboratories;Object
               recognition;Propulsion;Scattering;Training
               data;image\_classification;classification;assurance\_competence;ai\_reasoning;ai\_learning;assurance\_predictability;supervised\_learning;active\_learning;Assurances"
}

@INPROCEEDINGS{Joshi2009-ws,
  title     = "Multi-class active learning for image classification",
  booktitle = "2009 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Joshi, A J and Porikli, F and Papanikolopoulos, N",
  abstract  = "One of the principal bottlenecks in applying learning techniques
               to classification problems is the large amount of labeled
               training data required. Especially for images and video,
               providing training data is very expensive in terms of human time
               and effort. In this paper we propose an active learning approach
               to tackle the problem. Instead of passively accepting random
               training examples, the active learning algorithm iteratively
               selects unlabeled examples for the user to label, so that human
               effort is focused on labeling the most ``useful'' examples. Our
               method relies on the idea of uncertainty sampling, in which the
               algorithm selects unlabeled examples that it finds hardest to
               classify. Specifically, we propose an uncertainty measure that
               generalizes margin-based uncertainty to the multi-class case and
               is easy to compute, so that active learning can handle a large
               number of classes and large data sizes efficiently. We
               demonstrate results for letter and digit recognition on datasets
               from the UCI repository, object recognition results on the
               Caltech-101 dataset, and scene categorization results on a
               dataset of 13 natural scene categories. The proposed method
               gives large reductions in the number of training examples
               required over random selection to achieve similar classification
               accuracy, with little computational overhead.",
  pages     = "2372--2379",
  month     =  jun,
  year      =  2009,
  keywords  = "character recognition;image classification;learning (artificial
               intelligence);object recognition;uncertainty handling;digit
               recognition;image classification;labeling examples;letter
               recognition;multi-class active learning;object
               recognition;uncertainty sampling;Humans;Image
               classification;Iterative algorithms;Labeling;Layout;Measurement
               uncertainty;Object recognition;Sampling methods;Size
               measurement;Training
               data;image\_classification;classification;assurance\_predictability;assurance\_competence;ai\_reasoning;ai\_learning;supervised\_learning;active\_learning;Assurances"
}

@ARTICLE{Kapoor2010-cy,
  title     = "Gaussian Processes for Object Categorization",
  author    = "Kapoor, Ashish and Grauman, Kristen and Urtasun, Raquel and
               Darrell, Trevor",
  abstract  = "Discriminative methods for visual object category recognition
               are typically non-probabilistic, predicting class labels but not
               directly providing an estimate of uncertainty. Gaussian
               Processes (GPs) provide a framework for deriving regression
               techniques with explicit uncertainty models; we show here how
               Gaussian Processes with covariance functions defined based on a
               Pyramid Match Kernel (PMK) can be used for probabilistic object
               category recognition. Our probabilistic formulation provides a
               principled way to learn hyperparameters, which we utilize to
               learn an optimal combination of multiple covariance functions.
               It also offers confidence estimates at test points, and
               naturally allows for an active learning paradigm in which points
               are optimally selected for interactive labeling. We show that
               with an appropriate combination of kernels a significant boost
               in classification performance is possible. Further, our
               experiments indicate the utility of active learning with
               probabilistic predictive models, especially when the amount of
               training data labels that may be sought for a category is
               ultimately very small.",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer US",
  volume    =  88,
  number    =  2,
  pages     = "169--188",
  month     =  jun,
  year      =  2010,
  keywords  = "assurance\_competence;assurance\_normality;ai\_reasoning;image\_classification;classification;GPs;assurance\_implicit;trust\_informal\_treatment;active\_learning;Assurances",
  language  = "en"
}

@BOOK{Riley1996-qm,
  title     = "Operator reliance on automation: Theory and data",
  author    = "Riley, Victor",
  abstract  = "until recently, little has been known about what factors
               influence the decision to use or not use automation and what
               types of bias to which this decision may be subject / a better
               understanding of these factors and biases may help system
               developers anticipate the conditions under which operators may
               underrely or overrely on automation and guide the development of
               training methods and user interfaces to encourage rational
               automation use / the decision has been a critical link in the
               chains of events that have led to many incidents and accidents
               in aircraft, railroad, ship, process control, medical, and power
               plant operations an investigation of factors that influence
               automation reliance using a simple computer game (PsycINFO
               Database Record (c) 2016 APA, all rights reserved)",
  publisher = "Lawrence Erlbaum Associates, Inc",
  year      =  1996,
  keywords  = "theory of computer games for investigation of factors that
               influence automation usage patterns \&
               reliance;human\_study;automation;trust\_formal\_treatment;assurance\_implicit;in\_paper;in\_presentation;Supplemental
               Assurance;User Observation;Assurances",
  language  = "en"
}

@ARTICLE{Bainbridge2011-pl,
  title     = "The Benefits of Interactions with Physically Present Robots over
               {Video-Displayed} Agents",
  author    = "Bainbridge, Wilma A and Hart, Justin W and Kim, Elizabeth S and
               Scassellati, Brian",
  abstract  = "This paper explores how a robot's physical presence affects
               human judgments of the robot as a social partner. For this
               experiment, participants collaborated on simple book-moving
               tasks with a humanoid robot that was either physically present
               or displayed via a live video feed. Multiple tasks individually
               examined the following aspects of social interaction: greetings,
               cooperation, trust, and personal space. Participants readily
               greeted and cooperated with the robot whether present physically
               or in live video display. However, participants were more likely
               both to fulfill an unusual request and to afford greater
               personal space to the robot when it was physically present, than
               when it was shown on live video. The same was true when the live
               video displayed robot's gestures were augmented with
               disambiguating 3-D information. Questionnaire data support these
               behavioral findings and also show that participants had an
               overall more positive interaction with the physically present
               robot.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  3,
  number    =  1,
  pages     = "41--52",
  month     =  jan,
  year      =  2011,
  keywords  = "
               human\_study;trust\_formal\_treatment;assurance\_implicit;in\_paper;Integral
               Assurance;Human-like Behavior;Assurances",
  language  = "en"
}

@ARTICLE{Corritore2003-gx,
  title    = "On-line trust: concepts, evolving themes, a model",
  author   = "Corritore, Cynthia L and Kracher, Beverly and Wiedenbeck, Susan",
  abstract = "Trust is emerging as a key element of success in the on-line
              environment. Although considerable research on trust in the
              offline world has been performed, to date empirical study of
              on-line trust has been limited. This paper examines on-line
              trust, specifically trust between people and informational or
              transactional websites. It begins by analysing the definitions of
              trust in previous offline and on-line research. The relevant
              dimensions of trust for an on-line context are identified, and a
              definition of trust between people and informational or
              transactional websites is presented. We then turn to an
              examination of the causes of on-line trust. Relevant findings in
              the human--computer interaction literature are identified. A
              model of on-line trust between users and websites is presented.
              The model identifies three perceptual factors that impact on-line
              trust: perception of credibility, ease of use and risk. The model
              is discussed in detail and suggestions for future applications of
              the model are presented.",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  58,
  number   =  6,
  pages    = "737--758",
  year     =  2003,
  keywords = "On-line trust; Trust; Internet trust; User trust; Website trust;
              Credibility; Risk; Ease of
              use;trust\_definition;e-commerce;trust\_model;Assurances;Assurances/Trust
              Background"
}

@BOOK{Barber1983-yc,
  title    = "The logic and limits of trust",
  author   = "Barber, Bernard",
  year     =  1983,
  keywords = "Assurances"
}

@INPROCEEDINGS{Kaniarasu2012-mo,
  title     = "Potential Measures for Detecting Trust Changes",
  booktitle = "Proceedings of the Seventh Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal and
               Yanco, Holly",
  publisher = "ACM",
  pages     = "241--242",
  series    = "HRI '12",
  year      =  2012,
  address   = "New York, NY, USA",
  keywords  = "automation, experiments,
               trust;human\_study;assurance\_competence;assurance\_predictability;ai\_reasoning;Assurances"
}

@ARTICLE{Rouse1986-dz,
  title    = "Design and evaluation of computer-based decision support systems",
  author   = "Rouse, William B",
  journal  = "Microcomputer decision support systems",
  year     =  1986,
  keywords = "
              decision\_support;assurances;ai\_general;trust\_informal\_treatment;assurance\_explicit;explain;in\_paper;Supplemental
              Assurance;Reduce Complexity;Assurances;Assurances/Trust
              Background"
}

@ARTICLE{Sheridan1983-nd,
  title    = "Adapting automation to man, culture and society",
  author   = "Sheridan, T B and V{\'a}mos, T and Aida, S",
  abstract = "Anxiety about the effects of automation on workers and society is
              at least 150 years old. The recent explosion of microelectronics
              and robotic applications has sharpened our understanding of both
              the gains and the risks: mismatch to human physiological,
              psychological and cultural characteristics; alienation from
              fulfillment and dignity in work; widening of the gap between
              skilled and unskilled workers and between technologically
              developed and underdeveloped communities; decrement in individual
              security. Attention to these problems can ensure that automation
              results in a better society. The control engineer, who is
              responsible for enlarging the scale of automation, should also
              play a role in adapting it to people. For the time being,
              technology should be individually designed to each culture.",
  journal  = "Automatica",
  volume   =  19,
  number   =  6,
  pages    = "605--612",
  month    =  nov,
  year     =  1983,
  keywords = "Automation; social impacts; work; productivity; culture and
              robots;meh..;visionary\_paper;automation;Assurances;Assurances/Trust
              Background"
}

@ARTICLE{Sheridan1980-px,
  title    = "Computer control and human alienation",
  author   = "Sheridan, Thomas B",
  journal  = "Technol. Rev.",
  volume   =  83,
  number   =  1,
  pages    = "60--73",
  year     =  1980,
  keywords = "automation;meh..;Assurances;Assurances/Trust Background"
}

@ARTICLE{Muir1996-gt,
  title    = "Trust in automation. Part {II}. Experimental studies of trust and
              human intervention in a process control simulation",
  author   = "Muir, B M and Moray, N",
  abstract = "Two experiments are reported which examined operators' trust in
              and use of the automation in a simulated supervisory process
              control task. Tests of the integrated model of human trust in
              machines proposed by Muir (1994) showed that models of
              interpersonal trust capture some important aspects of the nature
              and dynamics of human-machine trust. Results showed that
              operators' subjective ratings of trust in the automation were
              based mainly upon their perception of its competence. Trust was
              significantly reduced by any sign of incompetence in the
              automation, even one which had no effect on overall system
              performance. Operators' trust changed very little with
              experience, with a few notable exceptions. Distrust in one
              function of an automatic component spread to reduce trust in
              another function of the same component, but did not generalize to
              another independent automatic component in the same system, or to
              other systems. There was high positive correlation between
              operators' trust in and use of the automation; operators used
              automation they trusted and rejected automation they distrusted,
              preferring to do the control task manually. There was an inverse
              relationship between trust and monitoring of the automation.
              These results suggest that operators' subjective ratings of trust
              and the properties of the automation which determine their trust,
              can be used to predict and optimize the dynamic allocation of
              functions in automated systems.",
  journal  = "Ergonomics",
  volume   =  39,
  number   =  3,
  pages    = "429--460",
  month    =  mar,
  year     =  1996,
  keywords = "
              human\_study;very\_similar\_to\_mine;assurances;automation;trust\_formal\_treatment;assurance\_implicit;in\_presentation;Supplemental
              Assurance;User Observation;Assurances/Trust Background",
  language = "en"
}

@ARTICLE{Muir1994-ow,
  title    = "Trust in automation: Part I. Theoretical issues in the study of
              trust and human intervention in automated systems",
  author   = "Muir, Bonnie M",
  abstract = "Abstract Today many systems are highly automated. The human
              operator's role in these systems is to supervise the automation
              and intervene to take manual control when necessary. The
              operator's choice of automatic or manual control has important
              consequences for system performance, and therefore it is
              important to understand and optimize this decision process. One
              important determinant of operators' choice of manual or automatic
              control may be their degree of trust in the automation. However,
              there have been no experimental tests of this hypothesis until
              recently, nor is there a model of human trust in machines to form
              a theoretical foundation for empirical studies. In this paper a
              model of human trust in machines is developed, taking models of
              trust between people as a starting point, and extending them to
              the human-machine relationship. The resulting model defines human
              trust in machines and specifies how trust changes with experience
              on a system, providing a framework for experimental research on
              trust and human intervention in automated systems.",
  journal  = "Ergonomics",
  volume   =  37,
  number   =  11,
  pages    = "1905--1922",
  year     =  1994,
  keywords = "
              trust\_definition;assurances;very\_similar\_to\_mine;automation;trust\_formal\_treatment;assurance\_explicit;in\_paper;Assurances/Trust
              Background;Assurances"
}

@INPROCEEDINGS{Salem2015-md,
  title     = "Would You Trust a (Faulty) Robot?: Effects of Error, Task Type
               and Personality on {Human-Robot} Cooperation and Trust",
  booktitle = "Proceedings of the Tenth Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Salem, Maha and Lakatos, Gabriella and Amirabdollahian, Farshid
               and Dautenhahn, Kerstin",
  publisher = "ACM",
  pages     = "141--148",
  series    = "HRI '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "cooperation, social human-robot interaction,
               trust;human\_study;ai\_planning;ai\_motion\_manipulation;ai\_interaction;ai\_general;assurance\_competence;assurance\_structural;assurance\_predictability;assurances;trust\_formal\_treatment;assurance\_implicit;in\_paper;Integral
               Assurance;Human-like Behavior;Assurances"
}

@INPROCEEDINGS{Desai2012-rc,
  title     = "Effects of changing reliability on trust of robot systems",
  booktitle = "2012 7th {ACM/IEEE} International Conference on {Human-Robot}
               Interaction ({HRI})",
  author    = "Desai, M and Medvedev, M and V{\'a}zquez, M and McSheehy, S and
               Gadea-Omelchenko, S and Bruggeman, C and Steinfeld, A and Yanco,
               H",
  abstract  = "Prior work in human-autonomy interaction has focused on plant
               systems that operate in highly structured environments. In
               contrast, many human-robot interaction (HRI) tasks are dynamic
               and unstructured, occurring in the open world. It is our belief
               that methods developed for the measurement and modeling of trust
               in traditional automation need alteration in order to be useful
               for HRI. Therefore, it is important to characterize the factors
               in HRI that influence trust. This study focused on the influence
               of changing autonomy reliability. Participants experienced a set
               of challenging robot handling scenarios that forced autonomy use
               and kept them focused on autonomy performance. The
               counterbalanced experiment included scenarios with different low
               reliability windows so that we could examine how drops in
               reliability altered trust and use of autonomy. Drops in
               reliability were shown to affect trust, the frequency and timing
               of autonomy mode switching, as well as participants'
               self-assessments of performance. A regression analysis on a
               number of robot, personal, and scenario factors revealed that
               participants tie trust more strongly to their own actions rather
               than robot performance.",
  pages     = "73--80",
  month     =  mar,
  year      =  2012,
  keywords  = "control engineering computing;human-robot interaction;regression
               analysis;reliability;HRI;autonomy mode switching;autonomy
               performance;changing reliability;human-autonomy
               interaction;human-robot interaction;participant
               self-assessments;regression analysis;reliability windows;robot
               systems;Automation;Reliability;Robot sensing
               systems;Switches;USA Councils;Unified modeling
               language;Trust;automation;experiments;human\_study;ai\_motion\_manipulation;ai\_general;assurances;human-robot
               team;trust\_formal\_treatment;assurance\_implicit;in\_paper;Integral
               Assurance;User Observation;Assurances"
}

@INPROCEEDINGS{Freedy2007-sg,
  title     = "Measurement of trust in human-robot collaboration",
  booktitle = "2007 International Symposium on Collaborative Technologies and
               Systems",
  author    = "Freedy, A and DeVisser, E and Weltman, G and Coeyman, N",
  abstract  = "We describe a collaborative performance model that captures the
               critical performance attributes of the distinctive human-robotic
               decision and control environment. The literature and our initial
               experimental studies show that the element of trust in
               human-robot collaboration is an extremely important factor in
               the performance model, and accordingly we have focused much of
               our attention on deriving suitable and practical measures of
               this variable. In this paper we describe the formulation of a
               decision-analytical based measure of trust as well as the
               results of two initial experiments designed to examine trust in
               a tactical human-robot collaborative task performed in our new
               mixed initiative team performance assessment system (MITPAS)
               simulation environment.",
  pages     = "106--114",
  month     =  may,
  year      =  2007,
  keywords  = "man-machine systems;robots;collaborative performance
               model;control environment;human-robot collaboration;mixed
               initiative team performance assessment system;trust
               measurement;Adaptation
               model;Automation;Collaboration;Firing;Robot
               kinematics;Robots;Training;Human Robot Collaboration;Human-Robot
               Performance Modeling;Measurement of Trust;Mixed-Initiative
               Systems;human\_study;trust\_definition;human-robot
               team;ai\_motion\_manipulation;assurance\_competence;assurances;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_implicit;in\_paper;in\_presentation;User
               Interaction;Integral Assurance;Assurances"
}

@INPROCEEDINGS{Grimmett2013-gj,
  title     = "Knowing when we don't know: Introspective classification for
               mission-critical decision making",
  booktitle = "2013 {IEEE} International Conference on Robotics and Automation",
  author    = "Grimmett, H and Paul, R and Triebel, R and Posner, I",
  abstract  = "Classification precision and recall have been widely adopted by
               roboticists as canonical metrics to quantify the performance of
               learning algorithms. This paper advocates that for robotics
               applications, which often involve mission-critical decision
               making, good performance according to these standard metrics is
               desirable but insufficient to appropriately characterise system
               performance. We introduce and motivate the importance of a
               classifier's introspective capacity: the ability to mitigate
               potentially overconfident classifications by an appropriate
               assessment of how qualified the system is to make a judgement on
               the current test datum. We provide an intuition as to how this
               introspective capacity can be achieved and systematically
               investigate it in a selection of classification frameworks
               commonly used in robotics: support vector machines, LogitBoost
               classifiers and Gaussian Process classifiers (GPCs). Our
               experiments demonstrate that for common robotics tasks a
               framework such as a GPC exhibits a superior introspective
               capacity while maintaining commensurate classification
               performance to more popular, alternative approaches.",
  pages     = "4531--4538",
  month     =  may,
  year      =  2013,
  keywords  = "Gaussian processes;decision making;image classification;robot
               vision;support vector machines;GPC;Gaussian process
               classifiers;LogitBoost classifiers;introspective
               capacity;introspective classification;mission-critical decision
               making;potentially overconfident classification
               mitigation;robotics tasks;support vector machines;Educational
               institutions;introspection;classification;image\_classification;ai\_reasoning;ai\_learning;assurance\_competence;assurance\_normality;Safety\_AI;robotics;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;prob\_class;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Triebel2013-ku,
  title           = "Confidence Boosting: Improving the Introspectiveness of a
                     Boosted Classifier for Efficient Learning",
  booktitle       = "Workshop on Autonomous Learing.",
  author          = "Triebel, Rudolph and Grimmett, Hugo and Posner, Ingmar",
  month           =  may,
  year            =  2013,
  keywords        = "
                     introspection;ai\_reasoning;ai\_learning;assurance\_competence;classification;image\_classification;Safety\_AI;robotics;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;prob\_class;Supplemental
                     Assurance;Quantify Uncertainty;Assurances",
  conference      = "IEEE International Conference on Robotics and Automation
                     (ICRA)"
}

@INPROCEEDINGS{Triebel2013-ow,
  title     = "Introspective active learning for scalable semantic mapping",
  booktitle = "Workshop. Robotics Science and Systems ({RSS})",
  author    = "Triebel, Rudolph and Grimmett, Hugo and Paul, Rohan and Posner,
               Ingmar",
  pages     = "809--816",
  year      =  2013,
  keywords  = "
               introspection;Safety\_AI;robotics;image\_classification;ai\_learning;assurance\_competence;GPs;supervised\_learning;assurance\_implicit;trust\_informal\_treatment;active\_learning;prob\_class;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INCOLLECTION{Triebel2016-kj,
  title     = "Driven Learning for Driving: How Introspection Improves Semantic
               Mapping",
  booktitle = "Robotics Research",
  author    = "Triebel, Rudolph and Grimmett, Hugo and Paul, Rohan and Posner,
               Ingmar",
  editor    = "Inaba, Masayuki and Corke, Peter",
  abstract  = "This paper explores the suitability of commonly employed
               classification methods to action-selection tasks in robotics,
               and argues that a classifier's introspective capacity is a vital
               but as yet largely under-appreciated attribute. As illustration
               we propose an active learning framework for semantic mapping in
               mobile robotics and demonstrate it in the context of autonomous
               driving. In this framework, data are selected for label
               disambiguation by a human supervisor using uncertainty sampling.
               Intuitively, an introspective classification framework---i.e.
               one which moderates its predictions by an estimate of how well
               it is placed to make a call in a particular situation---is
               particularly well suited to this task. To achieve an efficient
               implementation we extend the notion of introspection to a
               particular sparse Gaussian Process Classifier, the Informative
               Vector Machine (IVM). Furthermore, we leverage the
               information-theoretic nature of the IVM to formulate a
               principled mechanism for forgetting stale data, thereby bounding
               memory use and resulting in a truly life-long learning system.
               Our evaluation on a publicly available dataset shows that an
               introspective active learner asks more informative questions
               compared to a more traditional non-introspective approach like a
               Support Vector Machine (SVM) and in so doing, outperforms the
               SVM in terms of learning rate while retaining efficiency for
               practical use.",
  publisher = "Springer International Publishing",
  pages     = "449--465",
  series    = "Springer Tracts in Advanced Robotics",
  year      =  2016,
  keywords  = "
               introspection;Safety\_AI;robotics;image\_classification;ai\_learning;assurance\_competence;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;prob\_class;Supplemental
               Assurance;Quantify Uncertainty;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Berczi2015-rd,
  title     = "Learning to assess terrain from human demonstration using an
               introspective Gaussian-process classifier",
  booktitle = "2015 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Berczi, L P and Posner, I and Barfoot, T D",
  abstract  = "This paper presents an approach to learning robot terrain
               assessment from human demonstration. An operator drives a robot
               for a short period of time, supervising the gathering of
               traversable and untraversable terrain data. After this initial
               training period, the robot can then predict the traversability
               of new terrain based on its experiences. We improve on current
               methods in two ways: first, we maintain a richer
               (higher-dimensional) representation of the terrain that is
               better able to distinguish between different training examples.
               Second, we use a Gaussian-process classifier for terrain
               assessment due to its superior introspective abilities (leading
               to better uncertainty estimates) when compared to other
               classifier methods in the literature. Our method is tested on
               real data and shown to outperform current methods both in
               classification accuracy and uncertainty estimation.",
  pages     = "3178--3185",
  month     =  may,
  year      =  2015,
  keywords  = "Gaussian processes;image classification;learning (artificial
               intelligence);mobile robots;robot vision;terrain mapping;human
               demonstration;initial training period;introspective
               Gaussian-process classifier;introspective abilities;mobile
               robots;robot terrain assessment learning;terrain
               representation;traversability prediction;traversable terrain
               data gathering;untraversable terrain data
               gathering;Labeling;Learning systems;Robot sensing
               systems;Training;Uncertainty;introspection;robotics;ai\_perception;ai\_learning;supervised\_learning;image\_classification;assurance\_competence;GPs;trust\_informal\_treatment;assurance\_implicit;prob\_class;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@ARTICLE{Grimmett2016-yc,
  title    = "Introspective classification for robot perception",
  author   = "Grimmett, Hugo and Triebel, Rudolph and Paul, Rohan and Posner,
              Ingmar",
  abstract = "In robotics, the use of a classification framework which produces
              scores with inappropriate confidences will ultimately lead to the
              robot making dangerous decisions. In order to select a framework
              which will make the best decisions, we should pay careful
              attention to the ways in which it generates scores. Precision and
              recall have been widely adopted as canonical metrics to quantify
              the performance of learning algorithms, but for robotics
              applications involving mission-critical decision making, good
              performance in relation to these metrics is insufficient. We
              introduce and motivate the importance of a classifier's
              introspective capacity: the ability to associate an appropriate
              assessment of confidence with any test case. We propose that a
              key ingredient for introspection is a framework's potential to
              increase its uncertainty with the distance between a test datum
              its training data. We compare the introspective capacities of a
              number of commonly used classification frameworks in both
              classification and detection tasks, and show that better
              introspection leads to improved decision making in the context of
              tasks such as autonomous driving or semantic map generation.",
  journal  = "Int. J. Rob. Res.",
  volume   =  35,
  number   =  7,
  pages    = "743--762",
  year     =  2016,
  keywords = "
              introspection;Safety\_AI;image\_classification;assurance\_competence;ai\_learning;ai\_reasoning;ai\_knowledge\_rep;robotics;GPs;supervised\_learning;assurance\_implicit;trust\_informal\_treatment;prob\_class;Supplemental
              Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Dequaire2016-kh,
  title     = "Off the beaten track: Predicting localisation performance in
               visual teach and repeat",
  booktitle = "2016 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Dequaire, J and Tong, C H and Churchill, W and Posner, I",
  abstract  = "This paper proposes an appearance-based approach to estimating
               localisation performance in the context of visual teach and
               repeat. Specifically, it aims to estimate the likely corridor
               around a taught trajectory within which a vision-based
               localisation system is still able to localise itself. In
               contrast to prior art, our system is able to predict this
               localisation envelope for trajectories in similar, yet
               geographically distant locations where no repeat runs have yet
               been performed. Thus, by characterising the localisation
               performance in one region, we are able to predict performance in
               another. To achieve this, we leverage a Gaussian Process
               regressor to estimate the likely number of feature matches for
               any keyframe in the teach run, based on a combination of
               trajectory properties such as curvature and an appearance model
               of the keyframe. Using data from real traversals, we demonstrate
               that our approach performs as well as prior art when it comes to
               interpolating localisation performance based on a number of
               repeat runs, while also performing well at generalising
               performance estimation to freshly taught trajectories.",
  pages     = "795--800",
  month     =  may,
  year      =  2016,
  keywords  = "Gaussian processes;interpolation;robot vision;Gaussian process
               regressor;appearance-based approach;interpolation;vision-based
               localisation;vision-based localisation system;Cameras;Gaussian
               processes;Planning;Robots;Training;Trajectory;Visualization;ai\_planning;ai\_learning;ai\_motion\_manipulation;robotics;image\_classification;assurance\_competence;ai\_perception;GPs;trust\_informal\_treatment;assurance\_implicit;prob\_class;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Gurau2016-hs,
  title           = "Fit for Purpose? Predicting Perception Performance Based
                     on Past Experience",
  booktitle       = "2016 International Symposium on Experimental Robotics",
  author          = "Gur{\u a}u, Corina and Tong, Chi Hay and Posner, Ingmar",
  abstract        = "This paper explores the idea of predicting the likely
                     performance of a robot's perception system based on past
                     experience in the same workspace. In particular, we
                     propose to build a place-specific model of perception
                     performance from observations gathered over time. We
                     evaluate our method in a classical decision making
                     scenario in which the robot must choose when and where to
                     drive autonomously in 60 km of driving data from an urban
                     environment. We demonstrate that leveraging visual
                     appearance within a state-of-the-art navigation framework
                     increases the accuracy of our performance predictions.",
  publisher       = "Springer, Cham",
  pages           = "454--464",
  month           =  oct,
  year            =  2016,
  keywords        = "ai\_reasoning;ai\_perception;human-robot
                     team;assurance\_competence;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;Supplemental
                     Assurance;Quantify Uncertainty;Assurances",
  language        = "en",
  conference      = "International Symposium on Experimental Robotics"
}

@ARTICLE{Morrison2011-yf,
  title         = "Optimal Data Split Methodology for Model Validation",
  author        = "Morrison, Rebecca and Bryant, Corey and Terejanu, Gabriel
                   and Miki, Kenji and Prudhomme, Serge",
  abstract      = "The decision to incorporate cross-validation into validation
                   processes of mathematical models raises an immediate
                   question - how should one partition the data into
                   calibration and validation sets? We answer this question
                   systematically: we present an algorithm to find the optimal
                   partition of the data subject to certain constraints. While
                   doing this, we address two critical issues: 1) that the
                   model be evaluated with respect to predictions of a given
                   quantity of interest and its ability to reproduce the data,
                   and 2) that the model be highly challenged by the validation
                   set, assuming it is properly informed by the calibration
                   set. This framework also relies on the interaction between
                   the experimentalist and/or modeler, who understand the
                   physical system and the limitations of the model; the
                   decision-maker, who understands and can quantify the cost of
                   model failure; and the computational scientists, who strive
                   to determine if the model satisfies both the modeler's and
                   decision maker's requirements. We also note that our
                   framework is quite general, and may be applied to a wide
                   range of problems. Here, we illustrate it through a specific
                   example involving a data reduction model for an ICCD camera
                   from a shock-tube experiment located at the NASA Ames
                   Research Center (ARC).",
  month         =  aug,
  year          =  2011,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "physics.data-an",
  eprint        = "1108.6043"
}

@ARTICLE{Morrison2018-fz,
  title    = "Representing model inadequacy: A stochastic operator approach",
  author   = "Morrison, Rebecca E and Oliver, Todd A and Moser, Robert D",
  abstract = "Mathematical models of physical systems are subject to many
              uncertainties such as measurement errors and uncertain initial
              and boundary conditions. After accounting for these
              uncertainties, it is often revealed that discrepancies between
              the model output and the observations remain; if so, the model is
              said to be inadequate. In practice, the inadequate model may be
              the best that is available or tractable, and so despite its
              inadequacy the model may be used to make predictions of
              unobserved quantities. In this case, a representation of the
              inadequacy is necessary, so the impact of the observed
              discrepancy can be determined. We investigate this problem in the
              context of chemical kinetics and propose a new technique to
              account for model inadequacy that is both probabilistic and
              physically meaningful. A stochastic inadequacy operator
              $\mathcal\{S\}$ is introduced which is embedded in the ODEs
              describing the evolution of chemical species concentrations and
              which respects certain physical constraints such as conservation
              laws. The parameters of $\mathcal\{S\}$ are governed by
              probability distributions, which in turn are characterized by a
              set of hyperparameters. The model parameters and hyperparameters
              are calibrated using high-dimensional hierarchical Bayesian
              inference. We apply the method to a typical problem in chemical
              kinetics---the reaction mechanism of hydrogen combustion.",
  journal  = "SIAM/ASA Journal on Uncertainty Quantification",
  volume   =  6,
  number   =  2,
  pages    = "457--496",
  year     =  2018,
  keywords = "
              assurance\_competence;reactions;ai\_learning;assurance\_predictability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Integral
              Assurance;Interpretable Models;Assurances"
}

@INPROCEEDINGS{Eggensperger2013-ho,
  title     = "Towards an Empirical Foundation for Assessing Bayesian
               Optimization of Hyperparameters",
  booktitle = "{NIPS} workshop on Bayesian Optimization in Theory and Practice",
  author    = "Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank
               and Bergstra, James and Snoek, Jasper and Hoos, Holger H and
               Leyton-Brown, Kevin",
  abstract  = "Progress in practical Bayesian optimization is hampered by the
               fact that the only available standard benchmarks are artificial
               test functions that are not representative of practical
               applications. To alleviate this problem, we introduce a library
               of benchmarks from the prominent application of hyperparameter
               optimization and use it to compare Spearmint, TPE, and SMAC,
               three recent Bayesian optimization methods for hyperparameter
               optimization.",
  pages     = "1--5",
  year      =  2013,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Bradshaw2013-ck,
  title     = "The Seven Deadly Myths of`` Autonomous Systems''",
  author    = "Bradshaw, Jeffrey M and Hoffman, Robert R and Woods, David D and
               Johnson, Matthew",
  journal   = "IEEE Intell. Syst.",
  publisher = "IEEE",
  volume    =  28,
  number    =  3,
  pages     = "54--61",
  year      =  2013,
  keywords  = "NotRead;Assurances;Assurances/Trust Background"
}

@BOOK{Mittu2016-ia,
  title     = "Robust Intelligence and Trust in Autonomous Systems:",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  publisher = "Springer US",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background"
}

@BOOK{Luger2008-vf,
  title     = "Artificial Intelligence: Structures and Strategies for Complex
               Problem Solving",
  author    = "Luger, George F",
  publisher = "Addison-Wesley Publishing Company",
  edition   = "6th",
  year      =  2008,
  address   = "USA",
  keywords  = "AI;Textbook;TextBooks"
}

@BOOK{Nilsson2009-rp,
  title     = "The Quest for Artificial Intelligence",
  author    = "Nilsson, Nils J",
  abstract  = "Artificial intelligence (AI) is a field within computer science
               that is attempting to build enhanced intelligence into computer
               systems. This book traces the history of the subject, from the
               early dreams of eighteenth-century (and earlier) pioneers to the
               more successful work of today's AI engineers. AI is becoming
               more and more a part of everyone's life. The technology is
               already embedded in face-recognizing cameras, speech-recognition
               software, Internet search engines, and health-care robots, among
               other applications. The book's many diagrams and
               easy-to-understand descriptions of AI programs will help the
               casual reader gain an understanding of how these and other AI
               systems actually work. Its thorough (but unobtrusive)
               end-of-chapter notes containing citations to important source
               materials will be of great use to AI scholars and researchers.
               This book promises to be the definitive history of a field that
               has captivated the imaginations of scientists, philosophers, and
               writers for centuries.",
  publisher = "Cambridge University Press",
  month     =  oct,
  year      =  2009,
  keywords  = "AI;Textbook;TextBooks",
  language  = "en"
}

@BOOK{Poole2010-pv,
  title     = "Artificial Intelligence: Foundations of Computational Agents",
  author    = "Poole, David L and Mackworth, Alan K",
  abstract  = "Recent decades have witnessed the emergence of artificial
               intelligence as a serious science and engineering discipline.
               This textbook, aimed at junior to senior undergraduate students
               and first-year graduate students, presents artificial
               intelligence (AI) using a coherent framework to study the design
               of intelligent computational agents. By showing how basic
               approaches fit into a multidimensional design space, readers can
               learn the fundamentals without losing sight of the bigger
               picture. The book balances theory and experiment, showing how to
               link them intimately together, and develops the science of AI
               together with its engineering applications. Although structured
               as a textbook, the book's straightforward, self-contained style
               will also appeal to a wide audience of professionals,
               researchers, and independent learners. AI is a rapidly
               developing field: this book encapsulates the latest results
               without being exhaustive and encyclopedic. The text is supported
               by an online learning environment, AIspace, http://aispace.org,
               so that students can experiment with the main AI algorithms plus
               problems, animations, lecture slides, and a knowledge
               representation system, AIlog, for experimentation and problem
               solving.",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  2010,
  keywords  = "AI;Textbook;TextBooks",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Russell2010-wv,
  title     = "Artificial Intelligence: A Modern Approach",
  author    = "Russell, Stuart Jonathan and Norvig, Peter",
  abstract  = "Artificial Intelligence: A Modern Approach, 3e offers the most
               comprehensive, up-to-date introduction to the theory and
               practice of artificial intelligence. Number one in its field,
               this textbook is ideal for one or two-semester, undergraduate or
               graduate-level courses in Artificial Intelligence. Dr. Peter
               Norvig, contributing Artificial Intelligence author and
               Professor Sebastian Thrun, a Pearson author are offering a free
               online course at Stanford University on artificial intelligence.
               According to an article in The New York Times , the course on
               artificial intelligence is ``one of three being offered
               experimentally by the Stanford computer science department to
               extend technology knowledge and skills beyond this elite campus
               to the entire world.'' One of the other two courses, an
               introduction to database software, is being taught by Pearson
               author Dr. Jennifer Widom. Artificial Intelligence: A Modern
               Approach, 3e is available to purchase as an eText for your
               Kindle™, NOOK™, and the
               iPhone\textregistered{}/iPad\textregistered{}. To learn more
               about the course on artificial intelligence, visit
               http://www.ai-class.com. To read the full New York Times
               article, click here.",
  publisher = "Prentice Hall",
  series    = "Artificial Intelligence",
  edition   = "Third",
  year      =  2010,
  keywords  = "AI;Textbook;TextBooks;Robotics/(PO)MDPs",
  language  = "en"
}

@INPROCEEDINGS{Franklin1996-nx,
  title       = "Is it an Agent, or just a Program?: A Taxonomy for Autonomous
                 Agents",
  booktitle   = "International Workshop on Agent Theories, Architectures, and
                 Languages",
  author      = "Franklin, Stan and Graesser, Art",
  abstract    = "Abstract The advent of software agents gave rise to much
                 discussion of just what such an agent is, and of how they
                 differ from programs in general. Here we propose a formal
                 definition of an autonomous agent which clearly distinguishes
                 a software agent from just any",
  publisher   = "Springer",
  pages       = "21--35",
  institution = "Springer",
  year        =  1996,
  keywords    = "AI;visionary\_paper"
}

@ARTICLE{Wooldridge1995-sh,
  title     = "Intelligent agents: Theory and practice",
  author    = "Wooldridge, Michael and Jennings, Nicholas R",
  abstract  = "Abstract The concept of an agent has become important in both
               artificial intelligence (AT) and mainstream computer science.
               Our aim in this paper is to point the reader at what we perceive
               to be the most important theoretical and practical issues
               associated with the design",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge Univ Press",
  volume    =  10,
  number    =  02,
  pages     = "115--152",
  year      =  1995,
  keywords  = "AI;NotRead"
}

@INPROCEEDINGS{Da_Veiga2012-gh,
  title     = "Gaussian process modeling with inequality constraints",
  booktitle = "Annales de la Facult{\'e} des Sciences de Toulouse",
  author    = "Da Veiga, S{\'e}bastien and Marrel, Amandine",
  volume    =  21,
  pages     = "529--555",
  year      =  2012,
  keywords  = "
               assurance\_predictability;ai\_knowledge\_rep;ai\_reasoning;GPs;assurance\_normality;assurance\_implicit;trust\_informal\_treatment;V\&V;in\_paper;Integral
               Assurance;Value Alignment;BayesOpt;Assurances"
}

@ARTICLE{Tallis1961-ar,
  title     = "The Moment Generating Function of the Truncated Multi-normal
               Distribution",
  author    = "Tallis, G M",
  abstract  = "In this paper the moment generating function (m.g.f.) of the
               truncated n-dimensional normal distribution is obtained. From
               the m.g.f., formulae for E(Xi) and E(Xi Xj) are derived, and are
               used to investigate certain special cases. Some applications of
               these results to statistical genetics are also discussed.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  23,
  number    =  1,
  pages     = "223--229",
  year      =  1961,
  keywords  = "BayesOpt"
}

@ARTICLE{Alcala-Fdez2009-fu,
  title     = "{KEEL}: a software tool to assess evolutionary algorithms for
               data mining problems",
  author    = "Alcal{\'a}-Fdez, J and S{\'a}nchez, L and Garc{\'\i}a, S and del
               Jesus, M J and Ventura, S and Garrell, J M and Otero, J and
               Romero, C and Bacardit, J and Rivas, V M and Fern{\'a}ndez, J C
               and Herrera, F",
  abstract  = "This paper introduces a software tool named KEEL which is a
               software tool to assess evolutionary algorithms for Data Mining
               problems of various kinds including as regression,
               classification, unsupervised learning, etc. It includes
               evolutionary learning algorithms based on different approaches:
               Pittsburgh, Michigan and IRL, as well as the integration of
               evolutionary learning techniques with different pre-processing
               techniques, allowing it to perform a complete analysis of any
               learning model in comparison to existing software tools.
               Moreover, KEEL has been designed with a double goal: research
               and educational.",
  journal   = "Soft Comput",
  publisher = "Springer-Verlag",
  volume    =  13,
  number    =  3,
  pages     = "307--318",
  month     =  feb,
  year      =  2009,
  keywords  = "meh..",
  language  = "en"
}

@ARTICLE{Guyon2003-fj,
  title     = "An Introduction to Variable and Feature Selection",
  author    = "Guyon, Isabelle and Elisseeff, Andr{\'e}",
  abstract  = "Abstract Variable and feature selection have become the focus of
               much research in areas of application for which datasets with
               tens or hundreds of thousands of variables are available. These
               areas include text processing of internet documents, gene
               expression array analysis, and combinatorial chemistry. The
               objective of variable selection is three-fold: improving the
               prediction performance of the predictors, providing faster and
               more cost-effective ...",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  3,
  number    = "Mar",
  pages     = "1157--1182",
  year      =  2003,
  keywords  = "
               interpretability;assurances;ai\_learning;ai\_reasoning;assurance\_implicit;trust\_informal\_treatment;rep\_learn;Integral
               Assurance;Value Alignment;Assurances"
}

@ARTICLE{Lewicki1995-sx,
  title     = "Trust in relationships",
  author    = "Lewicki, Roy J and Bunker, Barbara Benedict",
  abstract  = "Roy J. Lewicki, Barbara Benedict Bunker s trust easier to
               destroy than it is to build? The presumption that the answer to
               this question is a resounding yes has been prevalent in
               cooperation and conflicr management research for over thirty
               years. Observation of the",
  journal   = "Adm. Sci. Q.",
  publisher = "researchgate.net",
  volume    =  5,
  pages     = "583--601",
  year      =  1995,
  keywords  = "trust\_definition;business;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{McKnight2002-qx,
  title     = "Developing and Validating Trust Measures for e-Commerce: An
               Integrative Typology",
  author    = "McKnight, D Harrison and Choudhury, Vivek and Kacmar, Charles",
  abstract  = "Evidence suggests that consumers often hesitate to transact with
               Web-based vendors because of uncertainty about vendor behavior
               or the perceived risk of having personal information stolen by
               hackers. Trust plays a central role in helping consumers
               overcome perceptions of risk and insecurity. Trust makes
               consumers comfortable sharing personal information, making
               purchases, and acting on Web vendor advice---behaviors essential
               to widespread adoption of e-commerce. Therefore, trust is
               critical to both researchers and practitioners. Prior research
               on e-commerce trust has used diverse, incomplete, and
               inconsistent definitions of trust, making it difficult to
               compare results across studies. This paper contributes by
               proposing and validating measures for a multidisciplinary,
               multidimensional model of trust in e-commerce. The model
               includes four high-level constructs---disposition to trust,
               institution-based trust, trusting beliefs, and trusting
               intentions---which are further delineated into 16 measurable,
               literature-grounded subconstructs. The psychometric properties
               of the measures are demonstrated through use of a hypothetical,
               legal advice Web site. The results show that trust is indeed a
               multidimensional concept. Proposed relationships among the trust
               constructs are tested (for internal nomological validity), as
               are relationships between the trust constructs and three other
               e-commerce constructs (for external nomological validity), as
               Web experience, personal innovativeness, and Web site quality.
               Suggestions for future research as well as implications for
               practice are discussed.",
  journal   = "Information Systems Research",
  publisher = "pubsonline.informs.org",
  volume    =  13,
  number    =  3,
  pages     = "334--359",
  year      =  2002,
  keywords  = "trust\_definition;e-commerce;Assurances"
}

@INPROCEEDINGS{Van_Belle2013-ph,
  title     = "Research directions in interpretable machine learning models",
  booktitle = "{ESANN}",
  author    = "Van Belle, Vanya and Lisboa, Paulo",
  year      =  2013,
  keywords  = "interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Assurances"
}

@ARTICLE{Van_Belle2012-dt,
  title    = "A mathematical model for interpretable clinical decision support
              with applications in gynecology",
  author   = "Van Belle, Vanya M C A and Van Calster, Ben and Timmerman, Dirk
              and Bourne, Tom and Bottomley, Cecilia and Valentin, Lil and
              Neven, Patrick and Van Huffel, Sabine and Suykens, Johan A K and
              Boyd, Stephen",
  abstract = "BACKGROUND: Over time, methods for the development of clinical
              decision support (CDS) systems have evolved from interpretable
              and easy-to-use scoring systems to very complex and
              non-interpretable mathematical models. In order to accomplish
              effective decision support, CDS systems should provide
              information on how the model arrives at a certain decision. To
              address the issue of incompatibility between performance,
              interpretability and applicability of CDS systems, this paper
              proposes an innovative model structure, automatically leading to
              interpretable and easily applicable models. The resulting models
              can be used to guide clinicians when deciding upon the
              appropriate treatment, estimating patient-specific risks and to
              improve communication with patients. METHODS AND FINDINGS: We
              propose the interval coded scoring (ICS) system, which imposes
              that the effect of each variable on the estimated risk is
              constant within consecutive intervals. The number and position of
              the intervals are automatically obtained by solving an
              optimization problem, which additionally performs variable
              selection. The resulting model can be visualised by means of
              appealing scoring tables and color bars. ICS models can be used
              within software packages, in smartphone applications, or on
              paper, which is particularly useful for bedside medicine and
              home-monitoring. The ICS approach is illustrated on two
              gynecological problems: diagnosis of malignancy of ovarian tumors
              using a dataset containing 3,511 patients, and prediction of
              first trimester viability of pregnancies using a dataset of 1,435
              women. Comparison of the performance of the ICS approach with a
              range of prediction models proposed in the literature illustrates
              the ability of ICS to combine optimal performance with the
              interpretability of simple scoring systems. CONCLUSIONS: The ICS
              approach can improve patient-clinician communication and will
              provide additional insights in the importance and influence of
              available variables. Future challenges include extensions of the
              proposed methodology towards automated detection of interaction
              effects, multi-class decision support systems, prognosis and
              high-dimensional data.",
  journal  = "PLoS One",
  volume   =  7,
  number   =  3,
  pages    = "e34312",
  month    =  mar,
  year     =  2012,
  keywords = "
              interpretability;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;review;Integral
              Assurance;Interpretable Models;Assurances",
  language = "en"
}

@ARTICLE{Freitas2006-qo,
  title     = "Are we really discovering interesting knowledge from data",
  author    = "Freitas, Alex A",
  journal   = "Expert Update (the BCS-SGAI magazine)",
  publisher = "The British Computer Society",
  volume    =  9,
  number    =  1,
  pages     = "41--47",
  year      =  2006,
  keywords  = "
               interpretability;assurance\_explicit;human\_study;trust\_informal\_treatment;human\_involved;Integral
               Assurance;Value Alignment;Assurances"
}

@ARTICLE{Sugiyama2013-ci,
  title     = "Learning under nonstationarity: covariate shift and
               class-balance change",
  author    = "Sugiyama, Masashi and Yamada, Makoto and du Plessis, Marthinus
               Christoffel",
  abstract  = "The goal of supervised learning such as regression and
               classification is to learn an input-- output dependency from
               input--output paired training samples so that test output y for
               unseen test input x can be accurately estimated. Various
               supervised learning algorithms were developed thus far, and they
               have been demonstrated to be useful in a wide range of
               applications. Most of the popular machine-learning algorithms
               assume that training and ...",
  journal   = "Wiley Interdiscip. Rev. Comput. Stat.",
  publisher = "Wiley Online Library",
  volume    =  5,
  number    =  6,
  pages     = "465--477",
  year      =  2013,
  keywords  = "
               Safety\_AI;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;risk\_safety\_nonstationary;in\_paper;Integral
               Assurance;Value Alignment;Assurances"
}

@BOOK{Sugiyama2012-mo,
  title     = "Machine Learning in Non-stationary Environments: Introduction to
               Covariate Shift Adaptation",
  author    = "Sugiyama, Masashi and Kawanabe, Motoaki",
  abstract  = "Theory, algorithms, and applications of machine learning
               techniques to overcome ``covariate shift'' non-stationarity. As
               the power of computing has grown over the past few decades, the
               field of machine learning has advanced rapidly in both theory
               and practice. Machine learning methods are usually based on the
               assumption that the data generation mechanism does not change
               over time. Yet real-world applications of machine learning,
               including image recognition, natural language processing, speech
               recognition, robot control, and bioinformatics, often violate
               this common assumption. Dealing with non-stationarity is one of
               modern machine learning's greatest challenges. This book focuses
               on a specific non-stationary environment known as covariate
               shift, in which the distributions of inputs (queries) change but
               the conditional distribution of outputs (answers) is unchanged,
               and presents machine learning theory, algorithms, and
               applications to overcome this variety of non-stationarity. After
               reviewing the state-of-the-art research in the field, the
               authors discuss topics that include learning under covariate
               shift, model selection, importance estimation, and active
               learning. They describe such real world applications of
               covariate shift adaption as brain-computer interface, speaker
               identification, and age prediction from facial images. With this
               book, they aim to encourage future research in machine learning,
               statistics, and engineering that strives to create truly
               autonomous learning machines able to learn under
               non-stationarity.",
  publisher = "MIT Press",
  year      =  2012,
  keywords  = "Safety\_AI;NotRead;Assurances",
  language  = "en"
}

@ARTICLE{Garcia2015-rs,
  title     = "A comprehensive survey on safe reinforcement learning",
  author    = "Garc{\i}a, J and Fern{\'a}ndez, F",
  abstract  = "Abstract Safe Reinforcement Learning can be defined as the
               process of learning policies that maximize the expectation of
               the return in problems in which it is important to ensure
               reasonable system performance and/or respect safety constraints
               during the learning and/or",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2015,
  keywords  = "
               Safety\_AI;trust\_informal\_treatment;assurance\_implicit;risk\_safety\_nonstationary;in\_paper;in\_presentation;Integral
               Assurance;Value Alignment;Assurances"
}

@INPROCEEDINGS{Zycinski2012-jj,
  title       = "Discriminant functional gene groups identification with
                 machine learning and prior knowledge",
  booktitle   = "{ESANN}",
  author      = "Zycinski, Grzegorz and Squillario, Margherita and Barla,
                 Annalisa and Sanavia, Tiziana and Verri, Alessandro and Di
                 Camillo, Barbara",
  institution = "Citeseer",
  year        =  2012,
  keywords    = "
                 interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Integral
                 Assurance;Interpretable Models;Assurances"
}

@INPROCEEDINGS{Kastner2012-yu,
  title     = "Integration of Structural Expert Knowledge about Classes for
               Classification Using the Fuzzy Supervised Neural Gas",
  booktitle = "{ESANN}",
  author    = "K{\"a}stner, Marika and Hermann, Wieland and Villmann, Thomas
               and Mittweida, Saxonia-Germany and Zwickau, Saxonia-Germany",
  year      =  2012,
  keywords  = "in\_table"
}

@BOOK{Venna2007-yj,
  title     = "Dimensionality reduction for visual exploration of similarity
               structures",
  author    = "Venna, Jarkko",
  publisher = "Helsinki University of Technology",
  year      =  2007,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_explicit;classification;vis\_dr;Supplemental
               Assurance;Assurances"
}

@ARTICLE{Park2016-ld,
  title         = "{ACDC}: {$\alpha$-Carving} Decision Chain for Risk
                   Stratification",
  author        = "Park, Yubin and Ho, Joyce and Ghosh, Joydeep",
  abstract      = "In many healthcare settings, intuitive decision rules for
                   risk stratification can help effective hospital resource
                   allocation. This paper introduces a novel variant of
                   decision tree algorithms that produces a chain of decisions,
                   not a general tree. Our algorithm, $\alpha$-Carving Decision
                   Chain (ACDC), sequentially carves out ``pure'' subsets of
                   the majority class examples. The resulting chain of decision
                   rules yields a pure subset of the minority class examples.
                   Our approach is particularly effective in exploring large
                   and class-imbalanced health datasets. Moreover, ACDC
                   provides an interactive interpretation in conjunction with
                   visual performance metrics such as Receiver Operating
                   Characteristics curve and Lift chart.",
  month         =  jun,
  year          =  2016,
  keywords      = "
                   medical;assurance\_predictability;ai\_knowledge\_rep;ai\_learning;ai\_perception;ai\_motion\_manipulation;ai\_interaction;decision\_tree;trust\_informal\_treatment;assurance\_explicit;interp\_models;Integral
                   Assurance;Interpretable Models;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.05325"
}

@INPROCEEDINGS{Turner2016-jq,
  title     = "A model explanation system",
  booktitle = "2016 {IEEE} 26th International Workshop on Machine Learning for
               Signal Processing ({MLSP})",
  author    = "Turner, R",
  abstract  = "We propose a new methodology for explaining the predictions of
               black box classifiers. We use the motivating paradigm that
               predictive performance is of primary importance but human
               analysts (e.g., in fraud detection) desire a classifier's
               predictions to be augmented with useful explanations. To be
               truly general and principled, we derive a scoring system for
               finding explanations based on formal requirements. In this
               system, the explanations are assumed to take the form of simple
               logical statements. We derive an efficient Monte Carlo algorithm
               to find explanations for black box classifiers with finite
               sample guarantees. The methodology is then applied to
               interesting examples in facial recognition and credit data.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1--6",
  year      =  2016,
  keywords  = "Monte Carlo methods;pattern classification;Monte Carlo
               algorithm;black box classifiers;classifier predictions;credit
               data;facial recognition;finite sample guarantees;formal
               requirements;logical statements;model explanation
               system;predictive performance;scoring system;Computational
               modeling;Credit cards;Data models;Decision trees;Face
               recognition;History;Monte Carlo
               methods;finance;assurance\_predictability;ai\_learning;monte\_carlo;image\_classification;trust\_informal\_treatment;assurance\_explicit;classification;explain;Assurances"
}

@ARTICLE{Turner2016-ao,
  title         = "A Model Explanation System: Latest Updates and Extensions",
  author        = "Turner, Ryan",
  abstract      = "We propose a general model explanation system (MES) for
                   ``explaining'' the output of black box classifiers. This
                   paper describes extensions to Turner (2015), which is
                   referred to frequently in the text. We use the motivating
                   example of a classifier trained to detect fraud in a credit
                   card transaction history. The key aspect is that we provide
                   explanations applicable to a single prediction, rather than
                   provide an interpretable set of parameters. We focus on
                   explaining positive predictions (alerts). However, the
                   presented methodology is symmetrically applicable to
                   negative predictions.",
  month         =  jun,
  year          =  2016,
  keywords      = "interpretability;legal;classification;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.09517"
}

@MISC{Goodrum_2016-fm,
  title        = "Finding Balance: Model Accuracy vs. Interpretability in
                  Regulated Environments",
  author       = "Goodrum, Will",
  abstract     = "Blog by Elder Research Data Scientist Will Goodrum highlights
                  the trade off of predictive model interpretability vs. model
                  accuracy in regulated environments.",
  month        =  nov,
  year         =  2016,
  howpublished = "\url{http://www.elderresearch.com/company/blog/predictive-model-accuracy-versus-interpretability}",
  note         = "Accessed: 2017-3-16",
  keywords     = "interpretability;trust\_popular\_media;in\_table;trust\_academic\_conversation;Assurances"
}

@ARTICLE{Obermann2016-xp,
  title     = "Interpretable Multiclass Models for Corporate Credit Rating
               Capable of Expressing Doubt",
  author    = "Obermann, Lennart and Waack, Stephan",
  abstract  = "Corporate credit rating is a process to classify commercial
               enterprises based on their creditworthiness. Machine learning
               algorithms can construct classification models, but in general
               they do not tend to be 100\% accurate. Since they can be used as
               decision support",
  journal   = "Frontiers in Applied Mathematics and Statistics",
  publisher = "Frontiers",
  volume    =  2,
  pages     = "16",
  year      =  2016,
  keywords  = "interpretability;transparency;in\_table"
}

@ARTICLE{Vellido2012-nm,
  title     = "Making machine learning models interpretable",
  author    = "Vellido, A and Mart{\'\i}n-Guerrero, J D and Lisboa, Pjg",
  abstract  = "Abstract. Data of different levels of complexity and of ever
               growing diversity of characteristics are the raw materials that
               machine learning practitioners try to model using their wide
               palette of methods and tools. The obtained models are meant to
               be a synthetic representation of the available, observed data
               that captures some of their intrinsic regularities or patterns.
               Therefore, the use of machine learning techniques for data
               analysis can be understood as ...",
  journal   = "ESANN",
  publisher = "Citeseer",
  year      =  2012,
  keywords  = "
               interpretability;ML\_theory;trust\_informal\_treatment;assurance\_explicit;vis\_dr;Integral
               Assurance;Supplemental Assurance;Interpretable Models;Assurances"
}

@ARTICLE{Jovanovic2016-gw,
  title    = "Building interpretable predictive models for pediatric hospital
              readmission using {Tree-Lasso} logistic regression",
  author   = "Jovanovic, Milos and Radovanovic, Sandro and Vukicevic, Milan and
              Van Poucke, Sven and Delibasic, Boris",
  abstract = "AbstractObjectives Quantification and early identification of
              unplanned readmission risk have the potential to improve the
              quality of care during hospitalization and after discharge.
              However, high dimensionality, sparsity, and class imbalance of
              electronic health data and the complexity of risk quantification,
              challenge the development of accurate predictive models.
              Predictive models require a certain level of interpretability in
              order to be applicable in real settings and create actionable
              insights. This paper aims to develop accurate and interpretable
              predictive models for readmission in a general pediatric patient
              population, by integrating a data-driven model (sparse logistic
              regression) and domain knowledge based on the international
              classification of diseases 9th---revision clinical modification
              (ICD-9-CM) hierarchy of diseases. Additionally, we propose a way
              to quantify the interpretability of a model and inspect the
              stability of alternative solutions. Materials and methods The
              analysis was conducted on >66,000 pediatric hospital discharge
              records from California, State Inpatient Databases, Healthcare
              Cost and Utilization Project between 2009 and 2011. We
              incorporated domain knowledge based on the ICD-9-CM hierarchy in
              a data driven, Tree-Lasso regularized logistic regression model,
              providing the framework for model interpretation. This approach
              was compared with traditional Lasso logistic regression resulting
              in models that are easier to interpret by fewer high-level
              diagnoses, with comparable prediction accuracy. Results The
              results revealed that the use of a Tree-Lasso model was as
              competitive in terms of accuracy (measured by area under the
              receiver operating characteristic curve---AUC) as the traditional
              Lasso logistic regression, but integration with the ICD-9-CM
              hierarchy of diseases provided more interpretable models in terms
              of high-level diagnoses. Additionally, interpretations of models
              are in accordance with existing medical understanding of
              pediatric readmission. Best performing models have similar
              performances reaching AUC values 0.783 and 0.779 for traditional
              Lasso and Tree-Lasso, respectfully. However, information loss of
              Lasso models is 0.35 bits higher compared to Tree-Lasso model.
              Conclusions We propose a method for building predictive models
              applicable for the detection of readmission risk based on
              Electronic Health records. Integration of domain knowledge (in
              the form of ICD-9-CM taxonomy) and a data-driven, sparse
              predictive algorithm (Tree-Lasso Logistic Regression) resulted in
              an increase of interpretability of the resulting model. The
              models are interpreted for the readmission prediction problem in
              general pediatric population in California, as well as several
              important subpopulations, and the interpretations of models
              comply with existing medical understanding of pediatric
              readmission. Finally, quantitative assessment of the
              interpretability of the models is given, that is beyond simple
              counts of selected low-level features.",
  journal  = "Artif. Intell. Med.",
  volume   =  72,
  pages    = "12--21",
  year     =  2016,
  keywords = "Lasso regression; Tree Lasso regression; Model interpretability;
              Hospital readmission
              prediction;interpretability;medical;in\_table;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Integral
              Assurance;Interpretable Models;Assurances"
}

@ARTICLE{Choi2016-by,
  title         = "{RETAIN}: An Interpretable Predictive Model for Healthcare
                   using Reverse Time Attention Mechanism",
  author        = "Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A
                   and Schuetz, Andy and Stewart, Walter F and Sun, Jimeng",
  abstract      = "Accuracy and interpretability are two dominant features of
                   successful predictive models. Typically, a choice must be
                   made in favor of complex black box models such as recurrent
                   neural networks (RNN) for accuracy versus less accurate but
                   more interpretable traditional models such as logistic
                   regression. This tradeoff poses challenges in medicine where
                   both accuracy and interpretability are important. We
                   addressed this challenge by developing the REverse Time
                   AttentIoN model (RETAIN) for application to Electronic
                   Health Records (EHR) data. RETAIN achieves high accuracy
                   while remaining clinically interpretable and is based on a
                   two-level neural attention model that detects influential
                   past visits and significant clinical variables within those
                   visits (e.g. key diagnoses). RETAIN mimics physician
                   practice by attending the EHR data in a reverse time order
                   so that recent clinical visits are likely to receive higher
                   attention. RETAIN was tested on a large health system EHR
                   dataset with 14 million visits completed by 263K patients
                   over an 8 year period and demonstrated predictive accuracy
                   and computational scalability comparable to state-of-the-art
                   methods such as RNN, and ease of interpretability comparable
                   to traditional models.",
  month         =  aug,
  year          =  2016,
  keywords      = "
                   medical;assurance\_predictability;assurance\_competence;ai\_reasoning;trust\_informal\_treatment;assurance\_explicit;interp\_models;Integral
                   Assurance;Interpretable Models;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1608.05745"
}

@ARTICLE{Swartout1983-ko,
  title    = "{XPLAIN}: a system for creating and explaining expert consulting
              programs",
  author   = "Swartout, William R",
  abstract = "Traditional methods for explaining programs provide explanations
              by converting the code of the program or traces of its execution
              to English. While such methods can sometimes adequately explain
              program behavior, they typically cannot provide justifications
              for that behavior. That is, such systems cannot tell why what the
              system is doing is a reasonable thing to be doing. The problem is
              that the knowledge required to provide these justifications was
              used to produce the program but is itself not recorded as part of
              the code, and hence is unavailable. The XPLAIN system uses an
              automatic programmer to generate a consulting program by
              refinement from abstract goals. The automatic programmer uses a
              domain model, consisting of descriptive facts about the
              application domain, and a set of domain principles which
              prescribe behavior and drive the refinement process forward. By
              examining the refinement structure created by the automatic
              programmer, XPLAIN provides justifications of the code. XPLAIN
              has been used to re-implement major portions of a Digitalis
              Therapy Advisor and provides superior explanations of its
              behavior.",
  journal  = "Artif. Intell.",
  volume   =  21,
  number   =  3,
  pages    = "285--325",
  month    =  sep,
  year     =  1983,
  keywords = "
              interpretability;medical;trust\_informal\_treatment;assurance\_explicit;explain;in\_paper;Supplemental
              Assurance;Integral Assurance;Reduce Complexity;Assurances"
}

@ARTICLE{Lipton2016-dq,
  title         = "Combating Reinforcement Learning's Sisyphean Curse with
                   Intrinsic Fear",
  author        = "Lipton, Zachary C and Gao, Jianfeng and Li, Lihong and Chen,
                   Jianshu and Deng, Li",
  abstract      = "To use deep reinforcement learning in the wild, we might
                   hope for an agent that can avoid catastrophic mistakes.
                   Unfortunately, even in simple environments, the popular deep
                   Q-network (DQN) algorithm is doomed by a Sisyphean curse.
                   Owing to the use of function approximation, these agents
                   eventually forget experiences as they become exceedingly
                   unlikely under a new policy. Consequently, for as long as
                   they continue to train, DQNs may periodically relive
                   catastrophic mistakes. In this paper, we demonstrate
                   unacceptable performance of DQNs on two toy problems. We
                   then introduce intrinsic fear, a new method that mitigates
                   these problems by avoiding states deemed dangerous. Our
                   approach incorporates a second model trained via supervised
                   learning to predict the probability of catastrophe within a
                   short number of steps. This score then acts to penalize the
                   Q-learning objective, shaping the reward function away from
                   catastrophic states.",
  month         =  nov,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_implicit;risk\_safety\_nonstationary;Integral
                   Assurance;Value Alignment;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.01211"
}

@INPROCEEDINGS{Mikolov2013-lt,
  title     = "Distributed Representations of Words and Phrases and their
               Compositionality",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
               Greg S and Dean, Jeff",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  pages     = "3111--3119",
  year      =  2013,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_implicit;rep\_learn;Integral
               Assurance;Value Alignment;Assurances"
}

@ARTICLE{Karpathy2017-xv,
  title    = "Deep {Visual-Semantic} Alignments for Generating Image
              Descriptions",
  author   = "Karpathy, Andrej and Fei-Fei, Li",
  abstract = "We present a model that generates natural language descriptions
              of images and their regions. Our approach leverages datasets of
              images and their sentence descriptions to learn about the
              inter-modal correspondences between language and visual data. Our
              alignment model is based on a novel combination of Convolutional
              Neural Networks over image regions, bidirectional Recurrent
              Neural Networks (RNN) over sentences, and a structured objective
              that aligns the two modalities through a multimodal embedding. We
              then describe a Multimodal Recurrent Neural Network architecture
              that uses the inferred alignments to learn to generate novel
              descriptions of image regions. We demonstrate that our alignment
              model produces state of the art results in retrieval experiments
              on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the
              generated descriptions outperform retrieval baselines on both
              full images and on a new dataset of region-level annotations.
              Finally, we conduct large-scale analysis of our RNN language
              model on the Visual Genome dataset of 4.1 million captions and
              highlight the differences between image and region-level caption
              statistics.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  39,
  number   =  4,
  pages    = "664--676",
  month    =  apr,
  year     =  2017,
  keywords = "interpretability;trust\_informal\_treatment;assurance\_explicit;Assurances",
  language = "en"
}

@ARTICLE{Huysmans2011-th,
  title    = "An empirical evaluation of the comprehensibility of decision
              table, tree and rule based predictive models",
  author   = "Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and
              Vanthienen, Jan and Baesens, Bart",
  abstract = "An important objective of data mining is the development of
              predictive models. Based on a number of observations, a model is
              constructed that allows the analysts to provide classifications
              or predictions for new observations. Currently, most research
              focuses on improving the accuracy or precision of these models
              and comparatively little research has been undertaken to increase
              their comprehensibility to the analyst or end-user. This is
              mainly due to the subjective nature of `comprehensibility', which
              depends on many factors outside the model, such as the user's
              experience and his/her prior knowledge. Despite this influence of
              the observer, some representation formats are generally
              considered to be more easily interpretable than others. In this
              paper, an empirical study is presented which investigates the
              suitability of a number of alternative representation formats for
              classification when interpretability is a key requirement. The
              formats under consideration are decision tables, (binary)
              decision trees, propositional rules, and oblique rules. An
              end-user experiment was designed to test the accuracy, response
              time, and answer confidence for a set of problem-solving tasks
              involving the former representations. Analysis of the results
              reveals that decision tables perform significantly better on all
              three criteria, while post-test voting also reveals a clear
              preference of users for decision tables in terms of ease of use.",
  journal  = "Decis. Support Syst.",
  volume   =  51,
  number   =  1,
  pages    = "141--154",
  month    =  apr,
  year     =  2011,
  keywords = "Data mining; Classification; Knowledge representation;
              Comprehensibility; Decision
              tables;interpretability;trust\_informal\_treatment;assurance\_explicit;human\_study;interp\_models;Integral
              Assurance;Interpretable Models;Assurances"
}

@ARTICLE{Ridgeway1998-lv,
  title     = "Interpretable Boosted Na{\"\i}ve Bayes Classification",
  author    = "Ridgeway, G and Madigan, D and Richardson, T and O'Kane, J",
  abstract  = "Abstract Voting methods such as boosting and bagging provide
               substantial improvements in classification performance in many
               problem domains. However, the resulting predictions can prove
               inscrutable to end-users. This is especially problematic in
               domains such as medicine, where end-user acceptance often
               depends on the ability of a classifier to explain its reasoning.
               Here we propose a variant of the boosted na{\"\i}ve Bayes
               classifier that facilitates ...",
  journal   = "KDD",
  publisher = "aaai.org",
  year      =  1998,
  keywords  = "interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Assurances"
}

@PHDTHESIS{Kim2015-rj,
  title     = "Interactive and interpretable machine learning models for human
               machine collaboration",
  author    = "Kim, Been",
  abstract  = "I envision a system that enables successful collaborations
               between humans and machine learning models by harnessing the
               relative strength to accomplish what neither can do alone.
               Machine learning techniques and humans have skills that
               complement each other - machine learning techniques are good at
               computation on data at the lowest level of granularity, whereas
               people are better at abstracting knowledge from their
               experience, and transferring the knowledge across domains. The
               goal of this thesis is to develop a framework for
               human-in-the-loop machine learning that enables people to
               interact effectively with machine learning models to make better
               decisions, without requiring in-depth knowledge about machine
               learning techniques. Many of us interact with machine learning
               systems everyday. Systems that mine data for product
               recommendations, for example, are ubiquitous. However these
               systems compute their output without end-user involvement, and
               there are typically no life or death consequences in the case
               the machine learning result is not acceptable to the user. In
               contrast, domains where decisions can have serious consequences
               (e.g., emergency response panning, medical decision-making),
               require the incorporation of human experts' domain knowledge.
               These systems also must be transparent to earn experts' trust
               and be adopted in their workflow. The challenge addressed in
               this thesis is that traditional machine learning systems are not
               designed to extract domain experts' knowledge from natural
               workflow, or to provide pathways for the human domain expert to
               directly interact with the algorithm to interject their
               knowledge or to better understand the system output. For machine
               learning systems to make a real-world impact in these important
               domains, these systems must be able to communicate with highly
               skilled human experts to leverage their judgment and expertise,
               and share useful information or patterns from the data. In this
               thesis, I bridge this gap by building human-in-the-loop machine
               learning models and systems that compute and communicate machine
               learning results in ways that are compatible with the human
               decision-making process, and that can readily incorporate human
               experts' domain knowledge. I start by building a machine
               learning model that infers human teams' planning decisions from
               the structured form of natural language of team meetings. I show
               that the model can infer a human teams' final plan with 86\%
               accuracy on average. I then design an interpretable machine
               learning model then ``makes sense to humans'' by exploring and
               communicating patterns and structure in data to support human
               decision-making. Through human subject experiments, I show that
               this interpretable machine learning model offers statistically
               significant quantitative improvements in interpretability while
               preserving clustering performance. Finally, I design a machine
               learning model that supports transparent interaction with humans
               without requiring that a user has expert knowledge of machine
               learning technique. I build a human-in-the-loop machine learning
               system that incorporates human feedback and communicates its
               internal states to humans, using an intuitive medium for
               interaction with the machine learning model. I demonstrate the
               application of this model for an educational domain in which
               teachers cluster programming assignments to streamline the
               grading process.",
  publisher = "Massachusetts Institute of Technology",
  year      =  2015,
  school    = "Massachusetts Institute of Technology",
  keywords  = "Aeronautics and Astronautics.;
               Thesis;interpretability;assurance\_explicit;trust\_formal\_treatment;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Caruana2015-za,
  title     = "Intelligible Models for {HealthCare}: Predicting Pneumonia Risk
               and Hospital 30-day Readmission",
  booktitle = "Proceedings of the 21th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery and Data Mining",
  author    = "Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul
               and Sturm, Marc and Elhadad, Noemie",
  publisher = "ACM",
  pages     = "1721--1730",
  series    = "KDD '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "additive models, classification, healthcare, intelligibility,
               interaction detection, logistic regression, risk
               prediction;interpretability;medical;in\_table;trust\_informal\_treatment;assurance\_explicit;interp\_models;Integral
               Assurance;Interpretable Models;Assurances"
}

@BOOK{Fukuyama1995-un,
  title     = "Trust: The social virtues and the creation of prosperity",
  author    = "Fukuyama, Francis",
  abstract  = "Why are some nations richer than others? Rand Corp. political
               scientist Francis Fukuyama argues in his latest book that some
               societies are able to develop cultural norms, such as hard work
               and mutual trust, more than others. Such positive cultural
               values foster economic",
  publisher = "Free Press Paperbacks",
  year      =  1995,
  keywords  = "trust\_definition;NotRead;Assurances/Trust Background"
}

@ARTICLE{Fung1999-bi,
  title    = "{EC-trust} (trust in electronic commerce): exploring the
              antecedent factors",
  author   = "Fung, Raymond and Lee, Matthew",
  journal  = "AMCIS 1999 Proceedings",
  pages    = "179",
  year     =  1999,
  keywords = "trust\_definition;meh..;Assurances/Trust Background"
}

@ARTICLE{Baier1986-im,
  title     = "Trust and Antitrust",
  author    = "Baier, Annette",
  journal   = "Ethics",
  publisher = "University of Chicago Press",
  volume    =  96,
  number    =  2,
  pages     = "231--260",
  year      =  1986,
  keywords  = "trust\_definition;in\_table;visionary\_paper;Assurances/Trust
               Background"
}

@BOOK{Gambetta1988-pi,
  title     = "Trust: Making and Breaking Cooperative Relations",
  author    = "Gambetta, Diego",
  publisher = "Blackwell",
  year      =  1988,
  keywords  = "in\_table;Assurances/Trust Background"
}

@ARTICLE{Lewicki2006-hj,
  title     = "Models of Interpersonal Trust Development: Theoretical
               Approaches, Empirical Evidence, and Future Directions",
  author    = "Lewicki, Roy J and Tomlinson, Edward C and Gillespie, Nicole",
  abstract  = "Most research on trust has taken a static, ?snapshot? view; that
               is, it has approached trust as an independent, mediating, or
               dependent variable captured by measuring trust at a single point
               in time. Limited attention has been given to conceptualizing and
               measuring trust development over time within interpersonal
               relationships. The authors organize the existing work on trust
               development into four broad areas: the behavioral approach and
               three specific conceptualizations of the psychological approach
               (unidimensional, two-dimensional, and transformational models).
               They compare and contrast across these approaches and use this
               analysis to identify unanswered questions and formulate
               directions for future research.",
  journal   = "J. Manage.",
  publisher = "SAGE Publications Inc",
  volume    =  32,
  number    =  6,
  pages     = "991--1022",
  month     =  dec,
  year      =  2006,
  keywords  = "in\_table;trust\_definition;business;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{Lewicki2006-gp,
  title     = "Trust, trust development, and trust repair",
  author    = "Lewicki, Roy J and Wiethoff, Carolyn",
  journal   = "The handbook of conflict resolution: Theory and practice",
  publisher = "Jossey-Bas Publishers San Francisco",
  pages     = "92--119",
  year      =  2006,
  keywords  = "trust\_definition;in\_table;business;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{Lewicki1998-ox,
  title     = "Trust and Distrust: New Relationships and Realities",
  author    = "Lewicki, Roy J and McAllister, Daniel J and Bies, Robert J",
  abstract  = "We propose a new theoretical framework for understanding
               simultaneous trust and distrust within relationships, grounded
               in assumptions of multidimensionality and the inherent tensions
               of relationships, and we separate this research from prior work
               grounded in assumptions of unidimensionality and balance.
               Drawing foundational support for this new framework from recent
               research on simultaneous positive and negative sentiments and
               ambivalence, we explore the theoretical and practical
               significance of the framework for future work on trust and
               distrust relationships within organizations.",
  journal   = "Acad. Manage. Rev.",
  publisher = "Academy of Management",
  volume    =  23,
  number    =  3,
  pages     = "438--458",
  year      =  1998,
  keywords  = "trust\_definition;business;Assurances/Trust
               Background;Assurances"
}

@BOOK{Rasmussen2006-ne,
  title     = "Gaussian processes for machine learning",
  author    = "Rasmussen, Carl Edward",
  abstract  = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
               Teregowda): Abstract. We give a basic introduction to Gaussian
               Process regression models. We focus on understanding the role of
               the stochastic process and how it is used to define a
               distribution over functions. We present the simple equations for
               incorporating training data and examine how to learn the
               hyperparameters using the marginal likelihood. We explain the
               practical advantages of Gaussian Process and end with
               conclusions and a look at the current trends in GP work.
               Supervised learning in the form of regression (for continuous
               outputs) and classification (for discrete outputs) is an
               important constituent of statistics and machine learning, either
               for analysis of data sets, or as a subgoal of a more complex
               problem. Traditionally parametric 1 models have been used for
               this purpose. These have a possible advantage in ease of
               interpretability, but for complex data sets, simple parametric
               models may lack expressive power, and their more complex
               counterparts (such as feed forward neural networks) may not be
               easy to work with in practice. The advent of kernel machines,
               such as Support Vector Machines and Gaussian Processes has
               opened the possibility of flexible models which are practical to
               work with. In this short tutorial we present the basic idea on
               how Gaussian Process models can be used to formulate a Bayesian
               framework for regression. We will focus on understanding the
               stochastic process and how it is used in supervised learning.
               Secondly, we will discuss practical matters regarding the role
               of hyperparameters in the covariance function, the marginal
               likelihood and the automatic Occam's razor. For broader
               introductions to Gaussian processes, consult [1], [2]. 1
               Gaussian Processes In this section we define Gaussian Processes
               and show how they can very naturally be used to define
               distributions over functions. In the following section we
               continue to show how this distribution is updated in the light
               of training examples. 1 By a parametric model, we here mean a
               model which during training ``absorbs '' the information from
               the training data into the parameters; after training the data
               can be discarded.",
  publisher = "Citeseer",
  year      =  2006,
  keywords  = "GPs;interpretability;Textbook;TextBooks"
}

@ARTICLE{Ishibuchi2007-yn,
  title    = "Analysis of interpretability-accuracy tradeoff of fuzzy systems
              by multiobjective fuzzy genetics-based machine learning",
  author   = "Ishibuchi, Hisao and Nojima, Yusuke",
  abstract = "This paper examines the interpretability-accuracy tradeoff in
              fuzzy rule-based classifiers using a multiobjective fuzzy
              genetics-based machine learning (GBML) algorithm. Our GBML
              algorithm is a hybrid version of Michigan and Pittsburgh
              approaches, which is implemented in the framework of evolutionary
              multiobjective optimization (EMO). Each fuzzy rule is represented
              by its antecedent fuzzy sets as an integer string of fixed
              length. Each fuzzy rule-based classifier, which is a set of fuzzy
              rules, is represented as a concatenated integer string of
              variable length. Our GBML algorithm simultaneously maximizes the
              accuracy of rule sets and minimizes their complexity. The
              accuracy is measured by the number of correctly classified
              training patterns while the complexity is measured by the number
              of fuzzy rules and/or the total number of antecedent conditions
              of fuzzy rules. We examine the interpretability-accuracy tradeoff
              for training patterns through computational experiments on some
              benchmark data sets. A clear tradeoff structure is visualized for
              each data set. We also examine the interpretability-accuracy
              tradeoff for test patterns. Due to the overfitting to training
              patterns, a clear tradeoff structure is not always obtained in
              computational experiments for test patterns.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  44,
  number   =  1,
  pages    = "4--31",
  month    =  jan,
  year     =  2007,
  keywords = "Classification; Fuzzy systems; Fuzzy data mining; Multiobjective
              optimization; Genetic algorithms; Genetics-based machine
              learning;interpretability;in\_table;Assurances"
}

@ARTICLE{Muller2008-sp,
  title    = "Machine learning for real-time single-trial {EEG-analysis}: From
              brain--computer interfacing to mental state monitoring",
  author   = "M{\"u}ller, Klaus-Robert and Tangermann, Michael and Dornhege,
              Guido and Krauledat, Matthias and Curio, Gabriel and Blankertz,
              Benjamin",
  abstract = "Machine learning methods are an excellent choice for compensating
              the high variability in EEG when analyzing single-trial data in
              real-time. This paper briefly reviews preprocessing and
              classification techniques for efficient EEG-based brain--computer
              interfacing (BCI) and mental state monitoring applications. More
              specifically, this paper gives an outline of the Berlin
              brain--computer interface (BBCI), which can be operated with
              minimal subject training. Also, spelling with the novel
              BBCI-based Hex-o-Spell text entry system, which gains
              communication speeds of 6--8 letters per minute, is discussed.
              Finally the results of a real-time arousal monitoring experiment
              are presented.",
  journal  = "J. Neurosci. Methods",
  volume   =  167,
  number   =  1,
  pages    = "82--90",
  month    =  jan,
  year     =  2008,
  keywords = "EEG; Sensorimotor rhythms; $\alpha$ -Rhythm; Single-trial
              EEG-analysis; Real-time; Machine learning; Mental state
              monitoring;interpretability;in\_table;Assurances"
}

@ARTICLE{Haury2011-zi,
  title    = "The influence of feature selection methods on accuracy, stability
              and interpretability of molecular signatures",
  author   = "Haury, Anne-Claire and Gestraud, Pierre and Vert, Jean-Philippe",
  abstract = "Biomarker discovery from high-dimensional data is a crucial
              problem with enormous applications in biology and medicine. It is
              also extremely challenging from a statistical viewpoint, but
              surprisingly few studies have investigated the relative strengths
              and weaknesses of the plethora of existing feature selection
              methods. In this study we compare 32 feature selection methods on
              4 public gene expression datasets for breast cancer prognosis, in
              terms of predictive performance, stability and functional
              interpretability of the signatures they produce. We observe that
              the feature selection method has a significant influence on the
              accuracy, stability and interpretability of signatures.
              Surprisingly, complex wrapper and embedded methods generally do
              not outperform simple univariate feature selection methods, and
              ensemble feature selection has generally no positive effect.
              Overall a simple Student's t-test seems to provide the best
              results.",
  journal  = "PLoS One",
  volume   =  6,
  number   =  12,
  pages    = "e28210",
  month    =  dec,
  year     =  2011,
  keywords = "
              interpretability;trust\_informal\_treatment;assurance\_implicit;rep\_learn;Value
              Alignment;Integral Assurance;Assurances",
  language = "en"
}

@ARTICLE{Garcia2009-gd,
  title     = "A study of statistical techniques and performance measures for
               genetics-based machine learning: accuracy and interpretability",
  author    = "Garc{\'\i}a, S and Fern{\'a}ndez, A and Luengo, J and Herrera, F",
  abstract  = "The experimental analysis on the performance of a proposed
               method is a crucial and necessary task to carry out in a
               research. This paper is focused on the statistical analysis of
               the results in the field of genetics-based machine Learning. It
               presents a study involving a set of techniques which can be used
               for doing a rigorous comparison among algorithms, in terms of
               obtaining successful classification models. Two accuracy
               measures for multi-class problems have been employed:
               classification rate and Cohen's kappa. Furthermore, two
               interpretability measures have been employed: size of the rule
               set and number of antecedents. We have studied whether the
               samples of results obtained by genetics-based classifiers, using
               the performance measures cited above, check the necessary
               conditions for being analysed by means of parametrical tests.
               The results obtained state that the fulfillment of these
               conditions are problem-dependent and indefinite, which supports
               the use of non-parametric statistics in the experimental
               analysis. In addition, non-parametric tests can be
               satisfactorily employed for comparing generic classifiers over
               various data-sets considering any performance measure. According
               to these facts, we propose the use of the most powerful
               non-parametric statistical tests to carry out multiple
               comparisons. However, the statistical analysis conducted on
               interpretability must be carefully considered.",
  journal   = "Soft Comput",
  publisher = "Springer-Verlag",
  volume    =  13,
  number    =  10,
  pages     = "959",
  month     =  aug,
  year      =  2009,
  keywords  = "interpretability;medical;classification;assurance\_predictability;ai\_learning;ai\_reasoning;decision\_tree",
  language  = "en"
}

@INPROCEEDINGS{Yu2017-se,
  title           = "Towards Deep Interpretability ({MUS-ROVER} {II)}: Learning
                     Hierarchical Representations of Tonal Music",
  booktitle       = "Proceedings of the 5th International Conference on
                     Learning Representations ({ICLR})",
  author          = "Yu, Haizi and Varshney, Lav R",
  abstract        = "Music theory studies the regularity of patterns in music
                     to capture concepts underlying music styles and composers'
                     decisions. This paper continues the study of building
                     \textbackslashemph\{automatic theorists\} (rovers) to
                     learn and represent music concepts that lead to human
                     interpretable knowledge and further lead to materials for
                     educating people. Our previous work took a first step in
                     algorithmic concept learning of tonal music, studying
                     high-level representations (concepts) of symbolic music
                     (scores) and extracting interpretable rules for
                     composition. This paper further studies the representation
                     \textbackslashemph\{hierarchy\} through the learning
                     process, and supports \textbackslashemph\{adaptive\} 2D
                     memory selection in the resulting language model. This
                     leads to a deeper-level interpretability that expands from
                     individual rules to a dynamic system of rules, making the
                     entire rule learning process more cognitive. The outcome
                     is a new rover, MUS-ROVER \textbackslashRN\{2\}, trained
                     on Bach's chorales, which outputs customizable syllabi for
                     learning compositional rules. We demonstrate comparable
                     results to our music pedagogy, while also presenting the
                     differences and variations. In addition, we point out the
                     rover's potential usages in style recognition and
                     synthesis, as well as applications beyond music.",
  year            =  2017,
  keywords        = "interpretability",
  conference      = "International Conference on Learning Representations"
}

@ARTICLE{Karaletsos2015-zx,
  title         = "Bayesian representation learning with oracle constraints",
  author        = "Karaletsos, Theofanis and Belongie, Serge and R{\"a}tsch,
                   Gunnar",
  abstract      = "Representation learning systems typically rely on massive
                   amounts of labeled data in order to be trained to high
                   accuracy. Recently, high-dimensional parametric models like
                   neural networks have succeeded in building rich
                   representations using either compressive, reconstructive or
                   supervised criteria. However, the semantic structure
                   inherent in observations is oftentimes lost in the process.
                   Human perception excels at understanding semantics but
                   cannot always be expressed in terms of labels. Thus,
                   \textbackslashemph\{oracles\} or
                   \textbackslashemph\{human-in-the-loop systems\}, for example
                   crowdsourcing, are often employed to generate similarity
                   constraints using an implicit similarity function encoded in
                   human perception. In this work we propose to combine
                   \textbackslashemph\{generative unsupervised feature
                   learning\} with a \textbackslashemph\{probabilistic
                   treatment of oracle information like triplets\} in order to
                   transfer implicit privileged oracle knowledge into explicit
                   nonlinear Bayesian latent factor models of the observations.
                   We use a fast variational algorithm to learn the joint model
                   and demonstrate applicability to a well-known image dataset.
                   We show how implicit triplet information can provide rich
                   information to learn representations that outperform
                   previous metric learning approaches as well as generative
                   models without this side-information in a variety of
                   predictive tasks. In addition, we illustrate that the
                   proposed approach compartmentalizes the latent spaces
                   semantically which allows interpretation of the latent
                   variables.",
  month         =  jun,
  year          =  2015,
  keywords      = "interpretability",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1506.05011"
}

@TECHREPORT{Sheridan1978-be,
  title       = "Human and computer control of undersea teleoperators",
  author      = "Sheridan, Thomas B and Verplank, William L",
  institution = "DTIC Document",
  year        =  1978,
  keywords    = "automation"
}

@INPROCEEDINGS{Freedman2016-gr,
  title     = "Safety in {AI-HRI}: Challenges Complementing User Experience
               Quality",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Freedman, Richard G and Zilberstein, Shlomo",
  abstract  = "Contemporary research in human-robot interaction (HRI)
               predominantly focuses on the user's experience while controlling
               a robot. However, with the increased deployment of artificial
               intelligence (AI) techniques, robots are quickly becoming more
               autonomous in both academic and industrial experimental
               settings. In addition to improving the user's interactive
               experience with AI-operated robots through personalization,
               dialogue, emotions, and dynamic behavior, there is also a
               growing need to consider the safety of the interaction. AI may
               not account for the user's less likely responses, making it
               possible for an unaware user to be injured by the robot if they
               have a collision. Issues of trust and acceptance may also come
               into play if users cannot always understand the robot's thought
               process, creating a potential for emotional harm. We identify
               challenges that will need to be addressed in safe AI-HRI and
               provide an overview of approaches to consider for them, many
               stemming from the contemporary research.",
  month     =  sep,
  year      =  2016,
  keywords  = "Safety\_AI;NotRead;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Maeda2016-tt,
  title     = "Anticipative Interaction Primitives for {Human-Robot}
               Collaboration",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Maeda, Guilherme and Maloo, Aayush and Ewerton, Marco and
               Lioutikov, Rudolf and Peters, Jan",
  abstract  = "This paper introduces our initial investigation on the problem
               of providing a semi-autonomous robot collaborator with
               anticipative capabilities to predict human actions. Anticipative
               robot behavior is a desired characteristic of robot
               collaborators that lead to fluid, proactive interactions. We are
               particularly interested in improving reactive methods that rely
               on human action recognition to activate the corresponding robot
               action. Action recognition invariably causes delay in the
               robot's response, and the goal of our method is to eliminate
               this delay by predicting the next human action. Prediction is
               achieved by using a lookup table containing variations of
               assembly sequences, previously demonstrated by different users.
               The method uses the nearest neighbor sequence in the table that
               matches the actual sequence of human actions. At the movement
               level, our method uses a probabilistic representation of
               interaction primitives to generate robot trajectories. The
               method is demonstrated using a 7 degree-of-freedom lightweight
               arm equipped with a 5-finger hand on an assembly task consisting
               of 17 steps.",
  month     =  sep,
  year      =  2016,
  keywords  = "HRI;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Curran2016-ij,
  title     = "{POMDPs} for {Risk-Aware} Autonomy",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Curran, William and Bowie, Cameron and Smart, William D",
  abstract  = "Although we would like our robots to have completely autonomous
               behavior, this is often not possible. Some parts of a task might
               be hard to automate, perhaps due to hard-to-interpret sensor
               information, or a complex environment. In this case, using
               shared autonomy or teleoperation is preferable to an error-prone
               autonomous approach. However, the question of which parts of a
               task to allocate to the human, and which to the robot can often
               be tricky. In this work, we introduce A 3 P, a risk-aware
               task-level reinforcement learning algorithm. A 3 P represents a
               task-level state machine as a POMDP. In this paper, we introduce
               A 3 P, a risk-aware algorithm that discovers when to hand off
               subtasks to a human assistant. A 3 P models the task as a
               Partially Observably Markov Decision Process (POMDP) and
               explicitly represents failures as additional state-action pairs.
               Based on the model, the algorithm allows the user to allocate
               subtasks the robot or the human in such a way as to manage the
               worst-case performance time for the overall task.",
  month     =  sep,
  year      =  2016,
  keywords  = "
               HRI;assurance\_competence;ai\_planning;robotics;POMDP;trust\_informal\_treatment;assurance\_implicit;risk\_safety\_nonstationary;in\_paper;Integral
               Assurance;Value Alignment;Assurances;Robotics/(PO)MDPs",
  language  = "en"
}

@INPROCEEDINGS{Chadalavada2016-ze,
  title           = "Empirical evaluation of human trust in an expressive
                     mobile robot",
  booktitle       = "Proceedings of {RSS} Workshop`` Social Trust in Autonomous
                     Robots 2016'', June 19, 2016",
  author          = "Chadalavada, Ravi Teja and Lilienthal, Achim and
                     Andreasson, Henrik and Krug, Robert",
  year            =  2016,
  keywords        = "HRI;Assurances",
  conference      = "RSS 2016"
}

@INPROCEEDINGS{Chadalavada2015-wx,
  title     = "That's on my mind! robot to human intention communication
               through on-board projection on shared floor space",
  booktitle = "2015 European Conference on Mobile Robots ({ECMR})",
  author    = "Chadalavada, R T and Andreasson, H and Krug, R and Lilienthal, A
               J",
  abstract  = "The upcoming new generation of autonomous vehicles for
               transporting materials in industrial environments will be more
               versatile, flexible and efficient than traditional AGVs, which
               simply follow pre-defined paths. However, freely navigating
               vehicles can appear unpredictable to human workers and thus
               cause stress and render joint use of the available space
               inefficient. Here we address this issue and propose on-board
               intention projection on the shared floor space for communication
               from robot to human. We present a research prototype of a
               robotic fork-lift equipped with a LED projector to visualize
               internal state information and intents. We describe the
               projector system and discuss calibration issues. The robot's
               ability to communicate its intentions is evaluated in realistic
               situations where test subjects meet the robotic forklift. The
               results show that already adding simple information, such as the
               trajectory and the space to be occupied by the robot in the near
               future, is able to effectively improve human response to the
               robot.",
  pages     = "1--6",
  year      =  2015,
  keywords  = "mobile robots;LED projector;autonomous vehicles;human intention
               communication;human workers;industrial environments;internal
               state information;onboard projection;robotic forklift;shared
               floor space;Calibration;Cameras;Service
               robots;Standards;Three-dimensional
               displays;Vehicles;HRI;assurance\_explicit;trust\_formal\_treatment;Supplemental
               Assurance;vis\_dr;Assurances"
}

@ARTICLE{Broad2017-jb,
  title    = "Trust Adaptation Leads to Lower Control Effort in Shared Control
              of Crane Automation",
  author   = "Broad, A and Schultz, J and Derry, M and Murphey, T and Argall, B",
  abstract = "We present a shared-control framework predicated on a measure of
              trust in the operator, that is calculated automatically based on
              the quality of the interactions between a human and autonomous
              system. This measure of trust is built upon a control-theoretic
              foundation that rewards stable operation of the system to give
              more trusted users additional control authority. The level of
              control authority is used to modify the human input, and as a
              result, we observe a minimization of the required effort of the
              controller. We validate this work within a planar crane
              environment with a receding horizon controller to assist with the
              regulation of the system dynamics. The human defines the
              reference trajectory for the controller. In an experimental study
              users navigate a suspended payload through a set of maze
              configurations. We find that adaptation of the trust metric over
              time provides the benefit of substantially ( $p <; 0.01$)
              improving the automated system's ability to modulate the user's
              input, resulting in stable reference trajectories that require
              less effort to track. In effect, the human and automation spend
              less time fighting each other during task execution, suggesting
              that the automated system and user each have a better
              understanding of the other's ability.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  2,
  number   =  1,
  pages    = "239--246",
  month    =  jan,
  year     =  2017,
  keywords = "cranes;path planning;trajectory control;control authority;crane
              automation control;maze configuration;planar crane
              environment;receding horizon controller;reference
              trajectory;suspended payload navigation;trust adaptation;trust
              measure;trust
              metric;Aerodynamics;Measurement;Optimization;Robots;Trajectory;Vehicle
              dynamics;Control architectures and programming;human factors and
              human-in-the-loop;physically assistive
              devices;HRI;automation;Assurances"
}

@INPROCEEDINGS{Smith2016-bv,
  title     = "Interdependence quantification for compositional control
               synthesis with an application in vehicle safety systems",
  booktitle = "2016 {IEEE} 55th Conference on Decision and Control ({CDC})",
  author    = "Smith, S W and Nilsson, P and Ozay, N",
  abstract  = "Composing controllers designed individually for interacting
               subsystems, while preserving the guarantees that each controller
               provides for each subsystem, is a challenging task. Motivated by
               this challenge, we consider in this paper the problem of
               synthesizing safety controllers for linear parameter-varying
               subsystems, where the system matrices of each subsystem depend
               (possibly nonlinearly) on the states of the other subsystems. In
               particular, we propose a method for synthesis of controlled
               invariant sets and associated controllers, that is robust
               against affine parametric uncertainties in the system matrices.
               Then we show for certain classes of parameter dependencies how
               to quantify the uncertainty imposed on the other subsystems by
               convexifying, with an affine map, the effects of these
               parameters. An analysis of this quantification is provided. In
               the second part of the paper, we focus on an application of this
               method to vehicle safety systems. We demonstrate how controllers
               for lane-keeping and adaptive cruise control can be synthesized
               in a compositional way using the proposed method. Our
               simulations illustrate how these controllers keep their
               individual safety guarantees when implemented simultaneously, as
               the theory suggests.",
  pages     = "5700--5707",
  month     =  dec,
  year      =  2016,
  keywords  = "adaptive control;control system synthesis;linear parameter
               varying systems;matrix algebra;road safety;road
               vehicles;uncertain systems;adaptive cruise control;affine
               map;affine parametric uncertainties;compositional control
               synthesis;controlled invariant sets;interacting
               subsystems;interdependence quantification;lane-keeping;linear
               parameter-varying subsystems;safety controllers;safety
               guarantees;system matrices;vehicle safety systems;Approximation
               algorithms;Robustness;Uncertainty;Vehicle dynamics;Vehicle
               safety;Vehicles;V\&V;Assurances"
}

@INPROCEEDINGS{Wang2016-vt,
  title     = "{Trust-Based} Symbolic Robot Motion Planning with
               {Human-in-the-Loop}",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Wang, Yue",
  abstract  = "Autonomous robots are becoming increasingly popular and such
               systems has led to complex design and analysis which brings the
               necessity of validation and verification. In particular,
               symbolic robot motion planning based on formal methods is
               verifiably correct. It is the process of specifying and planning
               robot tasks in a discrete space, then carrying them out in a
               continuous space in a manner that preserves the discrete-level
               task specifications. Despite progress in symbolic motion
               planning, many challenges remain, including addressing
               scalability for multi-robot systems and improving solutions by
               incorporating human intelligence in an adaptive fashion. On the
               other hand, extant works in human-robot interaction (HRI) often
               lack quantitative models and real-time analytical approaches.
               Here, we summarize our recent works on symbolic robot motion
               planning with human-in-the-loop as a step toward addressing
               these challenges. We specially focus on human trust in
               autonomous robots and embed trust analysis into the symbolic
               robot motion planning.",
  month     =  sep,
  year      =  2016,
  keywords  = "V\&V;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Da_Silva2016-qb,
  title     = "Extended Abstract: Formal Design of Cooperative {Multi-Agent}
               Systems",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "da Silva, Rafael Rodrigues and Wu, Bo and Dai, Jin and Lin, Hai",
  abstract  = "We propose a formal design framework to automatically synthesize
               coordination and control schemes for cooperative multi-agent
               systems by combining a top-down mission planning with a
               bottom-up motion planning. The multi-agent system is assigned a
               global mission, specified as regular languages over all the
               agents' capabilities, whereas basic motion controllers for each
               agent shall be designed with respect to given environment
               description. On one hand, a mission planning layer sits on the
               top of the proposed framework, decomposing the global mission
               into local tasks that are in consistency with each agent's
               individual capabilities, and compositionally verifying the joint
               effort of the agents via an assume guarantee paradigm. On the
               other hand, corresponding to these local missions, motion plans
               associated with each agent are synthesized by composing basic
               motion primitives, which are verified safe by differential
               dynamic logic (dL), through a Satisfiability Modulo Theories
               (SMT) solver that searches feasible solutions in face of
               constraints due to local task requirements and the environment
               description. It is shown that the proposed framework can handle
               changing environments as the motion primitives are reactive in
               nature, making the motion planning adaptive to local
               environmental changes. Furthermore, on-line mission
               reconfiguration can be triggered by the motion planning layer
               once no feasible solutions can be found through the SMT solver.
               The effectiveness of the overall design framework is
               demonstrated by an automated warehouse case study.",
  month     =  sep,
  year      =  2016,
  keywords  = "trust\_informal\_treatment;assurance\_implicit;V\&V;in\_paper;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Nishi2016-zq,
  title     = "Reduction of the State Observation Problem to an Identifiability
               Problem",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Nishi, Masataka",
  abstract  = "Data integrity is a property which a world state interpreted
               with a world model is consistent with the real operating
               environment. Even a formally verified safety claim of an
               autonomous system is prone to a malfunction caused by loss of
               data integrity. From a first-person viewpoint in a congested
               environment, some components of measurable part of the world
               state may become transiently deficient or unavailable because of
               the limited capability of sensor devices. If the system could
               get into a situation where the world state becomes suddenly
               unobservable, existing estimation methods may get unstable.
               These methods can hardly detect the loss of data integrity and
               produce an incorrect estimate without any notice. Our insight is
               that we can merge the original concept of observer theory with
               that of automated reasoning. Firstly, we propose a new way of
               unifying them into a problem of checking satisfiability of a
               formula that consists of predicates regarding the world model
               and decision variables regarding unmeasurable part of the world
               state. We can detect a loss of data integrity by checking if the
               problem is unsatisfiable. Secondly, we replace the idea of
               observability in control theory with identifiability with
               respect to a measure of tolerance and a world model. We show a
               procedure of estimating the world state with a bounded
               uncertainty specified by the measure of tolerance. Third, we
               show that a problem of sensor fusion, a problem of reasoning a
               world state of discrete and enumerated type, and a decision
               problem under uncertainty in the world state are formulated as
               an identifiability problem. The proposal presents a constructive
               basis for supporting the degree of confidence in the estimated
               world state.",
  month     =  sep,
  year      =  2016,
  keywords  = "V\&V;trust\_informal\_treatment;assurance\_implicit;in\_paper;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Lahijanian2016-nd,
  title     = "Social Trust: a Major Challenge for the Future of Autonomous
               Systems",
  booktitle = "{AAAI} Fall Symposium on {Cross-Disciplinary} Challenges for
               Autonomous Systems, {AAAI} Fall Symposium. {AAAI}",
  author    = "Lahijanian, Morteza and Kwiatkowska, Marta",
  abstract  = "The immense technological advancements in the past decade have
               enabled robots to enjoy high levels of autonomy, paving their
               way into our society. The recent catastrophic accidents
               involving autonomous systems (e.g., Tesla fatal car accident),
               however, show that sole engineering progress in the technology
               is not enough to guarantee a safe and productive partnership
               between a human and a robot. In this paper we argue that we also
               need to advance our understanding of the role of social trust
               within human-robot relationships, and formulate a theory for
               expressing and reasoning about trust in the context of decisions
               affecting collaboration or competition between humans and
               robots. Therefore, we call for cross-disciplinary collaborations
               to study the formalization of social trust in the context of
               human-robot relationship. We lay the groundwork for such a study
               in this paper.",
  year      =  2016,
  keywords  = "V\&V;trust\_definition;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Joshi2016-og,
  title     = "{ALDA}: Cognitive Assistant for Legal Document Analytics",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Joshi, Karuna P and Gupta, Aditi and Mittal, Sudip and Pearce,
               Claudia and Joshi, Anupam and Finin, Tim",
  abstract  = "In recent times, there has been an exponential growth in
               digitization of legal documents such as case records,
               contracts,terms of services, regulations, privacy documents and
               compliance guidelines. Courts have been digitizing their
               archivedcases and also making it available for e-discovery. On
               theother hand, businesses are now maintaining large data setsof
               legal contracts that they have signed with their
               employees,customers and contractors. Large public sector
               organizationsare often bound by complex legal legislation and
               statutes.Hence, there is a need of a cognitive assistant to
               analyze andreason over these legal rules and help people make
               decisions.Today the process of monitoring an ever increasing
               datasetof legal contracts and ensuring regulations and
               complianceis still very manual and labour intensive. This can
               prove tobe a bottleneck in the smooth functioning of an
               enterprise.Automating these digital workflows is quite hard
               because theinformation is available as text documents but it is
               not represented in a machine understandable way. With the
               advancements in cognitive assistance technologies, it is now
               possibleto analyze these digitized legal documents efficiently.
               In thispaper, we discuss ALDA, a legal cognitive assistant to
               analyze digital legal documents. We also present some of
               thepreliminary results we have obtained by analyzing legal
               documents using techniques such as semantic web, text miningand
               graph analysis.",
  month     =  sep,
  year      =  2016,
  keywords  = "CA;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Gutfreund2016-xe,
  title     = "Automatic Arguments Construction --- From Search Engine to
               Research Engine",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Gutfreund, Dan and Katz, Yoav and Slonim, Noam",
  abstract  = "While discussing a concrete controversial topic, most humans
               will find it challenging to swiftly raise a diverse set of
               convincing and relevant arguments. In this paper we present a
               system that, given a point of view about a controversial topic,
               automatically generates arguments supporting and contesting it.
               This is achieved by breaking the task of automatic argument
               construction into a pipeline of successive modules, each is
               responsible for a specific tangible task such as documents
               retrieval, identifying building blocks of arguments within a
               document, and analyzing whether these building blocks support or
               contest the point of view. By providing an interface for humans
               to interact and intervene at different points in the pipeline,
               we present an interactive research tool which, for a given topic
               and a corpus of documents such as Wikipedia or newspaper
               archive, provides a more comprehensive view and deeper insights
               than can be obtained using standard search engines.",
  month     =  sep,
  year      =  2016,
  keywords  = "CA;trust\_informal\_treatment;assurance\_implicit;assurance\_predictability;assurance\_competence;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Estes2016-zp,
  title     = "Digital Copilot: Cognitive Assistance for Pilots",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Estes, Steven and Burns, Kevin and Helleberg, John and Long,
               Kevin and Stein, Jeffrey and Pollack, Matthew",
  abstract  = "Over the past several years, pilot-oriented mobile applications
               have seen widespread adoption among recreational pilots. Pilots
               have reported they provide significant workload savings by
               eliminating the need to manage paper charts, manuals, and
               checklists in the cockpit. The pilot, nonetheless, still must go
               looking for the information when it is required, increasing
               accident risk by diverting attention away from control of the
               aircraft. In this paper, we provide an overview of a cognitive
               assistant that determines when information is required based on
               flight context and automatically provides it to the pilot at the
               appropriate time. In addition to an overview of the concept, a
               recent evaluation is discussed alongside future plans to
               evaluate the safety of the Digital Copilot.",
  month     =  sep,
  year      =  2016,
  language  = "en"
}

@INPROCEEDINGS{Wu2016-ei,
  title     = "Trust and Cooperation in {Human-Robot} Decision Making",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Wu, Jane and Paeng, Erin and Linder, Kari and Valdesolo,
               Piercarlo and Boerkoel, James C",
  abstract  = "Trust plays a key role in social interactions, particularly when
               the decisions we make depend on the people we face. In this
               paper, we use game theory to explore whether a person's
               decisions are influenced by the type of agent they interact
               with:human or robot. By adopting a coin entrustment game, we
               quantitatively measure trust and cooperation to see if such
               phenomena emerge differently when a person believes they are
               playing a robot rather than another human. We found that while
               people cooperate with other humans and robots at a similar rate,
               they grow to trust robots more completely than humans. As a
               possible explanation for these differences, our survey results
               suggest that participants perceive humans as having faculty for
               feelings and sympathy, whereas they perceive robots as being
               more precise and reliable.",
  month     =  sep,
  year      =  2016,
  keywords  = "
               HRI;human\_study;trust\_formal\_treatment;assurance\_predictability;in\_paper;assurance\_implicit;Integral
               Assurance;Human-like Behavior;Assurances",
  language  = "en"
}

@ARTICLE{Faghmous2014-og,
  title    = "A Big Data Guide to Understanding Climate Change: The Case for
              {Theory-Guided} Data Science",
  author   = "Faghmous, James H and Kumar, Vipin",
  abstract = "Global climate change and its impact on human life has become one
              of our era's greatest challenges. Despite the urgency, data
              science has had little impact on furthering our understanding of
              our planet in spite of the abundance of climate data. This is a
              stark contrast from other fields such as advertising or
              electronic commerce where big data has been a great success
              story. This discrepancy stems from the complex nature of climate
              data as well as the scientific questions climate science brings
              forth. This article introduces a data science audience to the
              challenges and opportunities to mine large climate datasets, with
              an emphasis on the nuanced difference between mining climate data
              and traditional big data approaches. We focus on data, methods,
              and application challenges that must be addressed in order for
              big data to fulfill their promise with regard to climate science
              applications. More importantly, we highlight research showing
              that solely relying on traditional big data techniques results in
              dubious findings, and we instead propose a theory-guided data
              science paradigm that uses scientific theory to constrain both
              the big data techniques as well as the results-interpretation
              process to extract accurate insight from large climate data.",
  journal  = "Big Data",
  volume   =  2,
  number   =  3,
  pages    = "155--163",
  month    =  sep,
  year     =  2014,
  keywords = "
              TGDS;interpretability;assurance\_predictability;ai\_learning;supervised\_learning;Computer
              Science -
              Learning;trust\_informal\_treatment;assurance\_implicit;Integral
              Assurance;interp\_models;Interpretable Models;Assurances",
  language = "en"
}

@ARTICLE{Parasuraman2000-js,
  title    = "A model for types and levels of human interaction with automation",
  author   = "Parasuraman, R and Sheridan, T B and Wickens, C D",
  abstract = "Technical developments in computer hardware and software now make
              it possible to introduce automation into virtually all aspects of
              human-machine systems. Given these technical capabilities, which
              system functions should be automated and to what extent? We
              outline a model for types and levels of automation that provides
              a framework and an objective basis for making such choices.
              Appropriate selection is important because automation does not
              merely supplant but changes human activity and can impose new
              coordination demands on the human operator. We propose that
              automation can be applied to four broad classes of functions: 1)
              information acquisition; 2) information analysis; 3) decision and
              action selection; and 4) action implementation. Within each of
              these types, automation can be applied across a continuum of
              levels from low to high, i.e., from fully manual to fully
              automatic. A particular system can involve automation of all four
              types at different levels. The human performance consequences of
              particular types and levels of automation constitute primary
              evaluative criteria for automation design using our model.
              Secondary evaluative criteria include automation reliability and
              the costs of decision/action consequences, among others. Examples
              of recommended types and levels of automation are provided to
              illustrate the application of the model to automation design.",
  journal  = "IEEE Trans. Syst. Man Cybern. A Syst. Hum.",
  volume   =  30,
  number   =  3,
  pages    = "286--297",
  month    =  may,
  year     =  2000,
  keywords = "NASA Discipline Space Human Factors; Non-NASA
              Center;automation;interesting\_ideas;Assurances",
  language = "en"
}

@MISC{Ericsson1991-md,
  title        = "Introspection and verbal reports on cognitive processes---Two
                  approaches to the study of thinking: A response to Howe -
                  {ScienceDirect}",
  author       = "Ericsson, K Anders and Crutcher, Robert J",
  abstract     = "... A detailed model of the cognitive processes mediating
                  introspection is needed, in which the introspective processes
                  extracting additional information about thinking are
                  specified and are shown to be consistent with our knowledge
                  about human cognition . ...",
  publisher    = "Elsevier",
  year         =  1991,
  howpublished = "\url{http://www.sciencedirect.com/science/article/pii/0732118X9190041J}",
  note         = "Accessed: 2017-3-2",
  keywords     = "Assurances"
}

@ARTICLE{Hayes-Roth1979-wj,
  title     = "A cognitive model of planning",
  author    = "Hayes-Roth, Barbara and Hayes-Roth, Frederick",
  journal   = "Cogn. Sci.",
  publisher = "Wiley Online Library",
  volume    =  3,
  number    =  4,
  pages     = "275--310",
  year      =  1979,
  keywords  = "Assurances"
}

@ARTICLE{McKnight2001-fa,
  title    = "What Trust Means in {E-Commerce} Customer Relationships: An
              Interdisciplinary Conceptual Typology",
  author   = "McKnight, D H and Chervany, N L",
  abstract = "Trust is a vital relationship concept that needs clarification
              because researchers across disciplines have defined it in so many
              different ways. A typology of trust types would make it easier to
              compare and communicate results, and would be especially valuable
              if the types of trust related to one other. The typology should
              be interdisciplinary because many disciplines research
              e-commerce. This paper justifies a parsimonious interdisciplinary
              typology and relates trust constructs to e-commerce consumer
              actions, defining both conceptual-level and operational-level
              trust constructs. Conceptual-level constructs consist of
              disposition to trust (primarily from psychology),
              institution-based trust (from sociology), and trusting beliefs
              and trusting intentions (primarily from social psychology). Each
              construct is decomposed into measurable subconstructs, and the
              typology shows how trust constructs relate to already existing
              Internet relationship constructs. The effects of Web vendor
              interventions on consumer behaviors are posited to be partially
              mediated by consumer trusting beliefs and trusting intentions in
              the e-vendor.",
  journal  = "International Journal of Electronic Commerce",
  volume   =  6,
  number   =  2,
  pages    = "35--59",
  year     =  2001,
  keywords = "e-commerce;trust\_model;in\_presentation;Assurances/Trust
              Background"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Groom2007-bz,
  title     = "Can robots be teammates?: Benchmarks in human--robot teams",
  author    = "Groom, Victoria and Nass, Clifford",
  abstract  = "The team has become a popular model to organize joint
               human--robot behavior. Robot teammates are designed with
               high-levels of autonomy and well-developed coordination skills
               to aid humans in unpredictable environments. In this paper, we
               challenge the assumption that robots will succeed as teammates
               alongside humans. Drawing from the literature on human teams, we
               evaluate robots' potential to meet the requirements of
               successful teammates. We argue that lacking humanlike mental
               models and a sense of self, robots may prove untrustworthy and
               will be rejected from human teams. Benchmarks for evaluating
               human--robot teams are included, as are guidelines for defining
               alternative structures for human--robot groups.",
  journal   = "Interact. Stud.",
  publisher = "John Benjamins",
  volume    =  8,
  number    =  3,
  pages     = "483--500",
  month     =  jan,
  year      =  2007,
  keywords  = "humanrobot interaction; robot teams; teamwork; psychological
               benchmarks; social robotics; trust;NotRead;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{Munjal_Desai2009-en,
  title    = "Creating Trustworthy Robots: Lessons and Inspirations from
              Automated Systems",
  author   = "Munjal Desai, University of Massachusetts-Lowell and Kristen
              Stubbs, University of Massachusetts-Lowell and Aaron Steinfeld,
              Carnegie Mellon University and Holly Yanco, University of
              Massachusetts-Lowell and {Authors}",
  abstract = "One of the most significant challenges of human-robot interaction
              research is designing systems which foster an appropriate level
              of trust in their users: in order to use a robot effectively and
              safely, a user must place neither too little nor too much trust
              in the system. In order to better understand the factors which
              influence trust in a robot, we present a survey of prior work on
              trust in automated systems. We also discuss issues specific to
              robotics which pose challenges not addressed in the automation
              literature, particularly related to reliability, capability, and
              adjustable autonomy. We conclude with the results of a
              preliminary web-based questionnaire which illustrate some of the
              biases which autonomous robots may need to overcome in order to
              promote trust in users.",
  series   = "Robotics Institute",
  year     =  2009,
  keywords = "
              trust\_formal\_treatment;human\_study;assurance\_implicit;in\_paper;Assurances/Trust
              Background;Assurances"
}

@TECHREPORT{Chen2014-dk,
  title       = "Situation awareness-based agent transparency",
  author      = "Chen, Jessie Y and Procci, Katelyn and Boyce, Michael and
                 Wright, Julia and Garcia, Andre and Barnes, Michael",
  institution = "DTIC Document",
  year        =  2014,
  keywords    = "
                 very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;in\_presentation;Assurances/Trust
                 Background;Assurances"
}

@ARTICLE{Parasuraman1997-co,
  title     = "Humans and automation: Use, misuse, disuse, abuse",
  author    = "Parasuraman, Raja and Riley, Victor",
  abstract  = "Abstract This paper addresses theoretical, empirical, and
               analytical studies pertaining to human use, misuse, disuse, and
               abuse of automation technology. Use refers to the voluntary
               activation or disengagement of automation by human operators.
               Trust, mental workload, and",
  journal   = "Human Factors: The Journal of the Human Factors and Ergonomics
               Society",
  publisher = "SAGE Publications",
  volume    =  39,
  number    =  2,
  pages     = "230--253",
  year      =  1997,
  keywords  = "automation;interesting\_ideas;in\_presentation;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{Castelfranchi2007-gb,
  title     = "The role of beliefs in goal dynamics: prolegomena to a
               constructive theory of intentions",
  author    = "Castelfranchi, Cristiano and Paglieri, Fabio",
  abstract  = "In this article we strive to provide a detailed and principled
               analysis of the role of beliefs in goal processing---that is,
               the cognitive transition that leads from a mere desire to a
               proper intention. The resulting model of belief-based goal
               processing has also relevant consequences for the analysis of
               intentions, and constitutes the necessary core of a constructive
               theory of intentions, i.e. a framework that not only analyzes
               what an intention is, but also explains how it becomes what it
               is. We discuss similarities and differences between our approach
               and other standard accounts of intention, in particular
               Bratman's planning theory. The aim here is to question and
               refine the conceptual foundations of many theories of
               intentional action: as a consequence, although our analysis is
               not formal in itself, it is ultimately meant to have deep
               consequences for formal models of intentional agency.",
  journal   = "Synthese",
  publisher = "Kluwer Academic Publishers",
  volume    =  155,
  number    =  2,
  pages     = "237--263",
  month     =  mar,
  year      =  2007,
  keywords  = "Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{Pinyol2013-uy,
  title     = "Computational trust and reputation models for open multi-agent
               systems: a review",
  author    = "Pinyol, Isaac and Sabater-Mir, Jordi",
  abstract  = "In open environments, agents depend on reputation and trust
               mechanisms to evaluate the behavior of potential partners. The
               scientific research in this field has considerably increased,
               and in fact, reputation and trust mechanisms have been already
               considered a key elements in the design of multi-agent systems.
               In this paper we provide a survey that, far from being
               exhaustive, intends to show the most representative models that
               currently exist in the literature. For this enterprise we
               consider several dimensions of analysis that appeared in three
               existing surveys, and provide new dimensions that can be
               complementary to the existing ones and that have not been
               treated directly. Moreover, besides showing the original
               classification that each one of the surveys provide, we also
               classify models that where not taken into account by the
               original surveys. The paper illustrates the proliferation in the
               past few years of models that follow a more cognitive approach,
               in which trust and reputation representation as mental attitudes
               is as important as the final values of trust and reputation.
               Furthermore, we provide an objective definition of trust, based
               on Castelfranchi's idea that trust implies a decision to rely on
               someone.",
  journal   = "Artif Intell Rev",
  publisher = "Springer Netherlands",
  volume    =  40,
  number    =  1,
  pages     = "1--25",
  month     =  jun,
  year      =  2013,
  language  = "en"
}

@INCOLLECTION{Kessler2017-bw,
  title     = "A Comparison of Trust Measures in {Human--Robot} Interaction
               Scenarios",
  booktitle = "Advances in Human Factors in Robots and Unmanned Systems",
  author    = "Kessler, Theresa T and Larios, Cintya and Walker, Tiffani and
               Yerdon, Valarie and Hancock, P A",
  abstract  = "When studying Human--Robot Interaction (HRI), we often employ
               measures of trust. Trust is essential in HRI, as inappropriate
               levels of trust result in misuse, abuse, or disuse of that
               robot. Some measures of trust specifically target automation,
               while others specifically target HRI. Although robots are a type
               of automation, it is unclear which of the broader factors that
               define automation are shared by robots. However, measurements of
               trust in automation and trust in robots should theoretically
               still yield similar results. We examined an HRI scenario using
               (1) an automation trust scale and (2) a robotic trust scale.
               Findings indicated conflicting results coming from these
               respective trust scales. It may well be that these two trust
               scales examine separate constructs and are therefore not
               interchangeable. This discord shows us that future evaluations
               are required to identify scale appropriate context applications
               for either automation or robotic operations.",
  publisher = "Springer, Cham",
  pages     = "353--364",
  year      =  2017,
  keywords  = "Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Wang2016-id,
  title     = "Trust Calibration Within a {Human-Robot} Team: Comparing
               Automatically Generated Explanations",
  booktitle = "The Eleventh {ACM/IEEE} International Conference on Human Robot
               Interaction",
  author    = "Wang, Ning and Pynadath, David V and Hill, Susan G",
  abstract  = "... If the robots are less suited, then we want the humans to
               appropriately gauge the ... Human teammates will learn the
               correctness of the robot's decisions upon entering the buildings
               ... We modified items on interpersonal trust to measure trust in
               the robot's ability, benevolence and ...",
  publisher = "IEEE Press",
  pages     = "109--116",
  series    = "HRI '16",
  year      =  2016,
  address   = "Piscataway, NJ, USA",
  keywords  = "explainable a.i., human-robot interaction, pomdp,
               trust;assurance\_predictability;human\_study;POMDP;assurance\_competence;ai\_planning;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;in\_presentation;Supplemental
               Assurance;Reduce Complexity;Assurances/Trust
               Background;Assurances"
}

@INCOLLECTION{Xu2016-rz,
  title     = "Towards Modeling {Real-Time} Trust in Asymmetric {Human--Robot}
               Collaborations",
  booktitle = "Robotics Research",
  author    = "Xu, Anqi and Dudek, Gregory",
  editor    = "Inaba, Masayuki and Corke, Peter",
  abstract  = "We are interested in enhancing the efficiency of human--robot
               collaborations, especially in ``supervisor-worker'' settings
               where autonomous robots work under the supervision of a human
               operator. We believe that trust serves a critical role in
               modeling the interactions within these teams, and also in
               streamlining their efficiency. We propose an operational
               formulation of human--robot trust on a short interaction time
               scale, which is tailored to a practical tele-robotics setting.
               We also report on a controlled user study that collected
               interaction data from participants collaborating with an
               autonomous robot to perform visual navigation tasks. Our
               analyses quantify key correlations between real-time
               human--robot trust assessments and diverse factors, including
               properties of failure events reflecting causal trust
               attribution, as well as strong influences from each user's
               personality. We further construct and optimize a predictive
               model of users' trust responses to discrete events, which
               provides both insights on this fundamental aspect of real-time
               human--machine interaction, and also has pragmatic significance
               for designing trust-aware robot agents.",
  publisher = "Springer International Publishing",
  pages     = "113--129",
  series    = "Springer Tracts in Advanced Robotics",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{Charalambous2016-si,
  title     = "The Development of a Scale to Evaluate Trust in Industrial
               Human-robot Collaboration",
  author    = "Charalambous, George and Fletcher, Sarah and Webb, Philip",
  abstract  = "Trust has been identified as a key element for the successful
               cooperation between humans and robots. However, little research
               has been directed at understanding trust development in
               industrial human-robot collaboration (HRC). With industrial
               robots becoming increasingly integrated into production lines as
               a means for enhancing productivity and quality, it will not be
               long before close proximity industrial HRC becomes a viable
               concept. Since trust is a multidimensional construct and heavily
               dependent on the context, it is vital to understand how trust
               develops when shop floor workers interact with industrial
               robots. To this end, in this study a trust measurement scale
               suitable for industrial HRC was developed in two phases. In
               phase one, an exploratory study was conducted to collect
               participants' opinions qualitatively. This led to the
               identification of trust related themes relevant to the
               industrial context and a related pool of questionnaire items was
               generated. In the second phase, three human-robot trials were
               carried out in which the questionnaire items were applied to
               participants using three different types of industrial robots.
               The results were statistically analysed to identify the key
               factors impacting trust and from these generate a trust
               measurement scale for industrial HRC.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  8,
  number    =  2,
  pages     = "193--209",
  month     =  apr,
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@TECHREPORT{Schaefer2014-dn,
  title       = "A meta-analysis of factors influencing the development of
                 trust in automation: Implications for human-robot interaction",
  author      = "Schaefer, Kristin E and Billings, Deborah R and Szalma, James
                 L and Adams, Jeffrey K and Sanders, Tracy L and Chen, Jessie Y
                 and Hancock, Peter A",
  abstract    = "... For example, within robotics , unmanned aerial systems are
                 designed to be used in ... environment for application),
                 management (ie, coordination of the actions of humans and
                 robots ... robot teaming, de Visser and Parasuraman (2011)
                 found that human - robot teams benefited from ...",
  publisher   = "DTIC Document",
  institution = "DTIC Document",
  year        =  2014,
  keywords    = "NotRead;automation;Assurances/Trust Background"
}

@INPROCEEDINGS{Schaefer2012-ng,
  title       = "Classification of robot form: factors predicting perceived
                 trustworthiness",
  booktitle   = "Proceedings of the Human Factors and Ergonomics Society Annual
                 Meeting",
  author      = "Schaefer, Kristin E and Sanders, Tracy L and Yordon, Ryan E
                 and Billings, Deborah R and Hancock, P A",
  abstract    = "... For example, in social interactions between humans ,
                 attractiveness leads to more positive appraisals (Calvert,
                 1988). ... Anthropomorphism of robotic forms: A response to
                 affordances? Proc. ... Proceedings of the 5th ACM/IEEE
                 International Conference on Human Robot Interaction . ...",
  publisher   = "pro.sagepub.com",
  volume      =  56,
  pages       = "1548--1552",
  institution = "SAGE Publications",
  year        =  2012,
  keywords    = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Van_den_Brule2014-hf,
  title     = "Do Robot Performance and Behavioral Style affect Human Trust?",
  author    = "van den Brule, Rik and Dotsch, Ron and Bijlstra, Gijsbert and
               Wigboldus, Daniel H J and Haselager, Pim",
  abstract  = "An important aspect of a robot's social behavior is to convey
               the right amount of trustworthiness. Task performance has shown
               to be an important source for trustworthiness judgments. Here,
               we argue that factors such as a robot's behavioral style can
               play an important role as well. Our approach to studying the
               effects of a robot's performance and behavioral style on human
               trust involves experiments with simulated robots in video
               human--robot interaction (VHRI) and immersive virtual
               environments (IVE). Although VHRI and IVE settings cannot
               substitute for the genuine interaction with a real robot, they
               can provide useful complementary approaches to experimental
               research in social human robot interaction. VHRI enables rapid
               prototyping of robot behaviors. Simulating human--robot
               interaction in IVEs can be a useful tool for measuring human
               responses to robots and help avoid the many constraints caused
               by real-world hardware. However, there are also difficulties
               with the generalization of results from one setting (e.g., VHRI)
               to another (e.g. IVE or the real world), which we discuss. In
               this paper, we use animated robot avatars in VHRI to rapidly
               identify robot behavioral styles that affect human trust
               assessment of the robot. In a subsequent study, we use an IVE to
               measure behavioral interaction between humans and an animated
               robot avatar equipped with behaviors from the VHRI experiment.
               Our findings reconfirm that a robot's task performance
               influences its trustworthiness, but the effect of the behavioral
               style identified in the VHRI study did not influence the robot's
               trustworthiness in the IVE study.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  6,
  number    =  4,
  pages     = "519--531",
  month     =  nov,
  year      =  2014,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@PHDTHESIS{Schaefer2013-uu,
  title     = "The perception and measurement of human-robot trust",
  author    = "Schaefer, Kristin E",
  abstract  = "... HRI Human Robot Interaction IRB Institutional Review Board
               ... PI Perceived Intelligence RIVET Robotic Interactive
               Visualization \& Exploration Technology ... In the past two
               decades especially, we have seen a rapid influx of robotics into
               many everyday social environments. ...",
  publisher = "etd.fcla.edu",
  year      =  2013,
  school    = "University of Central Florida Orlando, Florida",
  keywords  = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Yagoda2012-ox,
  title     = "You Want Me to Trust a {ROBOT}? The Development of a
               {Human--Robot} Interaction Trust Scale",
  author    = "Yagoda, Rosemarie E and Gillan, Douglas J",
  abstract  = "Trust plays a critical role when operating a robotic system in
               terms of both acceptance and usage. Considering trust is a
               multidimensional context dependent construct, the differences
               and common themes were examined to identify critical
               considerations within human--robot interaction (HRI). In order
               to examine the role of trust within HRI, a measurement tool was
               generated based on five attributes: team configuration, team
               processes, context, task, and system (Yagoda in Human Factors
               and Ergonomics Society Annual Meeting, San Francisco, CA, pp.
               304--308, 2010). The HRI trust scale was developed based on two
               studies. The first study conducts a content validity assessment
               of preliminary items generated, based on a review of previous
               research within HRI and automation, using subject matter experts
               (SMEs). The second study assesses the quality of each trust
               scale item derived from the first study. The results were then
               compiled to generate the HRI trust measurement tool.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  4,
  number    =  3,
  pages     = "235--248",
  month     =  aug,
  year      =  2012,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Lomas2012-ie,
  title     = "Explaining Robot Actions",
  booktitle = "Proceedings of the Seventh Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Lomas, Meghann and Chevalier, Robert and Cross, II, Ernest
               Vincent and Garrett, Robert Christopher and Hoare, John and
               Kopack, Michael",
  publisher = "ACM",
  pages     = "187--188",
  series    = "HRI '12",
  year      =  2012,
  address   = "New York, NY, USA",
  keywords  = "explanations, human-robot partnering, natural communications,
               robotic actions,
               trust;human\_study;assurance\_explicit;trust\_informal\_treatment;explain;Supplemental
               Assurance;Assurances"
}

@MISC{Ahmed_undated-ie,
  title     = "Explaining Intelligent Autonomy through Adaptive Dialog and
               Machine {Self-Confidence}",
  author    = "Ahmed, Nisar and Frew, Eric and Lawrence, Dale",
  publisher = "DARPA",
  number    = "BAA 16-53",
  keywords  = "NotRead;needs\_classification;Assurances"
}

@INCOLLECTION{Lawless2016-vy,
  title     = "The Intersection of Robust Intelligence and Trust: Hybrid Teams,
               Firms and Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Lawless, W F and Sofge, Donald",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "We are developing the physics of interdependent uncertainty
               relations to efficiently and effectively control interdependence
               for autonomous hybrid teams (i.e., arbitrary combinations of
               humans, robots and machines), which cannot be done presently.
               Uncertainty is created in states of interdependence between
               social objects: at one extreme, interdependence reduces to
               independent agents and certainty but with asocial, low-power
               solutions generating little meaning or understanding in social
               contexts; oppositely, the length of interdependence increases
               across a group, de-individuating its members until individual
               identity dissolves (e.g., cults, gangs, well-run teams),
               increasing power, efficiency and meaning internal to a group,
               but also the chances of mal-adaptation (e.g., tragic mistakes).
               We focus on how interdependence increases the robust
               intelligence of a group by increasing its autonomy while
               decreasing its entropy, but requiring external control to be
               indirect. For humans, teamwork is an unsolved theoretical
               problem; solving it should generalize to the effective
               computational control of hybrid teams, a path forward for the
               users of a team to trust it to operate safely in hostile
               environments. Present theories of interdependence, like game
               theory or social science, are inadequate to formulate strategies
               to control teams; alternative theories like machine learning can
               control swarms with pattern formations, but not states of
               interdependence, such as multi-tasking operations. While
               alternative theories cannot be used to model teams,
               decision-making or social conflict at the same time (hostile
               mergers; checks and balances), ours can.",
  publisher = "Springer US",
  pages     = "255--270",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Yanco2016-eu,
  title     = "Methods for Developing Trust Models for Intelligent Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Yanco, Holly A and Desai, Munjal and Drury, Jill L and
               Steinfeld, Aaron",
  editor    = "Mittu, Ranjiv and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  publisher = "Springer",
  pages     = "219--254",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background"
}

@INCOLLECTION{Gao2016-da,
  title     = "Designing for Robust and Effective Teamwork in {Human-Agent}
               Teams",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Gao, Fei and Cummings, M L and Solovey, Erin",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "We investigated the impact of team structure, task uncertainty,
               and information-sharing tools on team coordination and team
               performance in human-agent teams. In applications such as search
               and rescue, command and control, and air traffic control,
               operators in the future will likely need to work in teams
               together with robots. It is critical to understand how these
               teams could be robust against uncertainty and what influences
               team performance. We conducted two experiments in which teams of
               three operators controlled simulated heterogeneous robots on the
               same testbed. Experiment 1 investigated the impact of team
               structure and uncertainty of task arrival processes on team
               coordination and performance. Experiment 2 explored the usage of
               information-sharing tools under different uncertainty levels. In
               Experiment 1, it was found that divisional teams were more
               robust against the uncertainty on task arrival processes.
               However, this robustness was achieved with an overall worse
               performance compared to functional teams. Three reasons for the
               degraded performance were identified, namely duplication on task
               assignment, under-utilization of vehicles, and infrequent
               communication. In Experiment 2, it was found that
               information-sharing tools reduced the duplication on task
               assignments, improved overall task performance, and reduced
               workload. These results provide insights for achieving robust
               and effective teamwork. This goal can be achieved by using a
               team structure that could adapt to uncertainties together with
               effective information-sharing tools. These findings could inform
               the design of robust teams and the development of
               information-sharing tools to improve teamwork.",
  publisher = "Springer US",
  pages     = "167--190",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Robinette2016-ys,
  title     = "Investigating {Human-Robot} Trust in Emergency Scenarios:
               Methodological Lessons Learned",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Robinette, Paul and Wagner, Alan R and Howard, Ayanna M",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "The word ``trust'' has many definitions that vary based on
               context and culture, so asking participants if they trust a
               robot is not as straightforward as one might think. The
               perceived risk involved in a scenario and the precise wording of
               a question can bias the outcome of a study in ways that the
               experimenter did not intend. This chapter presents the lessons
               we have learned about trust while conducting human-robot
               experiments with 770 human subjects. We discuss our work
               developing narratives that describe trust situations as well as
               interactive human-robot simulations. These experimental
               paradigms have guided our research exploring the meaning of
               trust, trust loss, and trust repair. By using crowdsourcing to
               locate and manage experiment participants, considerable
               diversity of opinion is found; there are, however, several
               considerations that must be included. Conclusions drawn from
               these experiments demonstrate the types of biases that
               participants are prone to as well as techniques for mitigating
               these biases.",
  publisher = "Springer US",
  pages     = "143--166",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Sadrfaridpour2016-di,
  title     = "Modeling and Control of Trust in {Human-Robot} Collaborative
               Manufacturing",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Sadrfaridpour, Behzad and Saeidi, Hamed and Burke, Jenny and
               Madathil, Kapil and Wang, Yue",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "Human-Robot Collaboration (HRC) on the factory floor has opened
               a new realm of manufacturing in real-world settings. In such
               applications, a human and robot work together with each other as
               coworkers while HRC plays a critical role in safety,
               productivity, and flexibility. In particular, human-robot trust
               determines his/her acceptance and hence allocation of autonomy
               to a robot, which alter the overall task efficiency and human
               workload. Inspired by well-known human factors research, we
               develop a time-series trust model for human-robot collaboration
               tasks, which is a function of prior trust, robot performance,
               and human performance. The robot performance is evaluated by its
               flexibility to keep pace with the human coworker and is molded
               as the difference between human and robot speed. The human
               performance in doing physical tasks is directly related to
               his/her muscle fatigue level. We use the muscle fatigue and
               recovery dynamics to capture the fatigue level of the human body
               when performing repetitive kinesthetic tasks, which are typical
               types of human motions in manufacturing. The robot speed can be
               controlled in three different modes: manually by the associate,
               autonomously through robust intelligence algorithms, or
               collaboratively by the combination of manual and autonomous
               inputs. We first simulate a typical 9-h work day for human robot
               collaborative tasks and implement the proposed trust model and
               the three control schemes. Furthermore, we experimentally
               validate our model and control schemes by conducting a series of
               human-in-the-loop experiments using the Rethink Robotics Baxter
               robot.",
  publisher = "Springer US",
  pages     = "115--141",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Palmer2016-ks,
  title     = "The ``Trust V'': Building and Measuring Trust in Autonomous
               Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Palmer, Gari and Selwyn, Anne and Zwillinger, Dan",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "For systems to be used they must be trusted. They must engender
               both system trust (system meets specifications) and operational
               trust (system meets user expectations). Simple systems are
               easily trustable. A light switch has only a few possible states.
               Complicated systems typically demonstrate trustability through
               established methods during their requirements confirmation
               processes (such as Test \& Evaluation and Verification \&
               Validation). While autonomous systems can use these same
               processes to establish trust, different methods within these
               processes are needed to address specific autonomous attributes
               such as ``adaptive'' or ``self-directed''. We have created a
               framework that identifies methods for engendering trust in
               automated and autonomous systems. These methods are used
               throughout a product's lifecycle. This paradigm led us to invent
               new methods that create both systems and operational trust for
               autonomous systems. Several examples of these new methods, that
               are useful for all systems, are given.",
  publisher = "Springer US",
  pages     = "55--77",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Floyd2016-da,
  title     = "Learning Trustworthy Behaviors Using an Inverse Trust Metric",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Floyd, Michael W and Drinkwater, Michael and Aha, David W",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "The addition of a robot to a human team can be beneficial if the
               robot can perform important tasks, provide additional skills, or
               otherwise help the team achieve its goals. However, if the human
               team members do not trust the robot they may underutilize it or
               excessively monitor its behavior. We present an algorithm that
               allows a robot to estimate its trustworthiness based on
               interactions with a team member and adapt its behavior in an
               attempt to increase its trustworthiness. The robot is able to
               learn as it performs behavior adaptation and increase the
               efficiency of future adaptation. We compare our approach for
               inverse trust estimation and behavior adaptation to a variant
               that does not learn. Our results, in a simulated robotics
               environment, show that both approaches can identify trustworthy
               behaviors but the learning approach does so significantly
               faster.",
  publisher = "Springer US",
  pages     = "33--53",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Taylor2016-vh,
  title     = "Towards Modeling the Behavior of Autonomous Systems and Humans
               for Trusted Operations",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Taylor, Gavin and Mittu, Ranjeev and Sibley, Ciara and Coyne,
               Joseph",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "Greater unmanned system autonomy will lead to improvements in
               mission outcomes, survivability and safety. However, an increase
               in platform autonomy increases system complexity. For example,
               flexible autonomous platforms deployed in a range of
               environments place a burden on humans to understand evolving
               behaviors. More importantly, when problems arise within complex
               systems, they need to be managed without increasing operator
               workload. A supervisory control paradigm can reduce workload and
               allow a single human to manage multiple autonomous platforms.
               However, this requires consideration of the human as an
               integrated part of the overall system, not just as a central
               controller. This paradigm can benefit from novel and intuitive
               techniques that isolate and predict anomalous situations or
               state trajectories within complex autonomous systems in terms of
               mission context to allow efficient management of aberrant
               behavior. This information will provide the user with improved
               feedback about system behavior, which will in turn lead to more
               relevant and effective prescriptions for interaction,
               particularly during emergency procedures. This, in turn, will
               enable proper trust calibration. We also argue that by
               understanding the context of the user's decisions or system's
               actions (seamless integration of the human), the autonomous
               platform can provide more appropriate information to the user.",
  publisher = "Springer US",
  pages     = "11--31",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{De_Visser2017-mq,
  title    = "A Little Anthropomorphism Goes a Long Way",
  author   = "de Visser, Ewart J and Monfort, Samuel S and Goodyear, Kimberly
              and Lu, Li and O'Hara, Martin and Lee, Mary R and Parasuraman,
              Raja and Krueger, Frank",
  abstract = "OBJECTIVE: We investigated the effects of exogenous oxytocin on
              trust, compliance, and team decision making with agents varying
              in anthropomorphism (computer, avatar, human) and reliability
              (100\%, 50\%). BACKGROUND: Authors of recent work have explored
              psychological similarities in how people trust humanlike
              automation compared with how they trust other humans. Exogenous
              administration of oxytocin, a neuropeptide associated with trust
              among humans, offers a unique opportunity to probe the
              anthropomorphism continuum of automation to infer when agents are
              trusted like another human or merely a machine. METHOD:
              Eighty-four healthy male participants collaborated with automated
              agents varying in anthropomorphism that provided recommendations
              in a pattern recognition task. RESULTS: Under placebo,
              participants exhibited less trust and compliance with automated
              aids as the anthropomorphism of those aids increased. Under
              oxytocin, participants interacted with aids on the extremes of
              the anthropomorphism continuum similarly to placebos but
              increased their trust, compliance, and performance with the
              avatar, an agent on the midpoint of the anthropomorphism
              continuum. CONCLUSION: This study provides the first evidence
              that administration of exogenous oxytocin affected trust,
              compliance, and team decision making with automated agents. These
              effects provide support for the premise that oxytocin increases
              affinity for social stimuli in automated aids. APPLICATION:
              Designing automation to mimic basic human characteristics is
              sufficient to elicit behavioral trust outcomes that are driven by
              neurological processes typically observed in human-human
              interactions. Designers of automated systems should consider the
              task, the individual, and the level of anthropomorphism to
              achieve the desired outcome.",
  journal  = "Hum. Factors",
  volume   =  59,
  number   =  1,
  pages    = "116--133",
  month    =  feb,
  year     =  2017,
  keywords = "autonomous agents; compliance and reliance; human--automation
              interaction; neuroergonomics; oxytocin; trust in automation;
              virtual humans;NotRead;Assurances/Trust Background",
  language = "en"
}

@MISC{Cai_undated-kx,
  title    = "Tuning trust using cognitive cues for better human-machine
              collaboration",
  author   = "Cai, Hua and Lin, Yingzi",
  journal  = "PsycEXTRA Dataset",
  keywords = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Inagaki1998-cl,
  title    = "Trust self-confidence and authority in human-machine systems",
  author   = "Inagaki, T and Moray, N and Itoh, M",
  journal  = "Proceedings of the IFAC man-machine systems",
  pages    = "431--436",
  year     =  1998,
  keywords = "human\_study;Assurances/Trust Background"
}

@ARTICLE{Jian2000-tp,
  title    = "Foundations for an Empirically Determined Scale of Trust in
              Automated Systems",
  author   = "Jian, Jiun-Yin and Bisantz, Ann M and Drury, Colin G",
  abstract = "One component in the successful use of automated systems is the
              extent to which people trust the automation to perform
              effectively. In order to understand the relationship between
              trust in computerized systems and the use of those systems, we
              need to be able to effectively measure trust. Although
              questionnaires regarding trust have been used in prior studies,
              these questionnaires were theoretically rather than empirically
              generated and did not distinguish between three potentially
              different types of trust: human-human trust, human-machine trust,
              and trust in general. A 3-phased experiment, comprising a word
              elicitation study, a questionnaire study, and a paired comparison
              study, was performed to better understand similarities and
              differences in the concepts of trust and distrust, and among the
              different types of trust. Results indicated that trust and
              distrust can be considered opposites, rather than different
              concepts. Components of trust, in terms of words related to
              trust, were similar across the three types of trust. Results
              obtained from a cluster analysis were used to identify 12
              potential factors of trust between people and automated systems.
              These 12 factors were then used to develop a proposed scale to
              measure trust in automation.",
  journal  = "Int. J. Cogn. Ergon.",
  volume   =  4,
  number   =  1,
  pages    = "53--71",
  year     =  2000,
  keywords = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Lee1992-hx,
  title    = "Trust, control strategies and allocation of function in
              human-machine systems",
  author   = "Lee, J and Moray, N",
  abstract = "As automated controllers supplant human intervention in
              controlling complex systems, the operators' role often changes
              from that of an active controller to that of a supervisory
              controller. Acting as supervisors, operators can choose between
              automatic and manual control. Improperly allocating function
              between automatic and manual control can have negative
              consequences for the performance of a system. Previous research
              suggests that the decision to perform the job manually or
              automatically depends, in part, upon the trust the operators
              invest in the automatic controllers. This paper reports an
              experiment to characterize the changes in operators' trust during
              an interaction with a semi-automatic pasteurization plant, and
              investigates the relationship between changes in operators'
              control strategies and trust. A regression model identifies the
              causes of changes in trust, and a 'trust transfer function' is
              developed using time series analysis to describe the dynamics of
              trust. Based on a detailed analysis of operators' strategies in
              response to system faults we suggest a model for the choice
              between manual and automatic control, based on trust in automatic
              controllers and self-confidence in the ability to control the
              system manually.",
  journal  = "Ergonomics",
  volume   =  35,
  number   =  10,
  pages    = "1243--1270",
  month    =  oct,
  year     =  1992,
  keywords = "NotRead;Assurances/Trust Background",
  language = "en"
}

@ARTICLE{Lucas2010-dt,
  title     = "Should We Trust Experiments on Trust?",
  author    = "Lucas, A J and Lewis, C",
  journal   = "Hum. Dev.",
  publisher = "Karger Publishers",
  volume    =  53,
  number    =  4,
  pages     = "167--172",
  month     =  sep,
  year      =  2010,
  keywords  = "meh..;Assurances/Trust Background;Assurances",
  language  = "en"
}

@ARTICLE{Castelfranchi2000-xr,
  title    = "Trust and control: A dialectic link",
  author   = "Castelfranchi, Cristiano and Falcone, Rino",
  abstract = "The relationship between trust and control is quite relevant both
              for the very notion of trust and for modelling and implementing
              trust-control relations with autonomous systems, but it is not
              trivial at all. On the one side, it is true that where / when
              there is control there is no trust, and vice versa. However, this
              refers to a restricted notion of trust: i.e., ``trust in y,''
              which is just a part, a component of the global trust needed for
              relying on the action of another agent. It is claimed that
              control is antagonistic of this strict form of trust; but also
              that it completes and complements it for arriving to a global
              trust. In other words, putting control and guarantees is
              trust-building; it produces a sufficient trust, when trust in y's
              autonomous willingness and competence would not be enough. It is
              also argued that control requires new forms of trust: trust in
              the control itself or in the controller, trust in y as for being
              monitored and controlled; trust in possible authorities, etc.
              Finally, it is shown that, paradoxically, control could not be
              antagonistic of strict trust in y, but it can even create and
              increase it by making y more willing or more effective. In
              conclusion, depending on the circumstances, control makes y more
              reliable or less reliable; control can either decrease or
              increase trust. Two kinds of control are also analyzed,
              characterized by two different functions: ``pushing or
              influencing control'' aimed at preventing violations or mistakes,
              versus ``safety, correction, or adjustment control'' aimed at
              preventing failure or damages after a violation or a mistake. A
              good theory of trust cannot be complete without a theory of
              control.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  8,
  pages    = "799--823",
  year     =  2000,
  keywords = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Schillo2000-ap,
  title    = "Using trust for detecting deceitful agents in artificial
              societies",
  author   = "Schillo, Michael and Funk, Petra and Rovatsos, Michael",
  abstract = "Trust is one of the most important concepts guiding
              decision-making and contracting in human societies. In artificial
              societies, this concept has been neglected until recently. The
              inherent benevolence assumption implemented in many multiagent
              systems can have hazardous consequences when dealing with deceit
              in open systems. The aim of this paper is to establish a
              mechanism that helps agents to cope with environments inhabited
              by both selfish and cooperative entities. This is achieved by
              enabling agents to evaluate trust in others. A formalization and
              an algorithm for trust are presented so that agents can
              autonomously deal with deception and identify trustworthy parties
              in open systems. The approach is twofold: agents can observe the
              behavior of others and thus collect information for establishing
              an initial trust model. In order to adapt quickly to a new or
              rapidly changing environment, one enables agents to also make use
              of observations from other agents. The practical relevance of
              these ideas is demonstrated by means of a direct mapping from a
              scenario to electronic commerce.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  8,
  pages    = "825--848",
  year     =  2000,
  keywords = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Muir1987-mk,
  title    = "Trust between humans and machines, and the design of decision
              aids",
  author   = "Muir, Bonnie M",
  abstract = "A problem in the design of decision aids is how to design them so
              that decision makers will trust them and therefore use them
              appropriately. This problem is approached in this paper by taking
              models of trust between humans as a starting point, and extending
              these to the human-machine relationship. A definition and model
              of human-machine trust are proposed, and the dynamics of trust
              between humans and machines are examined. Based upon this
              analysis, recommendations are made for calibrating users' trust
              in decision aids.",
  journal  = "Int. J. Man. Mach. Stud.",
  volume   =  27,
  number   =  5,
  pages    = "527--539",
  month    =  nov,
  year     =  1987,
  keywords = "
              very\_similar\_to\_mine;assurances;decision\_support;automation;trust\_formal\_treatment;assurance\_explicit;in\_paper;Assurances/Trust
              Background;Assurances"
}

@ARTICLE{Zacharia2000-ur,
  title    = "Trust management through reputation mechanisms",
  author   = "Zacharia, Giorgos and Maes, Pattie",
  abstract = "The members of electronic communities are often unrelated to each
              other; they may have never met and have no information on each
              other's reputation. This kind of information is vital in
              electronic commerce interactions, where the potential
              counterpart's reputation can be a significant factor in the
              negotiation strategy. Two complementary reputation mechanisms are
              investigated which rely on collaborative rating and personalized
              evaluation of the various ratings assigned to each user. While
              these reputation mechanisms are developed in the context of
              electronic commerce, it is believed that they may have
              applicability in other types of electronic communities such as
              chatrooms, newsgroups, mailing lists, etc.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  9,
  pages    = "881--907",
  year     =  2000,
  keywords = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Sabater2005-wr,
  title     = "Review on Computational Trust and Reputation Models",
  author    = "Sabater, Jordi and Sierra, Carles",
  abstract  = "The scientific research in the area of computational mechanisms
               for trust and reputation in virtual societies is a recent
               discipline oriented to increase the reliability and performance
               of electronic communities. Computer science has moved from the
               paradigm of isolated machines to the paradigm of networks and
               distributed computing. Likewise, artificial intelligence is
               quickly moving from the paradigm of isolated and non-situated
               intelligence to the paradigm of situated, social and collective
               intelligence. The new paradigm of the so called intelligent or
               autonomous agents and multi-agent systems (MAS) together with
               the spectacular emergence of the information society
               technologies (specially reflected by the popularization of
               electronic commerce) are responsible for the increasing interest
               on trust and reputation mechanisms applied to electronic
               societies. This review wants to offer a panoramic view on
               current computational trust and reputation models.",
  journal   = "Artif Intell Rev",
  publisher = "Kluwer Academic Publishers",
  volume    =  24,
  number    =  1,
  pages     = "33--60",
  month     =  sep,
  year      =  2005,
  keywords  = "Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Wagner_undated-bk,
  title           = "Exploring human-robot trust: Insights from the first 1000
                     subjects",
  booktitle       = "2015 International Conference on Collaboration
                     Technologies and Systems ({CTS})",
  author          = "Wagner, Alan R",
  publisher       = "IEEE",
  pages           = "485--486",
  keywords        = "NotRead;Assurances/Trust Background",
  conference      = "2015 International Conference on Collaboration
                     Technologies and Systems (CTS)"
}

@INPROCEEDINGS{Ximenes_undated-wd,
  title           = "Extreme human-robot interfaces: Increasing trust and
                     assurance around robots",
  booktitle       = "The 23rd {IEEE} International Symposium on Robot and Human
                     Interactive Communication",
  author          = "Ximenes, Bianca H and Moreira, Icaro M and Kelner, Judith",
  publisher       = "IEEE",
  pages           = "1006--1011",
  keywords        = "NotRead;Assurances/Trust Background",
  conference      = "2014 RO-MAN: The 23rd IEEE International Symposium on
                     Robot and Human Interactive Communication"
}

@INPROCEEDINGS{Billings2012-vk,
  title     = "Human-robot interaction: Developing trust in robots",
  booktitle = "2012 7th {ACM/IEEE} International Conference on {Human-Robot}
               Interaction ({HRI})",
  author    = "Billings, D R and Schaefer, K E and Chen, J Y C and Hancock, P A",
  abstract  = "In all human-robot interaction, trust is an important element to
               consider because the presence or absence of trust certainly
               impacts the ultimate outcome of that interaction. Limited
               research exists that delineates the development and maintenance
               of this trust in various operational contexts. Our own prior
               research has investigated theoretical and empirically supported
               antecedents of human-robot trust. Here, we describe progress to
               date relating to the development of a comprehensive human-robot
               trust model based on our ongoing program of research.",
  pages     = "109--110",
  month     =  mar,
  year      =  2012,
  keywords  = "human-robot interaction;human-robot interaction;human-robot
               trust model;trust development;Collaboration;Current
               measurement;Educational institutions;Human
               factors;Humans;Robots;Training;Human-Robot
               Interaction;Trust;NotRead;Assurances/Trust Background"
}

@INPROCEEDINGS{Kaniarasu_undated-ek,
  title           = "Robot confidence and trust alignment",
  booktitle       = "2013 8th {ACM/IEEE} International Conference on
                     {Human-Robot} Interaction ({HRI})",
  author          = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal
                     and Yanco, Holly",
  publisher       = "IEEE",
  pages           = "155--156",
  keywords        = "NotRead;Assurances/Trust Background",
  conference      = "2013 8th ACM/IEEE International Conference on Human-Robot
                     Interaction (HRI)"
}

@INPROCEEDINGS{Sanders2011-kz,
  title       = "A Model of human-robot trust: Theoretical model development",
  booktitle   = "Proceedings of the Human Factors and Ergonomics Society Annual
                 Meeting",
  author      = "Sanders, Tracy and Oleson, Kristin E and Billings, D R and
                 Chen, Jessie Y C and Hancock, P A",
  volume      =  55,
  pages       = "1432--1436",
  institution = "SAGE Publications Sage CA: Los Angeles, CA",
  year        =  2011,
  keywords    = "NotRead;Assurances/Trust Background"
}

@ARTICLE{Hancock2011-dh,
  title    = "A meta-analysis of factors affecting trust in human-robot
              interaction",
  author   = "Hancock, Peter A and Billings, Deborah R and Schaefer, Kristin E
              and Chen, Jessie Y C and de Visser, Ewart J and Parasuraman, Raja",
  abstract = "OBJECTIVE: We evaluate and quantify the effects of human, robot,
              and environmental factors on perceived trust in human-robot
              interaction (HRI). BACKGROUND: To date, reviews of trust in HRI
              have been qualitative or descriptive. Our quantitative review
              provides a fundamental empirical foundation to advance both
              theory and practice. METHOD: Meta-analytic methods were applied
              to the available literature on trust and HRI. A total of 29
              empirical studies were collected, of which 10 met the selection
              criteria for correlational analysis and 11 for experimental
              analysis. These studies provided 69 correlational and 47
              experimental effect sizes. RESULTS: The overall correlational
              effect size for trust was r = +0.26,with an experimental effect
              size of d = +0.71. The effects of human, robot, and environmental
              characteristics were examined with an especial evaluation of the
              robot dimensions of performance and attribute-based factors. The
              robot performance and attributes were the largest contributors to
              the development of trust in HRI. Environmental factors played
              only a moderate role. CONCLUSION: Factors related to the robot
              itself, specifically, its performance, had the greatest current
              association with trust, and environmental factors were moderately
              associated. There was little evidence for effects of
              human-related factors. APPLICATION: The findings provide
              quantitative estimates of human, robot, and environmental factors
              influencing HRI trust. Specifically, the current summary provides
              effect size estimates that are useful in establishing design and
              training guidelines with reference to robot-related factors of
              HRI trust. Furthermore, results indicate that improper trust
              calibration may be mitigated by the manipulation of robot design.
              However, many future research needs are identified.",
  journal  = "Hum. Factors",
  volume   =  53,
  number   =  5,
  pages    = "517--527",
  month    =  oct,
  year     =  2011,
  keywords = "NotRead;Assurances/Trust Background",
  language = "en"
}

@ARTICLE{Laursen2013-ul,
  title    = "Robot to human: ``Trust me''",
  author   = "Laursen, L",
  abstract = "In a crisis control center, several teams of firefighters in
              Montelibretti, Italy, used laptops to guide a robotic ground
              vehicle into a smoke-filled highway tunnel. Inside, overturned
              motorcycles, errant cars, and spilled pallets impeded the
              robot{\^A}\?\`s progress. The rover, equipped with a video camera
              and autonomous navigation software, was capable of crawling
              through the wreckage unguided while humans monitored the video
              footage for accident victims. But most of the time the
              firefighters took manual control once the robot was a few meters
              into the tunnel.",
  journal  = "IEEE Spectrum",
  volume   =  50,
  number   =  3,
  pages    = "18--18",
  month    =  mar,
  year     =  2013,
  keywords = "Accidents;Cameras;Control systems;Disasters;Fires;Mobile
              robots;NotRead;Assurances/Trust Background"
}

@INCOLLECTION{Schaefer2016-yq,
  title     = "Measuring Trust in Human Robot Interactions: Development of the
               ``Trust Perception {Scale-HRI''}",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Schaefer, Kristin E",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "As robots penetrate further into the everyday environments,
               trust in these robots becomes a crucial issue. The purpose of
               this work was to create and validate a reliable scale that could
               measure changes in an individual's trust in a robot. Assessment
               of current trust theory identified measurable antecedents
               specific to the human, the robot, and the environment. Six
               experiments subsumed the development of the 40 item trust scale.
               Scale development included the creation of a 156 item pool. Two
               experiments identified the robot features and perceived
               functional characteristics that were related to the
               classification of a machine as a robot for this item pool. Item
               pool reduction techniques and subject matter expert (SME)
               content validation were used to reduce the scale to 42 items.
               The two final experiments were then conducted to validate the
               scale. The finalized 40 item pre-post interaction trust scale
               was designed to measure trust perceptions specific to
               human-robot interaction. The scale measures trust on a 0--100 \%
               rating scale and provides a percentage trust score. A 14 item
               sub-scale of this final version of the test recommended by SMEs
               may be sufficient for some HRI tasks, and the implications of
               this proposition are discussed.",
  publisher = "Springer US",
  pages     = "191--218",
  year      =  2016,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Kaniarasu_undated-gj,
  title           = "Effects of blame on trust in human robot interaction",
  booktitle       = "The 23rd {IEEE} International Symposium on Robot and Human
                     Interactive Communication",
  author          = "Kaniarasu, Poornima and Steinfeld, Aaron M",
  publisher       = "IEEE",
  pages           = "850--855",
  keywords        = "NotRead;Assurances/Trust Background",
  conference      = "2014 RO-MAN: The 23rd IEEE International Symposium on
                     Robot and Human Interactive Communication"
}

@INCOLLECTION{Wang2014-wk,
  title     = "{Human-Robot} Mutual Trust in (Semi)autonomous Underwater Robots",
  booktitle = "Cooperative Robots and Sensor Networks 2014",
  author    = "Wang, Yue and Shi, Zhenwu and Wang, Chuanfeng and Zhang, Fumin",
  editor    = "Koubaa, Anis and Khelil, Abdelmajid",
  abstract  = "It is envisioned that a human operator is able to monitor and
               control one or more (semi)autonomous underwater robots
               simultaneously in future marine operations. To enable such
               operations, a human operator must trust the capability of a
               robot to perform tasks autonomously, and the robot must
               establish its trust to the human operator based on human
               performance and follow guidance accordingly. Therefore, we seek
               to i model the mutual trust between humans and robots
               (especially (semi)autonomous underwater robots in this chapter),
               and ii) develop a set of trust-based algorithms to control the
               human-robot team so that the mutual trust level can be
               maintained at a desired level. We propose a time series based
               mutual trust model that takes into account robot performance,
               human performance and overall human-robot system fault rates.
               The robot performance model captures the performance evolution
               of a robot under autonomous mode and teleoperated mode,
               respectively. Furthermore, we specialize the robot performance
               model of a YSI EcoMapper autonomous underwater robot based on
               its distance to a desired waypoint. The human performance model
               is inspired by the Yerkes-Dodson law in psychology, which
               describes the relationship between human arousal and
               performance. Based on the mutual trust model, we first study a
               simple case of one human operator controlling a single robot and
               propose a trust-triggered control strategy depending on the
               limit conditions of the desired trust region. The method is then
               enhanced for the case of one human operator controlling a swarm
               of robots. In this framework, a periodic trust-based control
               strategy with a highest-trust-first scheduling algorithm is
               proposed. Matlab simulation results are provided to validate the
               proposed model and control strategies that guarantee effective
               real-time scheduling of teleoperated and autonomous controls in
               both one human one underwater robot case and one human multiple
               underwater robots case.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "115--137",
  series    = "Studies in Computational Intelligence",
  year      =  2014,
  keywords  = "NotRead;Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Kaniarasu2013-ho,
  title     = "Robot Confidence and Trust Alignment",
  booktitle = "Proceedings of the 8th {ACM/IEEE} International Conference on
               Human-robot Interaction",
  author    = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal and
               Yanco, Holly",
  publisher = "IEEE Press",
  pages     = "155--156",
  series    = "HRI '13",
  year      =  2013,
  address   = "Piscataway, NJ, USA",
  keywords  = "automation, experiments, robot confidence,
               trust;human\_study;assurance\_competence;ai\_motion\_manipulation;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;Supplemental
               Assurance;Reduce Complexity;Assurances/Trust
               Background;Assurances"
}

@ARTICLE{McDermott2011-ok,
  title     = "Adaptive aiding of human-robot teaming: Effects of imperfect
               automation on performance, trust, and workload",
  author    = "McDermott, Patricia L and Riley, Jennifer M and Gillan, Douglas
               J and Cuevas, Haydee M and de Visser, Ewart and Parasuraman,
               Raja",
  journal   = "Journal of Cognitive Engineering and Decision Making",
  publisher = "SAGE Publications Sage CA: Los Angeles, CA",
  volume    =  5,
  number    =  2,
  pages     = "209--231",
  year      =  2011,
  keywords  = "NotRead;Assurances/Trust Background"
}

@TECHREPORT{Billings2012-ao,
  title       = "Human-animal trust as an analog for human-robot trust: A
                 review of current evidence",
  author      = "Billings, Deborah R and Schaefer, Kristin E and Chen, Jessie Y
                 and Kocsis, Vivien and Barrera, Maria and Cook, Jacquelyn and
                 Ferrer, Michelle and Hancock, Peter A",
  institution = "DTIC Document",
  year        =  2012,
  keywords    = "NotRead;Assurances/Trust Background"
}

@INPROCEEDINGS{Oleson2011-mw,
  title     = "Antecedents of trust in human-robot collaborations",
  booktitle = "2011 {IEEE} International {Multi-Disciplinary} Conference on
               Cognitive Methods in Situation Awareness and Decision Support
               ({CogSIMA})",
  author    = "Oleson, K E and Billings, D R and Kocsis, V and Chen, J Y C and
               Hancock, P A",
  abstract  = "Robotic systems are being introduced into military echelons to
               extend warfighter capabilities in complex, dynamic environments.
               While these systems are designed to complement human
               capabilities (e.g., aiding in battlefield situation awareness
               and decision making, etc), they are often misused or disused
               because the user does not have an appropriate level of trust in
               his or her robotic counterpart(s). We describe a continuing body
               of research that identifies factors impacting a human's level of
               trust in a robotic teammate. The factors identified to date can
               be categorized as human influences (e.g., individual differences
               in terms of personality, experience, culture), machine
               influences (e.g., robotic platform, robot performance in terms
               of levels of automation, failure rates, false alarms), and
               environmental influences (e.g. task type, operational
               environment, shared mental models). A framework for human-robot
               team trust was constructed, which is evolving into a working
               model contingent upon the results of an on-going meta-analysis.",
  pages     = "175--178",
  month     =  feb,
  year      =  2011,
  keywords  = "human factors;human-robot interaction;military
               systems;environmental influence;human capability;human-robot
               collaboration;human-robot team trust;meta analysis;military
               echelon;robotic teammate;warflghter
               capability;Automation;Collaboration;Humans;Robot
               kinematics;Service robots;Training;Human-robot interaction;human
               factors;human-robot teams;trust;NotRead;Assurances/Trust
               Background"
}

@INPROCEEDINGS{Carter2003-ni,
  title     = "Value centric trust in multiagent systems",
  booktitle = "Proceedings {IEEE/WIC} International Conference on Web
               Intelligence ({WI} 2003)",
  author    = "Carter, J and Ghorbani, A A",
  abstract  = "We focus on the design and implementation of a new model of
               trust based on the formalizations of reputation, self-esteem,
               and similarity within an agent. We universalize reputation
               through the use of values found within all multiagent systems.
               The following values are manifested within multiagent systems:
               responsibility, honesty, independence, obedience, ambition,
               helpfulness, capability, knowledgability, and cost-efficiency.
               Manifestations of these values lead to a more universalized
               approach to formalizing reputation. This new model of trust is
               examined within the context of an e-commerce framework. It is
               analyzed with respect to stability, scalability, accuracy in
               attaining e-commerce objectives, and general effectiveness in
               discouraging untrustworthy behavior. Based on the experiments,
               the model is scalable and stable dependent upon the agent
               population of buyers and sellers. It achieves its primary
               objective of discouraging untrustworthy behavior as measured
               through the acceleration of Gross Domestic Product growth over
               time.",
  pages     = "3--9",
  month     =  oct,
  year      =  2003,
  keywords  = "economic indicators;electronic commerce;multi-agent
               systems;security of data;Gross Domestic
               Product;cost-efficiency;e-commerce;knowledgability;multiagent
               systems;responsibility;untrustworthy behavior;value centric
               trust;Accelerated aging;Computer science;Context
               modeling;Electronic commerce;Humans;Multiagent
               systems;Niobium;Scalability;Stability analysis;Time
               measurement;NotRead;Assurances/Trust Background"
}

@INPROCEEDINGS{Castelfranchi2003-ue,
  title     = "Trust in Information Sources As a Source for Trust: A Fuzzy
               Approach",
  booktitle = "Proceedings of the Second International Joint Conference on
               Autonomous Agents and Multiagent Systems",
  author    = "Castelfranchi, Cristiano and Falcone, Rino and Pezzulo, Giovanni",
  publisher = "ACM",
  pages     = "89--96",
  series    = "AAMAS '03",
  year      =  2003,
  address   = "New York, NY, USA",
  keywords  = "beliefs, fuzzy cognitive maps, medical house assistance
               scenarios, sources of beliefs, trust;Assurances/Trust Background"
}

@INCOLLECTION{Falcone2001-jc,
  title     = "Social Trust: A Cognitive Approach",
  booktitle = "Trust and Deception in Virtual Societies",
  author    = "Falcone, Rino and Castelfranchi, Cristiano",
  editor    = "Castelfranchi, Cristiano and Tan, Yao-Hua",
  abstract  = "As it was been written in the call of the original workshop ``In
               recent research on electronic commerce'' trust has been
               recognized as one of the key factors for successful electronic
               commerce adoption. In electronic commerce problems of trust are
               magnified, because agents reach out far beyond their familiar
               trade environments. Also it is far from obvious whether existing
               paper-based techniques for fraud detection and prevention are
               adequate to establish trust in an electronic network environment
               where you usually never meet your trade partner face to face,
               and where messages can be read or copied a million times without
               leaving any trace. With the growing impact of electronic
               commerce distance trust building becomes more and more
               important, and better models of trust and deception are needed.
               One trend is that in electronic communication channels extra
               agents, the so called Trusted Third Parties, are introduced in
               an agent community that take care of trust building among the
               other agents in the network. But in fact different kind of trust
               are needed and should be modelled and supported: trust in the
               environment and in the infrastructure (the socio-technical
               system); trust in your agent and in mediating agents; trust in
               the potential partners; trust in the warrantors and authorities
               (if any).",
  publisher = "Springer Netherlands",
  pages     = "55--90",
  year      =  2001,
  keywords  = "Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{Granatyr2015-hy,
  title     = "Trust and Reputation Models for Multiagent Systems",
  author    = "Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert
               and Scalabrin, Edson Em{\'\i}lio and Barth{\`e}s, Jean-Paul and
               Enembreck, Fabr{\'\i}cio",
  journal   = "ACM Comput. Surv.",
  publisher = "ACM",
  volume    =  48,
  number    =  2,
  pages     = "27:1--27:42",
  month     =  oct,
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "Trust, reputation, trust model;Assurances/Trust Background"
}

@MISC{Lloyd2014-bb,
  title        = "Automatic Statistician",
  booktitle    = "Automatic Statistician",
  author       = "Lloyd, James Robert",
  month        =  mar,
  year         =  2014,
  howpublished = "\url{https://www.automaticstatistician.com/examples/}",
  note         = "Accessed: 2017-2-16",
  keywords     = "trust\_popular\_media;trust\_academic\_conversation;Assurances"
}

@ARTICLE{Ghahramani2015-yq,
  title    = "Probabilistic machine learning and artificial intelligence",
  author   = "Ghahramani, Zoubin",
  abstract = "How can a machine learn from experience? Probabilistic modelling
              provides a framework for understanding what learning is, and has
              therefore emerged as one of the principal theoretical and
              practical approaches for designing machines that learn from data
              acquired through experience. The probabilistic framework, which
              describes how to represent and manipulate uncertainty about
              models and predictions, has a central role in scientific data
              analysis, machine learning, robotics, cognitive science and
              artificial intelligence. This Review provides an introduction to
              this framework, and discusses some of the state-of-the-art
              advances in the field, namely, probabilistic programming,
              Bayesian optimization, data compression and automatic model
              discovery.",
  journal  = "Nature",
  volume   =  521,
  number   =  7553,
  pages    = "452--459",
  month    =  may,
  year     =  2015,
  keywords = "trust\_popular\_media;trust\_academic\_conversation;Assurances",
  language = "en"
}

@ARTICLE{Castelvecchi2016-mr,
  title    = "Can we open the black box of {AI}?",
  author   = "Castelvecchi, Davide",
  journal  = "Nature",
  volume   =  538,
  number   =  7623,
  pages    = "20--23",
  month    =  oct,
  year     =  2016,
  keywords = "trust\_popular\_media;trust\_academic\_conversation;Assurances",
  language = "en"
}

@MISC{Khosravi2016-ke,
  title        = "Will You Trust {AI} To Be Your New Doctor?",
  booktitle    = "Forbes",
  author       = "Khosravi, Bijan",
  abstract     = "Whatever the case, AI has always had the appeal of a
                  futuristic spectacle; fascinating but not likely to take
                  place in our lifetimes. That image is about to change,
                  particularly in the healthcare industry.",
  month        =  mar,
  year         =  2016,
  howpublished = "\url{http://www.forbes.com/sites/bijankhosravi/2016/03/24/will-you-trust-ai-to-be-your-new-doctor-a-five-year-outcome/}",
  note         = "Accessed: 2017-2-16",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation;Assurances"
}

@MISC{Banavar2016-nm,
  title        = "What It Will Take for Us to Trust {AI}",
  booktitle    = "Harvard Business Review",
  author       = "Banavar, Guru",
  abstract     = "Ethics and accountability.",
  month        =  nov,
  year         =  2016,
  howpublished = "\url{https://hbr.org/2016/11/what-it-will-take-for-us-to-trust-ai}",
  note         = "Accessed: 2017-2-16",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation;Assurances"
}

@ARTICLE{Cohn1996-fd,
  title         = "Active Learning with Statistical Models",
  author        = "Cohn, D A and Ghahramani, Z and Jordan, M I",
  abstract      = "For many types of machine learning algorithms, one can
                   compute the statistically `optimal' way to select training
                   data. In this paper, we review how optimal data selection
                   techniques have been used with feedforward neural networks.
                   We then show how the same principles may be used to select
                   data for two alternative, statistically-based learning
                   architectures: mixtures of Gaussians and locally weighted
                   regression. While the techniques for neural networks are
                   computationally expensive and approximate, the techniques
                   for mixtures of Gaussians and locally weighted regression
                   are both efficient and accurate. Empirically, we observe
                   that the optimality criterion sharply decreases the number
                   of training examples the learner needs in order to achieve
                   good performance.",
  month         =  mar,
  year          =  1996,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "cs/9603104"
}

@INPROCEEDINGS{Hutter2006-ak,
  title      = "Performance Prediction and Automated Tuning of Randomized and
                Parametric Algorithms",
  booktitle  = "Principles and Practice of Constraint Programming - {CP} 2006",
  author     = "Hutter, Frank and Hamadi, Youssef and Hoos, Holger H and
                Leyton-Brown, Kevin",
  editor     = "Benhamou, Fr{\'e}d{\'e}ric",
  abstract   = "Machine learning can be used to build models that predict the
                run-time of search algorithms for hard combinatorial problems.
                Such empirical hardness models have previously been studied for
                complete, deterministic search algorithms. In this work, we
                demonstrate that such models can also make surprisingly
                accurate predictions of the run-time distributions of
                incomplete and randomized search methods, such as stochastic
                local search algorithms. We also show for the first time how
                information about an algorithm's parameter settings can be
                incorporated into a model, and how such models can be used to
                automatically adjust the algorithm's parameters on a
                per-instance basis in order to optimize its performance.
                Empirical results for Novelty + and SAPS on structured and
                unstructured SAT instances show very good predictive
                performance and significant speedups of our automatically
                determined parameter settings when compared to the default and
                best fixed distribution-specific parameter settings.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "213--228",
  series     = "Lecture Notes in Computer Science",
  month      =  sep,
  year       =  2006,
  keywords   = "assurance\_implicit;trust\_informal\_treatment;empirical\_performance;Assurances",
  language   = "en",
  conference = "International Conference on Principles and Practice of
                Constraint Programming"
}

@INPROCEEDINGS{Kuter2015-qh,
  title     = "Computational Mechanisms to Support Reporting of Self Confidence
               of {Automated/Autonomous} Systems",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Kuter, Ugur and Miller, Chris",
  abstract  = "This paper describes a new candidate method of computing
               autonomous ``self confidence.'' We describe how to analyze a
               plan for possible but unexpected break down cases and how to
               adapt the plan to circumvent those conditions. We view the
               result plan as more stable than the original one. The ability of
               achieving such plan stability is the core of how we propose to
               compute a system's self confidence in its decisions and plans.
               This paper summarizes this approach and presents a preliminary
               evaluation that shows our approach is promising.",
  month     =  sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;perf\_prediction;Supplemental
               Assurance;Quantify Uncertainty;Assurances",
  language  = "en"
}

@INPROCEEDINGS{OCallaghan2005-wa,
  title      = "Generating Corrective Explanations for Interactive Constraint
                Satisfaction",
  booktitle  = "Principles and Practice of Constraint Programming - {CP} 2005",
  author     = "O'Callaghan, Barry and O'Sullivan, Barry and Freuder, Eugene C",
  editor     = "van Beek, Peter",
  abstract   = "Interactive tasks such as online configuration and e-commerce
                can be modelled as constraint satisfaction problems (CSPs).
                These can be solved interactively by a user assigning values to
                variables. The user may require advice and explanations from a
                system to help him/her find a satisfactory solution.
                Explanations of failure in constraint programming tend to focus
                on conflict. However, what is really desirable is an
                explanation that is corrective in the sense that it provides
                the basis for moving forward in the problem-solving process.
                More specifically, when faced with a dead-end, or when a
                desirable value has been removed from a domain, we need to
                compute alternative assignments for a subset of the assigned
                variables that enables the user to move forward. This paper
                defines this notion of corrective explanation, and proposes an
                algorithm to generate such explanations. The approach is shown
                to perform well on both real-world configuration benchmarks and
                randomly generated problems.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "445--459",
  series     = "Lecture Notes in Computer Science",
  month      =  oct,
  year       =  2005,
  keywords   = "meh..;Assurances",
  language   = "en",
  conference = "International Conference on Principles and Practice of
                Constraint Programming"
}

@INPROCEEDINGS{Wallace2001-fm,
  title     = "Explanations for whom",
  booktitle = "{CP01} Workshop on {User-Interaction} in Constraint Satisfaction",
  author    = "Wallace, Richard J and Freuder, Eugene C",
  year      =  2001,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;explain;Supplemental
               Assurance;Reduce Complexity;Assurances"
}

@ARTICLE{Hadfield-Menell2016-ws,
  title         = "The {Off-Switch} Game",
  author        = "Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter
                   and Russell, Stuart",
  abstract      = "It is clear that one of the primary tools we can use to
                   mitigate the potential risk from a misbehaving AI system is
                   the ability to turn the system off. As the capabilities of
                   AI systems improve, it is important to ensure that such
                   systems do not adopt subgoals that prevent a human from
                   switching them off. This is a challenge because many
                   formulations of rational agents create strong incentives for
                   self-preservation. This is not caused by a built-in
                   instinct, but because a rational agent will maximize
                   expected utility and cannot achieve whatever objective it
                   has been given if it is dead. Our goal is to study the
                   incentives an agent has to allow itself to be switched off.
                   We analyze a simple game between a human H and a robot R,
                   where H can press R's off switch but R can disable the off
                   switch. A traditional agent takes its reward function for
                   granted: we show that such agents have an incentive to
                   disable the off switch, except in the special case where H
                   is perfectly rational. Our key insight is that for R to want
                   to preserve its off switch, it needs to be uncertain about
                   the utility associated with the outcome, and to treat H's
                   actions as important observations about that utility. (R
                   also has no incentive to switch itself off in this setting.)
                   We conclude that giving machines an appropriate level of
                   uncertainty about their objectives leads to safer designs,
                   and we argue that this setting is a useful generalization of
                   the classical AI paradigm of rational agents.",
  month         =  nov,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_implicit;V\&V;in\_paper;Integral
                   Assurance;Value Alignment;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1611.08219"
}

@ARTICLE{Leyton-Brown2009-yr,
  title     = "Empirical Hardness Models: Methodology and a Case Study on
               Combinatorial Auctions",
  author    = "Leyton-Brown, Kevin and Nudelman, Eugene and Shoham, Yoav",
  journal   = "J. ACM",
  publisher = "ACM",
  volume    =  56,
  number    =  4,
  pages     = "22:1--22:52",
  month     =  jul,
  year      =  2009,
  address   = "New York, NY, USA",
  keywords  = "Empirical analysis of algorithms, algorithm portfolios,
               combinatorial auctions, runtime
               prediction;trust\_informal\_treatment;assurance\_implicit;empirical\_performance;in\_presentation;Assurances"
}

@MASTERSTHESIS{Aitken2016-cv,
  title    = "Assured {Human-Autonomy} Interaction through Machine
              {Self-Confidence}",
  author   = "Aitken, Matthew",
  year     =  2016,
  school   = "University of Colorado at Boulder",
  keywords = "
              assurance\_explicit;trust\_formal\_treatment;in\_paper;Supplemental
              Assurance;Quantify Uncertainty;Reduce Complexity;Assurances"
}

@BOOK{Back1996-jp,
  title     = "Evolutionary Algorithms in Theory and Practice: Evolution
               Strategies, Evolutionary Programming, Genetic Algorithms",
  author    = "Back, Thomas",
  abstract  = "This book presents a unified view of evolutionary algorithms:
               the exciting new probabilistic search tools inspired by
               biological models that have immense potential as practical
               problem-solvers in a wide variety of settings, academic,
               commercial, and industrial. In this work, the author compares
               the three most prominent representatives of evolutionary
               algorithms: genetic algorithms, evolution strategies, and
               evolutionary programming. The algorithms are presented within a
               unified framework, thereby clarifying the similarities and
               differences of these methods. The author also presents new
               results regarding the role of mutation and selection in genetic
               algorithms, showing how mutation seems to be much more important
               for the performance of genetic algorithms than usually assumed.
               The interaction of selection and mutation, and the impact of the
               binary code are further topics of interest. Some of the
               theoretical results are also confirmed by performing an
               experiment in meta-evolution on a parallel computer. The
               meta-algorithm used in this experiment combines components from
               evolution strategies and genetic algorithms to yield a hybrid
               capable of handling mixed integer optimization problems. As a
               detailed description of the algorithms, with practical
               guidelines for usage and implementation, this work will interest
               a wide range of researchers in computer science and engineering
               disciplines, as well as graduate students in these fields.",
  publisher = "Oxford University Press",
  month     =  jan,
  year      =  1996,
  keywords  = "BayesOpt",
  language  = "en"
}

@ARTICLE{Tran2015-lq,
  title         = "The Variational Gaussian Process",
  author        = "Tran, Dustin and Ranganath, Rajesh and Blei, David M",
  abstract      = "Variational inference is a powerful tool for approximate
                   inference, and it has been recently applied for
                   representation learning with deep generative models. We
                   develop the variational Gaussian process (VGP), a Bayesian
                   nonparametric variational family, which adapts its shape to
                   match complex posterior distributions. The VGP generates
                   approximate posterior samples by generating latent inputs
                   and warping them through random non-linear mappings; the
                   distribution over random mappings is learned during
                   inference, enabling the transformed outputs to adapt to
                   varying complexity. We prove a universal approximation
                   theorem for the VGP, demonstrating its representative power
                   for learning any model. For inference we present a
                   variational objective inspired by auto-encoders and perform
                   black box inference over a wide class of models. The VGP
                   achieves new state-of-the-art results for unsupervised
                   learning, inferring models such as the deep latent Gaussian
                   model and the recently proposed DRAW.",
  month         =  nov,
  year          =  2015,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.06499"
}

@ARTICLE{Theis2015-tx,
  title         = "A note on the evaluation of generative models",
  author        = "Theis, Lucas and van den Oord, A{\"a}ron and Bethge,
                   Matthias",
  abstract      = "Probabilistic generative models can be used for compression,
                   denoising, inpainting, texture synthesis, semi-supervised
                   learning, unsupervised feature learning, and other tasks.
                   Given this wide range of applications, it is not surprising
                   that a lot of heterogeneity exists in the way these models
                   are formulated, trained, and evaluated. As a consequence,
                   direct comparison between models is often difficult. This
                   article reviews mostly known but often underappreciated
                   properties relating to the evaluation and interpretation of
                   generative models with a focus on image models. In
                   particular, we show that three of the currently most
                   commonly used criteria---average log-likelihood, Parzen
                   window estimates, and visual fidelity of samples---are
                   largely independent of each other when the data is
                   high-dimensional. Good performance with respect to one
                   criterion therefore need not imply good performance with
                   respect to the other criteria. Our results show that
                   extrapolation from one criterion to another is not warranted
                   and generative models need to be evaluated directly with
                   respect to the application(s) they were intended for. In
                   addition, we provide examples demonstrating that Parzen
                   window estimates should generally be avoided.",
  month         =  nov,
  year          =  2015,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.01844"
}

@ARTICLE{Bakry2015-td,
  title         = "Digging Deep into the layers of {CNNs}: In Search of How
                   {CNNs} Achieve View Invariance",
  author        = "Bakry, Amr and Elhoseiny, Mohamed and El-Gaaly, Tarek and
                   Elgammal, Ahmed",
  abstract      = "This paper is focused on studying the view-manifold
                   structure in the feature spaces implied by the different
                   layers of Convolutional Neural Networks (CNN). There are
                   several questions that this paper aims to answer: Does the
                   learned CNN representation achieve viewpoint invariance? How
                   does it achieve viewpoint invariance? Is it achieved by
                   collapsing the view manifolds, or separating them while
                   preserving them? At which layer is view invariance achieved?
                   How can the structure of the view manifold at each layer of
                   a deep convolutional neural network be quantified
                   experimentally? How does fine-tuning of a pre-trained CNN on
                   a multi-view dataset affect the representation at each layer
                   of the network? In order to answer these questions we
                   propose a methodology to quantify the deformation and
                   degeneracy of view manifolds in CNN layers. We apply this
                   methodology and report interesting results in this paper
                   that answer the aforementioned questions.",
  month         =  aug,
  year          =  2015,
  keywords      = "NotRead;assurance\_predictability;interpretability;trust\_informal\_treatment;assurance\_implicit;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1508.01983"
}

@ARTICLE{Bashivan2015-fc,
  title         = "Learning Representations from {EEG} with Deep
                   {Recurrent-Convolutional} Neural Networks",
  author        = "Bashivan, Pouya and Rish, Irina and Yeasin, Mohammed and
                   Codella, Noel",
  abstract      = "One of the challenges in modeling cognitive events from
                   electroencephalogram (EEG) data is finding representations
                   that are invariant to inter- and intra-subject differences,
                   as well as to inherent noise associated with such data.
                   Herein, we propose a novel approach for learning such
                   representations from multi-channel EEG time-series, and
                   demonstrate its advantages in the context of mental load
                   classification task. First, we transform EEG activities into
                   a sequence of topology-preserving multi-spectral images, as
                   opposed to standard EEG analysis techniques that ignore such
                   spatial information. Next, we train a deep
                   recurrent-convolutional network inspired by state-of-the-art
                   video classification to learn robust representations from
                   the sequence of images. The proposed approach is designed to
                   preserve the spatial, spectral, and temporal structure of
                   EEG which leads to finding features that are less sensitive
                   to variations and distortions within each dimension.
                   Empirical evaluation on the cognitive load classification
                   task demonstrated significant improvements in classification
                   accuracy over current state-of-the-art approaches in this
                   field.",
  month         =  nov,
  year          =  2015,
  keywords      = "NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1511.06448"
}

@ARTICLE{Bengio2013-uv,
  title    = "Representation learning: a review and new perspectives",
  author   = "Bengio, Yoshua and Courville, Aaron and Vincent, Pascal",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical
              connections between representation learning, density estimation,
              and manifold learning.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  month    =  aug,
  year     =  2013,
  keywords = "
              trust\_informal\_treatment;assurance\_implicit;rep\_learn;Integral
              Assurance;Value Alignment;Assurances",
  language = "en"
}

@INPROCEEDINGS{Weng_Wong2014-tj,
  title      = "Correct High-level Robot Behavior in Environments with
                Unexpected Events",
  booktitle  = "Robotics: Science and Systems {X}",
  author     = "Weng Wong, Kai and Ehlers, R{\"u}diger and Kress-Gazit, Hadas",
  publisher  = "Robotics: Science and Systems Foundation",
  month      =  jul,
  year       =  2014,
  keywords   = "trust\_informal\_treatment;assurance\_implicit;V\&V;in\_paper;Assurances",
  conference = "Robotics: Science and Systems 2014"
}

@INPROCEEDINGS{Conner2007-uw,
  title           = "Valet parking without a valet",
  booktitle       = "2007 {IEEE/RSJ} International Conference on Intelligent
                     Robots and Systems",
  author          = "Conner, D C and Kress-Gazit, H and Choset, H and Rizzi, A
                     A and Pappas, G J",
  abstract        = "What would it be like if we could give our robot high
                     level commands and it would automatically execute them in
                     a verifiably correct fashion in dynamically changing
                     environments? This work demonstrates a method for
                     generating continuous feedback control inputs that satisfy
                     high-level specifications. Using a collection of
                     continuous local feedback control policies in concert with
                     a synthesized discrete automaton, this paper demonstrates
                     the approach on an Ackermann-steered vehicle that
                     satisfies the command ``drive around until you find an
                     empty parking space, then park.'' The system reacts to
                     changing environmental conditions using only local
                     information, while guaranteeing the correct high level
                     behavior. The local policies consider the vehicle body
                     shape as well as bounds on drive and steering velocities.
                     The discrete automaton that invokes the local policies
                     guarantees executions that satisfy the high-level
                     specification based only on information about the current
                     availability of the nearest parking space. This paper also
                     demonstrates coordination of two vehicles using the
                     approach.",
  publisher       = "IEEE",
  pages           = "572--577",
  month           =  oct,
  year            =  2007,
  keywords        = "continuous systems;feedback;intelligent robots;mobile
                     robots;service robots;steering systems;velocity
                     control;Ackermann-steered vehicle;continuous local
                     feedback control;discrete automaton;drive
                     velocity;high-level specification;parking
                     space;robot;steering velocity;valet parking;vehicle body
                     shape;Automata;Automatic control;Feedback
                     control;Intelligent robots;Orbital robotics;Robot
                     kinematics;Robotics and automation;Shape;Space
                     vehicles;Vehicle
                     driving;V\&V;trust\_informal\_treatment;assurance\_implicit;in\_paper;Assurances",
  conference      = "2007 IEEE/RSJ International Conference on Intelligent
                     Robots and Systems"
}

@INPROCEEDINGS{Israelsen2014-um,
  title     = "Generalized Laguerre Reduction of the Volterra Kernel for
               Practical Identification of Nonlinear Dynamic Systems",
  booktitle = "{AIChE} Spring Meeting 2014",
  author    = "Israelsen, Brett W and Smith, Dale A",
  abstract  = "Abstract: The Volterra series can be used to model a large
               subset of nonlinear, dynamic systems. A major drawback is the
               number of coefficients required model such systems. In order to
               reduce the number of required coefficients, Laguerre polynomials
               are used to estimate the Volterra kernels. Existing literature
               proposes algorithms for a fixed number of Volterra kernels, and
               Laguerre series. This paper presents a novel algorithm for
               generalized calculation of the finite order Volterra-Laguerre
               (VL) series for a MIMO system. An ...",
  pages     = "Paper--349348",
  year      =  2014,
  address   = "New Orleans, LA",
  keywords  = "laguerre; model reduction; statistical learning; system
               identification; volterra;My Papers;myPapers"
}

@INPROCEEDINGS{Pina2008-gr,
  title     = "Identifying generalizable metric classes to evaluate human-robot
               teams",
  booktitle = "Proc. 3rd Ann. Conf. {Human-Robot} Interaction",
  author    = "Pina, P and Cummings, M L and Crandall, J W and Della Penna, M",
  abstract  = "ABSTRACT In this paper, we describe an effort to identify
               generalizable metric classes to evaluate human-robot teams. We
               describe conceptual models for supervisory control of a single
               and multiple robots. Based on these models, we identify and
               discuss the main metric classes that must be taken into
               consideration to understand team performance. Finally, we
               discuss a case study of a search and rescue mission to
               illustrate the use of these metric ...",
  publisher = "academia.edu",
  pages     = "13--20",
  year      =  2008,
  keywords  = "NotRead"
}

@ARTICLE{Konolige1985-vx,
  title     = "A Computational Theory of Belief Introspection",
  author    = "Konolige, K",
  abstract  = "... the finite base set is decidable. 3 Comparison to Related
               Work Our definition of an ideal introspective agent has many
               points of similarity with work by Halpern and Moses [2] and
               Moore [12]. In both these latter cases an underlying ...",
  journal   = "IJCAI",
  publisher = "pdfs.semanticscholar.org",
  year      =  1985,
  keywords  = "introspection;trust\_informal\_treatment;assurance\_implicit;Assurances"
}

@ARTICLE{Charif2013-vo,
  title     = "Dynamic service composition enabled by introspective agent
               coordination",
  author    = "Charif, Y and Sabouret, N",
  abstract  = "Abstract Service composition has received much interest from
               many research communities. The major research efforts published
               to date propose the use of service orchestration to model this
               problem. However, the designed orchestration approaches are
               static since they",
  journal   = "Auton. Agent. Multi. Agent. Syst.",
  publisher = "Springer",
  year      =  2013,
  keywords  = "introspection;trust\_informal\_treatment;assurance\_implicit;Assurances"
}

@ARTICLE{Yilmaz2007-ff,
  title     = "A strategy for improving dynamic composability: ontology-driven
               introspective agent architectures",
  author    = "Yilmaz, Levent",
  abstract  = "ABSTRACT Seamless composability of disparate simulations within
               systems of systems context is challenging. Large complex
               simulations must respond to changing technology, environments,
               and objectives. The problem exacerbates when dynamic
               extensibility and",
  journal   = "Journal of Systemics, Cybernetics and Informatics",
  publisher = "iiisci.org",
  volume    =  5,
  number    =  5,
  pages     = "1--9",
  year      =  2007,
  keywords  = "introspection;Assurances/Self-Aware"
}

@ARTICLE{Israelsen2016-wf,
  title         = "Towards Adaptive Training of Agent-based Sparring Partners
                   for Fighter Pilots",
  author        = "Israelsen, Brett W and Ahmed, Nisar and Center, Kenneth and
                   Green, Roderick and Bennett, Jr, Winston",
  abstract      = "A key requirement for the current generation of artificial
                   decision-makers is that they should adapt well to changes in
                   unexpected situations. This paper addresses the situation in
                   which an AI for aerial dog fighting, with tunable parameters
                   that govern its behavior, must optimize behavior with
                   respect to an objective function that is evaluated and
                   learned through simulations. Bayesian optimization with a
                   Gaussian Process surrogate is used as the method for
                   investigating the objective function. One key benefit is
                   that during optimization, the Gaussian Process learns a
                   global estimate of the true objective function, with
                   predicted outcomes and a statistical measure of confidence
                   in areas that haven't been investigated yet. Having a model
                   of the objective function is important for being able to
                   understand possible outcomes in the decision space; for
                   example this is crucial for training and providing feedback
                   to human pilots. However, standard Bayesian optimization
                   does not perform consistently or provide an accurate
                   Gaussian Process surrogate function for highly volatile
                   objective functions. We treat these problems by introducing
                   a novel sampling technique called Hybrid Repeat/Multi-point
                   Sampling. This technique gives the AI ability to learn
                   optimum behaviors in a highly uncertain environment. More
                   importantly, it not only improves the reliability of the
                   optimization, but also creates a better model of the entire
                   objective surface. With this improved model the agent is
                   equipped to more accurately/efficiently predict performance
                   in unexplored scenarios.",
  month         =  dec,
  year          =  2016,
  keywords      = "myPapers",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1612.04315"
}

@INPROCEEDINGS{Israelsen2016-ea,
  title      = "Hybrid {Repeat/Multi-point} Sampling for Highly Volatile
                Objective Functions",
  booktitle  = "{NIPS} Workshop on Bayesian Optimization",
  author     = "Israelsen, Brett and Ahmed, Nisar",
  abstract   = "A key drawback of the current generation of artificial
                decision-makers is that they do not adapt well to changes in
                unexpected situations. This paper addresses the situation in
                which an AI for aerial dog fighting, with tunable parameters
                that govern its behavior, will optimize behavior with respect
                to an objective function that must be evaluated and learned
                through simulations. Once this objective function has been
                modeled, the agent can then choose its desired behavior in
                different situations. Bayesian optimization with a Gaussian
                Process surrogate is used as the method for investigating the
                objective function. One key benefit is that during optimization
                the Gaussian Process learns a global estimate of the true
                objective function, with predicted outcomes and a statistical
                measure of confidence in areas that haven't been investigated
                yet. However, standard Bayesian optimization does not perform
                consistently or provide an accurate Gaussian Process surrogate
                function for highly volatile objective functions. We treat
                these problems by introducing a novel sampling technique called
                Hybrid Repeat/Multi-point Sampling. This technique gives the AI
                ability to learn optimum behaviors in a highly uncertain
                environment. More importantly, it not only improves the
                reliability of the optimization, but also creates a better
                model of the entire objective surface. With this improved model
                the agent is equipped to better adapt behaviors.",
  month      =  dec,
  year       =  2016,
  keywords   = "My Papers;myPapers",
  conference = "Neural Information Processing Systems"
}

@ARTICLE{Nguyen2014-ra,
  title         = "Deep Neural Networks are Easily Fooled: High Confidence
                   Predictions for Unrecognizable Images",
  author        = "Nguyen, Anh and Yosinski, Jason and Clune, Jeff",
  abstract      = "Deep neural networks (DNNs) have recently been achieving
                   state-of-the-art performance on a variety of
                   pattern-recognition tasks, most notably visual
                   classification problems. Given that DNNs are now able to
                   classify objects in images with near-human-level
                   performance, questions naturally arise as to what
                   differences remain between computer and human vision. A
                   recent study revealed that changing an image (e.g. of a
                   lion) in a way imperceptible to humans can cause a DNN to
                   label the image as something else entirely (e.g. mislabeling
                   a lion a library). Here we show a related result: it is easy
                   to produce images that are completely unrecognizable to
                   humans, but that state-of-the-art DNNs believe to be
                   recognizable objects with 99.99\% confidence (e.g. labeling
                   with certainty that white noise static is a lion).
                   Specifically, we take convolutional neural networks trained
                   to perform well on either the ImageNet or MNIST datasets and
                   then find images with evolutionary algorithms or gradient
                   ascent that DNNs label with high confidence as belonging to
                   each dataset class. It is possible to produce images totally
                   unrecognizable to human eyes that DNNs believe with near
                   certainty are familiar objects, which we call ``fooling
                   images'' (more generally, fooling examples). Our results
                   shed light on interesting differences between human vision
                   and current DNNs, and raise questions about the generality
                   of DNN computer vision.",
  month         =  dec,
  year          =  2014,
  keywords      = "MLTheory/DeepLearning",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1412.1897"
}

@ARTICLE{Park1991-ow,
  title    = "Universal Approximation Using {Radial-Basis-Function} Networks",
  author   = "Park, J and Sandberg, I W",
  abstract = "There have been several recent studies concerning feedforward
              networks and the problem of approximating arbitrary functionals
              of a finite number of real variables. Some of these studies deal
              with cases in which the hidden-layer nonlinearity is not a
              sigmoid. This was motivated by successful applications of
              feedforward networks with nonsigmoidal hidden-layer units. This
              paper reports on a related study of radial-basis-function (RBF)
              networks, and it is proved that RBF networks having one hidden
              layer are capable of universal approximation. Here the emphasis
              is on the case of typical RBF networks, and the results show that
              a certain class of RBF networks with the same smoothing factor in
              each kernel node is broad enough for universal approximation.",
  journal  = "Neural Comput.",
  volume   =  3,
  number   =  2,
  pages    = "246--257",
  year     =  1991,
  keywords = "MLTheory/DeepLearning"
}

@ARTICLE{Dietterich1999-rm,
  title         = "Hierarchical Reinforcement Learning with the {MAXQ} Value
                   Function Decomposition",
  author        = "Dietterich, Thomas G",
  abstract      = "This paper presents the MAXQ approach to hierarchical
                   reinforcement learning based on decomposing the target
                   Markov decision process (MDP) into a hierarchy of smaller
                   MDPs and decomposing the value function of the target MDP
                   into an additive combination of the value functions of the
                   smaller MDPs. The paper defines the MAXQ hierarchy, proves
                   formal results on its representational power, and
                   establishes five conditions for the safe use of state
                   abstractions. The paper presents an online model-free
                   learning algorithm, MAXQ-Q, and proves that it converges wih
                   probability 1 to a kind of locally-optimal policy known as a
                   recursively optimal policy, even in the presence of the five
                   kinds of state abstraction. The paper evaluates the MAXQ
                   representation and MAXQ-Q through a series of experiments in
                   three domains and shows experimentally that MAXQ-Q (with
                   state abstractions) converges to a recursively optimal
                   policy much faster than flat Q learning. The fact that MAXQ
                   learns a representation of the value function has an
                   important benefit: it makes it possible to compute and
                   execute an improved, non-hierarchical policy via a procedure
                   similar to the policy improvement step of policy iteration.
                   The paper demonstrates the effectiveness of this
                   non-hierarchical execution experimentally. Finally, the
                   paper concludes with a comparison to related work and a
                   discussion of the design tradeoffs in hierarchical
                   reinforcement learning.",
  pages         = "227--303",
  month         =  may,
  year          =  1999,
  keywords      = "TALAF;AFRL\_STTR",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "cs/9905014"
}

@ARTICLE{Walter2015-tl,
  title    = "A Situationally Aware Voice-commandable Robotic Forklift Working
              Alongside People in Unstructured Outdoor Environments",
  author   = "Walter, Matthew R and Antone, Matthew and Chuangsuwanich, Ekapol
              and Correa, Andrew and Davis, Randall and Fletcher, Luke and
              Frazzoli, Emilio and Friedman, Yuli and Glass, James and How,
              Jonathan P and Jeon, Jeong Hwan and Karaman, Sertac and Luders,
              Brandon and Roy, Nicholas and Tellex, Stefanie and Teller, Seth",
  abstract = "One long-standing challenge in robotics is the realization of
              mobile autonomous robots able to operate safely in human
              workplaces, and be accepted by the human occupants. We describe
              the development of a multiton robotic forklift intended to
              operate alongside people and vehicles, handling palletized
              materials within existing, active outdoor storage facilities. The
              system has four novel characteristics. The first is a multimodal
              interface that allows users to efficiently convey task-level
              commands to the robot using a combination of pen-based gestures
              and natural language speech. These tasks include the
              manipulation, transport, and placement of palletized cargo within
              dynamic, human-occupied warehouses. The second is the robot's
              ability to learn the visual identity of an object from a single
              user-provided example and use the learned model to reliably and
              persistently detect objects despite significant spatial and
              temporal excursions. The third is a reliance on local sensing
              that allows the robot to handle variable palletized cargo and
              navigate within dynamic, minimally prepared environments without
              a global positioning system. The fourth concerns the robot's
              operation in close proximity to people, including its human
              supervisor, pedestrians who may cross or block its path, moving
              vehicles, and forklift operators who may climb inside the robot
              and operate it manually. This is made possible by interaction
              mechanisms that facilitate safe, effective operation around
              people. This paper provides a comprehensive description of the
              system's architecture and implementation, indicating how
              real-world operational requirements motivated key design choices.
              We offer qualitative and quantitative analyses of the robot
              operating in real settings and discuss the lessons learned from
              our effort.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  4,
  pages    = "590--628",
  month    =  jun,
  year     =  2015,
  keywords = "Human-RobotCollaboration",
  language = "en"
}

@ARTICLE{DeDonato2015-rp,
  title    = "Human-in-the-loop Control of a Humanoid Robot for Disaster
              Response: A Report from the {DARPA} Robotics Challenge Trials:
              Human-in-the-loop Control of a Humanoid Robot for Disaster
              Response",
  author   = "DeDonato, Mathew and Dimitrov, Velin and Du, Ruixiang and
              Giovacchini, Ryan and Knoedler, Kevin and Long, Xianchao and
              Polido, Felipe and Gennert, Michael A and Pad{\i}r, Ta{\c
              s}k{\i}n and Feng, Siyuan and Moriguchi, Hirotaka and Whitman,
              Eric and Xinjilefu, X and Atkeson, Christopher G",
  abstract = "The DARPA Robotics Challenge (DRC) requires teams to integrate
              mobility, manipulation, and perception to accomplish several
              disaster?response tasks. We describe our hardware choices and
              software architecture, which enable human?in?the?loop control of
              a 28 degree?of?freedom Atlas humanoid robot over a limited
              bandwidth link. We discuss our methods, results, and lessons
              learned for the DRC Trials tasks. The effectiveness of our system
              architecture was demonstrated as the WPI?CMU DRC Team scored 11
              out of a possible 32 points, ranked seventh (out of 16) at the
              DRC Trials, and was selected as a finalist for the DRC Finals.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  2,
  pages    = "275--292",
  month    =  mar,
  year     =  2015,
  keywords = "Human-RobotCollaboration",
  language = "en"
}

@INPROCEEDINGS{Ahmed2010-mb,
  title     = "Variational Bayesian data fusion of multi-class discrete
               observations with applications to cooperative human-robot
               estimation",
  booktitle = "2010 {IEEE} International Conference on Robotics and Automation",
  author    = "Ahmed, N and Campbell, M",
  abstract  = "A new method is presented for fusing conventional continuous
               sensor observations with discrete multi-categorical
               state-dependent information, which can be furnished by humans in
               many cooperative human-robot interaction problems. The hybrid
               likelihood function for mapping between continuous hidden states
               and categorical observations are specified via softmax models.
               Although softmax models avoid discretization of continuous
               states, they are challenging to implement for real-time data
               fusion since they are not analytically integrable. An
               approximation based on variational Bayesian (VB) methods is
               presented here to obtain fast closed-form Gaussian solutions to
               the desired posteriors in cases where the hidden continuous
               states have Gaussian pdfs. A joint human-robot target
               localization example illustrates the properties and utility of
               the VB hybrid fusion strategy, which also applies more generally
               to inference in hybrid Bayesian networks and mixture models.",
  publisher = "IEEE",
  pages     = "186--191",
  month     =  may,
  year      =  2010,
  keywords  = "Bayes methods;Gaussian processes;human-robot interaction;sensor
               fusion;variational techniques;closed-form Gaussian
               solution;continuous sensor observations;cooperative human-robot
               estimation;cooperative human-robot interaction problem;discrete
               multicategorical state-dependent information;hybrid Bayesian
               network;hybrid likelihood function;joint human-robot target
               localization;mixture model;multiclass discrete
               observations;softmax model;variational Bayesian data
               fusion;Bayesian methods;Humanoid robots;Humans;Orbital
               robotics;Position measurement;Robot kinematics;Robot sensing
               systems;Robotics and automation;State estimation;USA
               Councils;Human-RobotCollaboration"
}

@ARTICLE{Sklar2013-le,
  title    = "A Case for Argumentation to Enable {Human-Robot} Collaboration",
  author   = "Sklar, Elizabeth and Azhar, Mq and Flyr, Todd and Parsons, Simon",
  journal  = "Workshop on Argumentation for Multiagent Systems (ArgMAS)",
  pages    = "1--17",
  year     =  2013,
  keywords = "Human-RobotCollaboration"
}

@ARTICLE{Murphy2008-ty,
  title     = "Cooperative use of unmanned sea surface and micro aerial
               vehicles at Hurricane Wilma",
  author    = "Murphy, Robin R and Steimle, Eric and Griffin, Chandler and
               Cullins, Charlie and Hall, Mike and Pratt, Kevin",
  abstract  = "On Oct. 24, 2005, Hurricane Wilma, a category 5 storm, made
               landfall at Cape Romano, Florida. Three days later, the Center
               for Robot-Assisted Search and Rescue at the University of South
               Florida deployed an iSENYS helicopter and a prototype unmanned
               water surface vehicle, AEOS-1, to survey damage in parts of
               Marco Island, 14 km from landfall. The effort was the first
               known use of unmanned sea surface vehicles (USVs) for emergency
               response and established their suitability for the recovery
               phase of disaster management by detecting damage to seawalls and
               piers, locating submerged debris (moorings and handrails), and
               determining safe lanes for sea navigation. It provides a
               preliminary domain theory of postdisaster port and littoral
               inspection with unmanned vehicles for use by the human--robot
               interaction community. It was also the first known demonstration
               of the strongly heterogeneous USV--micro aerial vehicle (MAV)
               team for any domain. The effort identified cooperative UAV--USV
               strategies and open issues for autonomous operations near
               structures. The effort showed that the MAV provided a
               much-needed external view for situation awareness and provided
               spotting for areas to be inspected. Concepts of operations for
               USV damage inspection and USV--MAV cooperation emerged,
               including a formula for computing the human--robot ratio: Nh =
               (2 $\times$ Nv) + 1, where Nh is the number of humans and Nv is
               the number of vehicles. The outstanding research issues span
               three areas: challenges for USVs operating near littoral
               structures, general issues for USV--MAV cooperation, and new
               applications. It is expected that the lessons learned will be
               transferrable to defense and homeland safety and security
               applications, such as port security, and other phases of
               emergency response, including rescue. \copyright{} 2008 Wiley
               Periodicals, Inc.",
  journal   = "J. Field Robotics",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  25,
  number    =  3,
  pages     = "164--180",
  month     =  mar,
  year      =  2008,
  keywords  = "Human-RobotCollaboration",
  language  = "en"
}

@PHDTHESIS{Ahmed2012-yn,
  title    = "Probabilistic modeling and estimation with human inputs in
              semi-autonomous systems",
  author   = "Ahmed, Nisar Razzi",
  abstract = "This thesis addresses three important issues that arise in the
              analysis and design of joint human-robot teams. Each issue deals
              with a different aspect of the following question: how to best
              combine human and robot capabilities to accomplish some set of
              tasks? The first issue addressed here is that of predicting human
              supervisory control performance in large-scale networked teams of
              robots. It is shown that models based on individual operator
              characteristics such as working memory capacity can be used to
              probabilistically predict human supervisory control metrics under
              different operating conditions via linear regression, Bayesian
              network, and Gaussian process models. The second issue addressed
              here is that of modeling human supervisors of multi-robot teams
              as discrete strategic decision makers. A probabilistic
              discriminative modeling approach is presented here, and novel
              fully Bayesian learning techniques are presented and validated
              for identifying appropriate discriminative model parameters and
              model structures from experimental data. The third issue
              addressed here is that of combining useful information from human
              observations with information obtained from traditional robot
              sensors. A novel recursive Bayesian estimation framework is
              presented for fusing imprecise soft categorical human
              observations with robot sensor data via Gaussian and Gaussian
              mixture approximations. The proposed data fusion approach is
              validated in hardware with a real human-robot team on a
              cooperative multi-target search experiment.",
  year     =  2012,
  school   = "Cornell University",
  keywords = "0537:Engineering; 0554:Civil engineering; 0771:Robotics; Applied
              sciences; Civil engineering; Control systems; Engineering; Human
              inputs; Human-robot interactions; Machine learning; Probabilistic
              estimation; Probabilistic modeling; Robotics; Semi-autonomous
              systems;Human-RobotCollaboration"
}

@INPROCEEDINGS{Kaupp2005-pk,
  title     = "Human sensor model for range observations",
  booktitle = "{IJCAI} Workshop Reasoning with Uncertainty in Robotics",
  author    = "Kaupp, Tobias and Makarenko, Alexei and Ramos, Fabio and
               Durrant-Whyte, Hugh",
  year      =  2005,
  keywords  = "Integral Assurance;User Interaction;Human-RobotCollaboration"
}

@ARTICLE{Kubelka2015-fv,
  title    = "Robust Data Fusion of Multimodal Sensory Information for Mobile
              Robots",
  author   = "Kubelka, Vladim{\'\i}r and Oswald, Lorenz and Pomerleau, Fran{\c
              c}ois and Colas, Francis and Svoboda, Tom{\'a}{\v s} and
              Reinstein, Michal",
  abstract = "Urban search and rescue (USAR) missions for mobile robots require
              reliable state estimation systems resilient to conditions given
              by the dynamically changing environment. We design and evaluate a
              data fusion system for localization of a mobile skid-steer robot
              intended for USAR missions. We exploit a rich sensor suite
              including both proprioceptive (inertial measurement unit and
              tracks odometry) and exteroceptive sensors (omnidirectional
              camera and rotating laser rangefinder). To cope with the
              specificities of each sensing modality (such as significantly
              differing sampling frequencies), we introduce a novel fusion
              scheme based on an extended Kalman filter for six degree of
              freedom orientation and position estimation. We demonstrate the
              performance on field tests of more than 4.4 km driven under
              standard USAR conditions. Part of our datasets include ground
              truth positioning, indoor with a Vicon motion capture system and
              outdoor with a Leica theodolite tracker. The overall median
              accuracy of localization---achieved by combining all four
              modalities---was 1.2\% and 1.4\% of the total distance traveled
              for indoor and outdoor environments, respectively. To identify
              the true limits of the proposed data fusion, we propose and
              employ a novel experimental evaluation procedure based on failure
              case scenarios. In this way, we address the common issues such as
              slippage, reduced camera field of view, and limited laser
              rangefinder range, together with moving obstacles spoiling the
              metric map. We believe such a characterization of the failure
              cases is a first step toward identifying the behavior of state
              estimation under such conditions. We release all our datasets to
              the robotics community for possible benchmarking.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  4,
  pages    = "447--473",
  month    =  jun,
  year     =  2015,
  keywords = "Human-RobotCollaboration",
  language = "en"
}

@ARTICLE{Geber1975-zy,
  title     = "Congenital malformations of the central nervous system produced
               by narcotic analgesics in the hamster",
  author    = "Geber, W F and Schramm, L C",
  abstract  = "Maternal dose--fetal teratogenic response data were obtained for
               a variety of narcotic and related compounds by single
               subcutaneous injections of the drugs into pregnant hamsters
               during the critical periods of central nervous system
               organogenesis. The number of abnormal fetuses from females
               injected with diacetylmorphine (heroin), thebaine, phenazocine,
               pentazocine, propoxyphene, and methadone increased as the
               maternal dose of the compounds was increased. By contrast,
               morphine, hydromorphone, and meperidine produced an increase in
               the number (per cent) of fetal anomalies only up to a certain
               maternal dose level. Further increases in maternal dose levels
               did not produce additional fetal anomalies. Comparative studies
               of single and multiple maternal doses indicated that
               diacetylmorphine (heroin) and methadone produced a four- to
               sixfold increase in fetal anomalies with repetitive doses
               whereas the percentage of malformed fetuses remained the same
               with hydromorphone (Dilaudid). The narcotic antagonists
               nalorphine, naloxone, levallophan, and cyclazocine blocked the
               teratogenic effects of both single and multiple doses of the
               narcotics.",
  journal   = "Am. J. Obstet. Gynecol.",
  publisher = "Citeseer",
  volume    =  123,
  number    =  7,
  pages     = "705--713",
  month     =  dec,
  year      =  1975,
  keywords  = "introduction to graphical;GraphicalModels",
  language  = "en"
}

@BOOK{Barber2012-ia,
  title     = "Bayesian Reasoning and Machine Learning",
  author    = "Barber, David",
  abstract  = "Machine learning methods extract value from vast data sets
               quickly and with modest resources. They are established tools in
               a wide range of industrial applications, including search
               engines, DNA sequencing, stock market analysis, and robot
               locomotion, and their use is spreading rapidly. People who know
               the methods have their choice of rewarding jobs. This hands-on
               text opens these opportunities to computer science students with
               modest mathematical backgrounds. It is designed for final-year
               undergraduates and master's students with limited background in
               linear algebra and calculus. Comprehensive and coherent, it
               develops everything from basic reasoning to advanced techniques
               within the framework of graphical models. Students learn more
               than a menu of techniques, they develop analytical and
               problem-solving skills that equip them for the real world.
               Numerous examples and exercises, both computer based and
               theoretical, are included in every chapter. Resources for
               students and instructors, including a MATLAB toolbox, are
               available online.",
  publisher = "Cambridge University Press",
  pages     = "646",
  month     =  feb,
  year      =  2012,
  keywords  = "computational; information theoretic learning with statistics;
               learning; statistics \& optimisation; theory \&
               algorithms;classification;interpretability;time
               series;Textbook;TextBooks;GraphicalModels;Assurances",
  language  = "en"
}

@ARTICLE{Ahmed2008-lu,
  title     = "Bayesian Networks and Decision Graphs",
  author    = "Ahmed, S E",
  journal   = "Technometrics",
  publisher = "Springer",
  volume    =  50,
  number    =  1,
  pages     = "97--97",
  series    = "Information science and statistics",
  year      =  2008,
  address   = "New York",
  keywords  = "Textbook;TextBooks;GraphicalModels"
}

@INPROCEEDINGS{Ahmed2015-pr,
  title     = "Bayesian hidden Markov models for {UAV-enabled} target
               localization on road networks with soft-hard data",
  booktitle = "{SPIE} Defense + Security",
  author    = "Ahmed, Nisar and Casbeer, David and Cao, Yongcan and Kingston,
               Derek",
  publisher = "International Society for Optics and Photonics",
  pages     = "94640Q--94640Q--15",
  month     =  may,
  year      =  2015,
  keywords  = "Networks; Roads; Unmanned aerial vehicles; Unattended ground
               sensors; Sensors; Simulations; Matrices;GraphicalModels"
}

@ARTICLE{Weiss2002-yo,
  title    = "Motion illusions as optimal percepts",
  author   = "Weiss, Yair and Simoncelli, Eero P and Adelson, Edward H",
  abstract = "The pattern of local image velocities on the retina encodes
              important environmental information. Although humans are
              generally able to extract this information, they can easily be
              deceived into seeing incorrect velocities. We show that these
              'illusions' arise naturally in a system that attempts to estimate
              local image velocity. We formulated a model of visual motion
              perception using standard estimation theory, under the
              assumptions that (i) there is noise in the initial measurements
              and (ii) slower motions are more likely to occur than faster
              ones. We found that specific instantiation of such a velocity
              estimator can account for a wide variety of psychophysical
              phenomena.",
  journal  = "Nat. Neurosci.",
  volume   =  5,
  number   =  6,
  pages    = "598--604",
  month    =  jun,
  year     =  2002,
  keywords = "GraphicalModels",
  language = "en"
}

@BOOK{Bishop1995-tc,
  title     = "Neural Networks for Pattern Recognition",
  author    = "Bishop, Christopher M",
  abstract  = "This book provides the first comprehensive treatment of
               feed-forward neural networks from the perspective of statistical
               pattern recognition. After introducing the basic concepts of
               pattern recognition, the book describes techniques for modelling
               probability density functions, and discusses the properties and
               relative merits of the multi-layer perceptron and radial basis
               function network models. It also motivates the use of various
               forms of error functions, and reviews the principal algorithms
               for error function minimization. As well as providing a detailed
               discussion of learning and generalization in neural networks,
               the book also covers the important topics of data processing,
               feature extraction, and prior knowledge. The book concludes with
               an extensive treatment of Bayesian techniques and their
               applications to neural networks.",
  publisher = "Clarendon Press",
  volume    =  92,
  pages     = "482",
  month     =  nov,
  year      =  1995,
  keywords  = "Important;Textbook;TextBooks;GraphicalModels",
  language  = "en"
}

@ARTICLE{Kollar2008-go,
  title     = "Trajectory Optimization using Reinforcement Learning for Map
               Exploration",
  author    = "Kollar, Thomas and Roy, Nicholas",
  abstract  = "Automatically building maps from sensor data is a necessary and
               fundamental skill for mobile robots; as a result, considerable
               research attention has focused on the technical challenges
               inherent in the mapping problem. While statistical inference
               techniques have led to computationally efficient mapping
               algorithms, the next major challenge in robotic mapping is to
               automate the data collection process. In this paper, we address
               the problem of how a robot should plan to explore an unknown
               environment and collect data in order to maximize the accuracy
               of the resulting map. We formulate exploration as a constrained
               optimization problem and use reinforcement learning to find
               trajectories that lead to accurate maps. We demonstrate this
               process in simulation and show that the learned policy not only
               results in improved map building, but that the learned policy
               also transfers successfully to a real robot exploring on MIT
               campus.",
  journal   = "Int. J. Rob. Res.",
  publisher = "Sage PublicationsSage UK: London, England",
  volume    =  27,
  number    =  2,
  pages     = "175--196",
  month     =  feb,
  year      =  2008,
  keywords  = "reinforcement learning; trajectory
               optimiza-;NotRead;reinforcement learning;ReinforcementLearning",
  language  = "en"
}

@BOOK{Deisenroth2010-ue,
  title     = "Efficient Reinforcement Learning using Gaussian Processes",
  author    = "Deisenroth, Marc",
  abstract  = "This book examines Gaussian processes in both model-based
               reinforcement learning (RL) and inference in nonlinear dynamic
               systems. First, we introduce PILCO, a fully Bayesian approach
               for efficient RL in continuous-valued state and action spaces
               when no expert knowledge is available. PILCO takes model
               uncertainties consistently into account during long-term
               planning to reduce model bias. Second, we propose principled
               algorithms for robust filtering and smoothing in GP dynamic
               systems.",
  publisher = "KIT Scientific Publishing",
  volume    =  9,
  year      =  2010,
  keywords  = "computational; information theoretic learning with statistics;
               learning; statistics \& optimisation; theory \&
               algorithms;NotRead;ReinforcementLearning"
}

@ARTICLE{Gosavi2009-mj,
  title    = "Reinforcement Learning: A Tutorial Survey and Recent Advances",
  author   = "Gosavi, Abhijit",
  abstract = "The purpose of this tutorial is to provide an introduction to
              reinforcement learning (RL) at a level easily understood by
              students and researchers in a wide range of disciplines. The
              intent is not to present a\textbackslashnrigorous mathematical
              discussion that requires a great deal of effort on the part of
              the reader, but rather to present a conceptual framework that
              might serve as an introduction to a more rigorous study of RL.
              The fundamental principles and techniques used to solve RL
              problems are presented. The most popular RL algorithms are
              presented. Section 1 presents an overview of RL and provides a
              simple example to develop intuition of the underlying dynamic
              programming mechanism. In Section 2 the parts of a reinforcement
              learning problem are discussed. These include the environment,
              reinforcement function, and value function. Section 3 gives a
              description of the most widely used reinforcement learning
              algorithms. These include TD($\lambda$) and both the residual and
              direct forms of value iteration, Q-learning, and advantage
              learning. In Section 4 some of the ancillary issues in RL are
              briefly discussed, such as choosing an exploration strategy and
              an appropriate discount factor. The conclusion is given in
              Section 5. Finally, Section 6 is a glossary of commonly used
              terms followed by references in Section 7 and a bibliography of
              RL applications in Section 8. The tutorial structure is such that
              each section builds on the information provided in previous
              sections.\textbackslashnIt is assumed that the reader has some
              knowledge of learning algorithms that rely on gradient descent
              (such as the backpropagation of errors algorithm).",
  journal  = "INFORMS J. Comput.",
  volume   =  21,
  number   =  2,
  pages    = "178--192",
  month    =  may,
  year     =  2009,
  keywords = "NotRead;ReinforcementLearning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Glorennec2000-bq,
  title     = "Reinforcement learning: An overview",
  author    = "Glorennec, P Y",
  abstract  = "Abstract Reinforcement learning relies on the association
               between a goal and a scalar signal, interpreted as reward or
               punishment. The objective is not to reproduce some reference
               signal, but to progessively find, by trial and error, the policy
               maximizing the",
  journal   = "European Symposium on Intelligent Techniques (ESIT …",
  publisher = "Citeseer",
  number    = "September",
  pages     = "14--15",
  year      =  2000,
  keywords  = "NotRead;ReinforcementLearning",
  language  = "en\_US"
}

@ARTICLE{Kober2013-vt,
  title    = "Reinforcement learning in robotics: A survey",
  author   = "Kober, J and Bagnell, J A and Peters, J",
  abstract = "Reinforcement learning offers to robotics a framework and set of
              tools for the design of sophisticated and hard-to-engineer
              behaviors. Conversely, the challenges of robotic problems provide
              both inspiration, impact, and validation for developments in
              reinforcement learning. The relationship between disciplines has
              sufficient promise to be likened to that between physics and
              mathematics. In this article, we attempt to strengthen the links
              between the two research communities by providing a survey of
              work in reinforcement learning for behavior generation in robots.
              We highlight both key challenges in robot reinforcement learning
              as well as notable successes. We discuss how contributions tamed
              the complexity of the domain and study the role of algorithms,
              representations, and prior knowledge in achieving these
              successes. As a result, a particular focus of our paper lies on
              the choice between model-based and model-free as well as between
              value-function-based and policy-search methods. By analyzing a
              simple problem in some detail we demonstrate how reinforcement
              learning approaches may be profitably applied, and we note
              throughout open questions and the tremendous potential for future
              research.",
  journal  = "Int. J. Rob. Res.",
  volume   =  32,
  number   =  11,
  pages    = "1238--1274",
  month    =  sep,
  year     =  2013,
  keywords = "learning control; reinforcement learning; robot;
              survey;NotRead;reinforcement learning;ReinforcementLearning",
  language = "en"
}

@INCOLLECTION{Ruvolo2009-rl,
  title     = "Optimization on a Budget: A Reinforcement Learning Approach",
  booktitle = "Advances in Neural Information Processing Systems 21",
  author    = "Ruvolo, Paul L and Fasel, Ian and Movellan, Javier R",
  editor    = "Koller, D and Schuurmans, D and Bengio, Y and Bottou, L",
  publisher = "Curran Associates, Inc.",
  pages     = "1385--1392",
  year      =  2009,
  keywords  = "NotRead;ReinforcementLearning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Gosavi2014-ua,
  title     = "{Simulation-Based} Optimization: Parametric Optimization
               Techniques and Reinforcement Learning",
  author    = "Gosavi, Abhijit",
  abstract  = "Simulation-Based Optimization: Parametric Optimization
               Techniques and Reinforcement Learning introduce the evolving
               area of static and dynamic simulation-based optimization.
               Covered in detail are model-free optimization techniques --
               especially designed for those discrete-event, stochastic systems
               which can be simulated but whose analytical models are difficult
               to find in closed mathematical forms.Key features of this
               revised and improved Second Edition include:· Extensive
               coverage, via step-by-step recipes, of powerful new algorithms
               for static simulation optimization, including simultaneous
               perturbation, backtracking adaptive search and nested
               partitions, in addition to traditional methods, such as response
               surfaces, Nelder-Mead search and meta-heuristics (simulated
               annealing, tabu search, and genetic algorithms)· Detailed
               coverage of the Bellman equation framework for Markov Decision
               Processes (MDPs), along with dynamic programming (value and
               policy iteration) for discounted, average, and total reward
               performance metrics· An in-depth consideration of dynamic
               simulation optimization via temporal differences and
               Reinforcement Learning: Q-Learning, SARSA, and R-SMART
               algorithms, and policy search, via API, Q-P-Learning,
               actor-critics, and learning automata· A special examination of
               neural-network-based function approximation for Reinforcement
               Learning, semi-Markov decision processes (SMDPs), finite-horizon
               problems, two time scales, case studies for industrial tasks,
               computer codes (placed online) and convergence proofs, via
               Banach fixed point theory and Ordinary Differential
               EquationsThemed around three areas in separate sets of chapters
               -- Static Simulation Optimization, Reinforcement Learning and
               Convergence Analysis -- this book is written for researchers and
               students in the fields of engineering (industrial, systems,
               electrical and computer), operations research, computer science
               and applied mathematics.",
  publisher = "Springer",
  volume    =  55,
  pages     = "508",
  series    = "Operations Research/Computer Science Interfaces Series",
  month     =  oct,
  year      =  2014,
  address   = "Boston, MA",
  keywords  = "NotRead;Textbook;TextBooks;ReinforcementLearning",
  language  = "en"
}

@ARTICLE{Ormoneit2002-zi,
  title     = "{Kernel-Based} Reinforcement Learning",
  author    = "Ormoneit, Dirk and Sen, {\'S}aunak",
  abstract  = "We present a kernel-based approach to reinforcement learning
               that overcomes the stability problems of temporal-difference
               learning in continuous state-spaces. First, our algorithm
               converges to a unique solution of an approximate Bellman's
               equation regardless of its initialization values. Second, the
               method is consistent in the sense that the resulting policy
               converges asymptotically to the optimal policy. Parametric value
               function estimates such as neural networks do not possess this
               property. Our kernel-based approach also allows us to show that
               the limiting distribution of the value function estimate is a
               Gaussian process. This information is useful in studying the
               bias-variance tradeoff in reinforcement learning. We find that
               all reinforcement learning approaches to estimating the value
               function, parametric or non-parametric, are subject to a bias.
               This bias is typically larger in reinforcement learning than in
               a comparable regression problem.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  49,
  number    = "2-3",
  pages     = "161--178",
  month     =  nov,
  year      =  2002,
  keywords  = "Kernel smoothing; Kernel-based learning; Lazy learning; Local
               averaging; Markov decision process; Reinforcement
               learning;Artificial Intelligence (incl. Robotics);Computer
               Science- general;NotRead;reinforcement
               learning;ReinforcementLearning",
  language  = "en"
}

@INPROCEEDINGS{Engel2005-vp,
  title     = "Reinforcement Learning with Gaussian Processes",
  booktitle = "Proceedings of the 22Nd International Conference on Machine
               Learning",
  author    = "Engel, Yaakov and Mannor, Shie and Meir, Ron",
  abstract  = "Gaussian Process Temporal Difference (GPTD) learning offers a
               Bayesian solution to the policy evaluation problem of
               reinforcement learning. In this paper we extend the GPTD
               framework by addressing two pressing issues, which were not
               adequately treated in the original GPTD paper (Engel et al.,
               2003). The first is the issue of stochasticity in the state
               transitions, and the second is concerned with action selection
               and policy improvement. We present a new generative model for
               the value function, deduced from its relation with the
               discounted return. We derive a corresponding on-line algorithm
               for learning the posterior moments of the value Gaussian
               process. We also present a SARSA based extension of GPTD, termed
               GPSARSA, that allows the selection of actions and the gradual
               improvement of policies without requiring a world-model.",
  publisher = "ACM",
  pages     = "201--208",
  series    = "ICML '05",
  year      =  2005,
  address   = "New York, NY, USA",
  keywords  = "NotRead;ReinforcementLearning"
}

@INPROCEEDINGS{Chevalier2013-qq,
  title      = "Fast Computation of the {Multi-Points} Expected Improvement
                with Applications in Batch Selection",
  booktitle  = "Learning and Intelligent Optimization",
  author     = "Chevalier, Cl{\'e}ment and Ginsbourger, David",
  editor     = "Nicosia, Giuseppe and Pardalos, Panos",
  abstract   = "The Multi-points Expected Improvement criterion (or
                \textbackslash(q\textbackslash)-EI) has recently been studied
                in batch-sequential Bayesian Optimization. This paper deals
                with a new way of computing \textbackslash(q\textbackslash)-EI,
                without using Monte-Carlo simulations, through a closed-form
                formula. The latter allows a very fast computation of
                \textbackslash(q\textbackslash)-EI for reasonably low values of
                \textbackslash(q\textbackslash) (typically, less than 10). New
                parallel kriging-based optimization strategies, tested on
                different toy examples, show promising results.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "7997 LNCS",
  pages      = "59--69",
  series     = "Lecture Notes in Computer Science",
  month      =  jan,
  year       =  2013,
  keywords   = "Computer experiments; Expected improvement; Kriging; Parallel
                optimization;Acquisition/InfillFxns;BayesOpt;TALAF;batch
                selection;qEI;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language   = "en",
  conference = "International Conference on Learning and Intelligent
                Optimization"
}

@ARTICLE{Dick2013-mi,
  title    = "High-dimensional integration: The quasi-Monte Carlo way",
  author   = "Dick, Josef and Kuo, Frances Y and Sloan, Ian H",
  abstract = "This paper is a contemporary review of QMC (`quasi-Monte Carlo')
              methods, that is, equal-weight rules for the approximate
              evaluation of high-dimensional integrals over the unit cube [0,
              1]s,where s may be large, or even infinite. Af- ter a general
              introduction, the paper surveys recent developments in lattice
              methods, digital nets, and related themes. Among those recent
              developments are methods of construction of both lattices and
              digital nets, to yield QMC rules that have a prescribed rate of
              convergence for sufficiently smooth func- tions, and ideally also
              guaranteed slow growth (or no growth) of the worst-case error as
              s increases. A crucial role is played by parameters called
              `weights', since a careful use of the weight parameters is needed
              to ensure that the worst-case errors in an appropriately weighted
              function space are bounded, or grow only slowly, as the dimension
              s increases. Important tools for the analysis are weighted
              function spaces, reproducing kernel Hilbert spaces, and
              discrepancy, all of which are discussed with an appropriate level
              of detail.",
  journal  = "Acta Numer.",
  volume   =  22,
  number   = "April 2013",
  pages    = "133--288",
  month    =  may,
  year     =  2013,
  keywords = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters",
  language = "en"
}

@BOOK{Ng2013-al,
  title     = "Learning and Intelligent Optimization",
  author    = "Ng, Amos H C and Dudas, Catarina and Bostr{\"o}m, Henrik and
               Deb, Kalyanmoy",
  editor    = "Nicosia, Giuseppe and Pardalos, Panos",
  abstract  = "This paper introduces a novel methodology for the optimization,
               analysis and decision support in production systems engineering.
               The methodology is based on the innovization procedure,
               originally introduced to unveil new and innovative design
               principles in engineering design problems. The innovization
               procedure stretches beyond an optimization task and attempts to
               discover new design/operational rules/principles relating to
               decision variables and objectives, so that a deeper
               understanding of the underlying problem can be obtained. By
               integrating the concept of innovization with simulation and data
               mining techniques, a new set of powerful tools can be developed
               for general systems analysis. The uniqueness of the approach
               introduced in this paper lies in that decision rules extracted
               from the multi-objective optimization using data mining are used
               to modify the original optimization. Hence, faster convergence
               to the desired solution of the decision-maker can be achieved.
               In other words, faster convergence and deeper knowledge of the
               relationships between the key decision variables and objectives
               can be obtained by interleaving the multi-objective optimization
               and data mining process. In this paper, such an interleaved
               approach is illustrated through a set of experiments carried out
               on a simulation model developed for a real-world production
               system analysis problem. \copyright{} 2013 Springer-Verlag.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  7997,
  pages     = "1--18",
  series    = "Lecture Notes in Computer Science",
  year      =  2013,
  address   = "Berlin, Heidelberg",
  keywords  = "Data mining; Innovization; Multi-objective optimization;
               Production system simulation;BayesOpt;TALAF;BayesOpt"
}

@BOOK{Shewchuk1994-og,
  title     = "An Introduction to the Conjugate Gradient Method Without the
               Agonizing Pain",
  author    = "Shewchuk, Jonathan Richard",
  abstract  = "The Conjugate Gradient Method is the most prominent iterative
               method for solving sparse systems of linear equations.
               Unfortunately, many textbook treatments of the topic are written
               with neither illustrations nor intuition, and their victims can
               be found to this day babbling senselessly in the corners of
               dusty libraries. For this reason, a deep, geometric
               understanding of the method has been reserved for the elite
               brilliant few who have painstakingly decoded the mumblings of
               their forebears. Nevertheless, the Conjugate Gradient Method is
               a composite of simple, elegant ideas that almost anyone can
               understand. Of course, a reader as intelligent as yourself will
               learn them almost effortlessly. The idea of quadratic forms is
               introduced and used to derive the methods of Steepest Descent,
               Conjugate Directions, and Conjugate Gradients. Eigenvectors are
               explained and used to examine the convergence of the Jacobi
               Method, Steepest Descent, and Conjugate Gradients. Other topics
               include preconditioning and the nonlinear Conjugate Gradient
               Method. I have taken pains to make this article easy to read.
               Sixty-six illustrations are provided. Dense prose is avoided.
               Concepts are explained in several differentways. Most equations
               are coupled with an intuitive interpretation.",
  publisher = "Carnegie-Mellon University. Department of Computer Science",
  volume    =  49,
  pages     = "64",
  year      =  1994,
  keywords  = "1; 2; 5; agonizing pain; conjugate gradient method; convergence
               analysis; eigen do; eigenvalues; i try; jacobi iterations;
               preconditioning; thinking with
               eigenvectors;BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{MacKay1992-sp,
  title    = "{Information-Based} Objective Functions for Active Data Selection",
  author   = "MacKay, D J C",
  abstract = "Learning can be made more efficient if we can actively select
              particularly salient data points. Within a Bayesian learning
              framework, objective functions are discussed that measure the
              expected informativeness of candidate measurements. Three
              alternative specifications of what we want to gain information
              about lead to three different criteria for data selection. All
              these criteria depend on the assumption that the hypothesis space
              is correct, which may prove to be their main weakness.",
  journal  = "Neural Comput.",
  volume   =  4,
  number   =  4,
  pages    = "590--604",
  month    =  jul,
  year     =  1992,
  keywords = "
              Acquisition/InfillFxns;BayesOpt;TALAF;trust\_informal\_treatment;assurance\_implicit;active\_learning;Supplemental
              Assurance;Quantify
              Uncertainty;BayesOpt;BayesOpt/Acquisition/InfillFxns;Assurances"
}

@ARTICLE{Williams1998-kr,
  title    = "Bayesian classification with Gaussian processes",
  author   = "Williams, C K I and Barber, D",
  abstract = "We consider the problem of assigning an input vector to one of m
              classes by predicting P(c|x) for c=1,...,m. For a two-class
              problem, the probability of class one given x is estimated by
              $\sigma$(y(x)), where $\sigma$(y)=1/(1+e-y). A Gaussian process
              prior is placed on y(x), and is combined with the training data
              to obtain predictions for new x points. We provide a Bayesian
              treatment, integrating over uncertainty in y and in the
              parameters that control the Gaussian process prior the necessary
              integration over y is carried out using Laplace's approximation.
              The method is generalized to multiclass problems (m>2) using the
              softmax function. We demonstrate the effectiveness of the method
              on a number of datasets",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  20,
  number   =  12,
  pages    = "1342--1351",
  month    =  dec,
  year     =  1998,
  keywords = "Bayes methods;Gaussian processes;Markov processes;Monte Carlo
              methods;optimisation;pattern classification;probability;Bayesian
              classification;Gaussian processes;Laplace approximation;Markov
              chain;Monte Carlo method;input vector;multiclass
              problems;optimisation;parameter
              uncertainty;probability;softmax;two-class problem;Bayesian
              methods;Computer Society;Gaussian noise;Gaussian
              processes;Logistics;Monte Carlo methods;Process control;Training
              data;Uncertain systems;Uncertainty;Bayes
              methods;BayesOpt;Important;Markov
              processes;monte\_carlo;NotRead;TALAF;Uncertainty;optimisation;pattern
              classification;probability;GPs;BayesOpt"
}

@ARTICLE{Serofino2014-ro,
  title    = "Optimizing Without Derivatives: What Does the No Free Lunch
              Theorem Actually Say?",
  author   = "Serofino, Loris",
  journal  = "Not. Am. Math. Soc.",
  volume   =  61,
  number   =  07,
  pages    = "750",
  month    =  aug,
  year     =  2014,
  keywords = "BayesOpt;TALAF;BayesOpt",
  language = "en"
}

@INPROCEEDINGS{Snoek2012-tt,
  title     = "Practical Bayesian Optimization of Machine Learning Algorithms",
  booktitle = "Adv. Neural Inf. Process. Syst. 25",
  author    = "Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P",
  abstract  = "The use of machine learning algorithms frequently involves
               careful tuning of learning parameters and model hyperparameters.
               Unfortunately, this tuning is often a
               \{\textbackslashtextquoteleft\}\{\textbackslashtextquoteleft\}black
               art\{\textbackslashtextquoteright\}\{\textbackslashtextquoteright\}
               requiring expert experience, rules of thumb, or sometimes
               brute-force search. There is therefore great appeal for
               automatic approaches that can optimize the performance of any
               given learning algorithm to the problem at hand. In this work,
               we consider this problem through the framework of Bayesian
               optimization, in which a learning
               algorithm\{\textbackslashtextquoteright\}s generalization
               performance is modeled as a sample from a Gaussian process (GP).
               We show that certain choices for the nature of the GP, such as
               the type of kernel and the treatment of its hyperparameters, can
               play a crucial role in obtaining a good optimizer that can
               achieve expert-level performance. We describe new algorithms
               that take into account the variable cost (duration) of learning
               algorithm experiments and that can leverage the presence of
               multiple cores for parallel experimentation. We show that these
               proposed algorithms improve on previous automatic procedures and
               can reach or surpass human expert-level optimization for many
               algorithms including latent Dirichlet allocation, structured
               SVMs and convolutional neural networks.",
  pages     = "2951--2959",
  year      =  2012,
  keywords  = "bayesian optimization; deep learning; gaussian
               process;BayesOpt;Important;TALAF;BayesOpt"
}

@MISC{DBSCAN_wiki2016-cr,
  title    = "{DBSCAN}",
  author   = "{DBSCAN\_wiki}",
  abstract = "Density-based spatial clustering of applications with noise
              (DBSCAN) is a data clustering algorithm proposed by Martin Ester,
              Hans-Peter Kriegel, J\{{\"o}\}rg Sander and Xiaowei Xu in 1996.
              It is a density-based clustering algorithm: given a set of points
              in some space, it groups together points that are closely packed
              together (points with many nearby neighbors), marking as outliers
              points that lie alone in low-density regions (whose nearest
              neighbors are too far away). DBSCAN is one of the most common
              clustering algorithms and also most cited in scientific
              literature. In 2014, the algorithm was awarded the test of time
              award (an award given to algorithms which have received
              substantial attention in theory and practice) at the leading data
              mining conference, KDD.",
  month    =  jan,
  year     =  2016,
  keywords = "Acquisition/InfillFxns; BayesOpt;
              TALAF;Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{MacKay1999-ws,
  title    = "Comparison of Approximate Methods for Handling Hyperparameters",
  author   = "MacKay, David J C",
  abstract = "I examine two approximate methods for computational
              implementation of Bayesian hierarchical models, that is, models
              that include unknown hyperparameters such as regularization
              constants and noise levels. In the evidence framework, the model
              parameters are integrated over, and the resulting evidence is
              maximized over the hyperparameters. The optimized hyperparameters
              are used to define a gaussian approximation to the posterior
              distribution. In the alternative MAP method, the true posterior
              probability is found by integrating over the hyperparameters. The
              true posterior is then maximized over the model parameters, and a
              gaussian approximation is made. The similarities of the two
              approaches and their relative merits are discussed, and
              comparisons are made with the ideal hierarchical Bayesian
              solution. In moderately ill-posed problems, integration over
              hyperparameters yields a probability distribution with a skew
              peak, which causes signifi-cant biases to arise in the MAP
              method. In contrast, the evidence framework is shown to introduce
              negligible predictive error under straightforward conditions.
              General lessons are drawn concerning inference in many
              dimensions.",
  journal  = "Neural Comput.",
  volume   =  11,
  number   =  5,
  pages    = "1035--1068",
  year     =  1999,
  keywords = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Finkel2003-mt,
  title    = "{DIRECT} optimization algorithm user guide",
  author   = "Finkel, Daniel E",
  abstract = "O objetivo deste guia {\'e} breve para introduzir o leitor para o
              algoritmo de otimiza{\c c}{\~a}o DIRECT, descrever o tipo de
              problemas que resolve, como usar o programa que acompanha MATLAB
              direct.m, e fornecer uma sinopse de como ele procura o m{\'\i}nio
              global . Um exemplo de DIRECT sendo usado em um problema de teste
              {\'e} fornecida, e os motiviation para o algoritmo {\'e}
              tamb{\'e}m discutida. O ap{\^e}ndice fornece f{\'o}rmulas e dados
              para o 7 fun{\c c}{\~o}es de teste que foram utilizadas em [3].",
  journal  = "Center for Research in Scientific Computation, North Carolina
              State University",
  volume   =  2,
  pages    = "1--14",
  year     =  2003,
  keywords = "BayesOpt;DIRECT;TALAF;BayesOpt"
}

@MISC{noauthor_undated-pu,
  title        = "optimization - Hyperparameter estimation in Gaussian process
                  - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/30069/hyperparameter-estimation-in-gaussian-process}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Cawley2010-ey,
  title     = "On Over-fitting in Model Selection and Subsequent Selection Bias
               in Performance Evaluation",
  author    = "Cawley, Gavin C and Talbot, Nicola L C",
  abstract  = "Model selection strategies for machine learning algorithms
               typically involve\textbackslashnthe numerical optimisation of an
               appropriate model selection criterion, often\textbackslashnbased
               on an estimator of generalisation performance, such as k-fold
               \textbackslashncross-validation. The error of such an estimator
               can be broken down into bias \textbackslashnand variance
               components. While unbiasedness is often cited as a beneficial
               \textbackslashnquality of a model selection criterion, we
               demonstrate that a low variance is \textbackslashnat least as
               important, as a non-negligible variance introduces the potential
               \textbackslashnfor over-fitting in model selection as well as in
               training the model. While \textbackslashnthis observation is in
               hindsight perhaps rather obvious, the degradation in
               \textbackslashnperformance due to over-fitting the model
               selection criterion can be \textbackslashnsurprisingly large, an
               observation that appears to have received little
               \textbackslashnattention in the machine learning literature to
               date. In this paper, we show \textbackslashnthat the effects of
               this form of over-fitting are often of comparable
               \textbackslashnmagnitude to differences in performance between
               learning algorithms, and thus \textbackslashncannot be ignored
               in empirical evaluation. Furthermore, we show that some
               \textbackslashncommon performance evaluation practices are
               susceptible to a form of selection \textbackslashnbias as a
               result of this form of over-fitting and hence are unreliable. We
               \textbackslashndiscuss methods to avoid over-fitting in model
               selection and subsequent \textbackslashnselection bias in
               performance evaluation, which we hope will be incorporated
               \textbackslashninto best practice. While this study concentrates
               on cross-validation based \textbackslashnmodel selection, the
               findings are quite general and apply to any model
               \textbackslashnselection practice involving the optimisation of
               a model selection criterion \textbackslashnevaluated over a
               finite sample of data, including maximisation of the Bayesian
               \textbackslashnevidence and optimisation of performance bounds.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  11,
  pages     = "2079--2107",
  month     =  mar,
  year      =  2010,
  keywords  = "bias-variance trade-off; model selection; over-; performance
               evaluation; selection
               bias;BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@INPROCEEDINGS{Swersky2013-ej,
  title    = "Multi-task bayesian optimization",
  author   = "Swersky, Kevin and Snoek, Jasper and Adams, Ryan P",
  pages    = "2004--2012",
  year     =  2013,
  keywords = "BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{Prechelt1998-ue,
  title    = "Automatic early stopping using cross validation: quantifying the
              criteria",
  author   = "Prechelt, Lutz",
  abstract = "Cross validation can be used to detect when overfitting starts
              during supervised training of a neural network; training is then
              stopped before convergence to avoid the overfitting ('early
              stopping'). The exact criterion used for cross validation based
              early stopping, however, is chosen in an ad-hoc fashion by most
              researchers or training is stopped interactively. To aid a more
              well-founded selection of the stopping criterion, 14 different
              automatic stopping criteria from three classes were evaluated
              empirically for their efficiency and effectiveness in 12
              different classification and approximation tasks using
              multi-layer perceptrons with RPROP training. The experiments show
              that, on average, slower stopping criteria allow for small
              improvements in generalization (in the order of 4\%), but cost
              about a factor of 4 longer in training time.",
  journal  = "Neural Netw.",
  volume   =  11,
  number   =  4,
  pages    = "761--767",
  month    =  jun,
  year     =  1998,
  keywords = "Cross validation; Early stopping; Empirical study;
              Generalization; Overfitting; Supervised
              learning;BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters",
  language = "en"
}

@ARTICLE{Reunanen2003-hn,
  title     = "Overfitting in making comparisons between variable selection
               methods",
  author    = "Reunanen, Juha",
  abstract  = "This paper addresses a common methodological flaw in the
               comparison of variable selection meth-ods. A practical approach
               to guide the search or the selection process is to compute
               cross-validation performance estimates of the different variable
               subsets. Used with computationally intensive search algorithms,
               these estimates may overfit and yield biased predictions.
               Therefore, they cannot be used reliably to compare two selection
               methods, as is shown by the empirical results of this paper.
               In-stead, like in other instances of the model selection
               problem, independent test sets should be used for determining
               the final performance. The claims made in the literature about
               the superiority of more exhaustive search algorithms over
               simpler ones are also revisited, and some of them infirmed.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  3,
  pages     = "1371--1382",
  month     =  mar,
  year      =  2003,
  keywords  = "Algorithm comparison; Cross-validation; Overfitting; Variable
               selection; k nearest
               neighbors;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Sobester2005-ms,
  title     = "On the Design of Optimization Strategies Based on Global
               Response Surface Approximation Models",
  author    = "S{\'o}bester, Andr{\'a}s and Leary, Stephen J and Keane, Andy J",
  abstract  = "Striking the correct balance between global exploration of
               search spaces and local exploitation of promising basins of
               attraction is one of the principal concerns in the design of
               global optimization algorithms. This is true in the case of
               techniques based on global response surface approximation models
               as well. After constructing such a model using some initial
               database of designs it is far from obvious how to select further
               points to examine so that the appropriate mix of exploration and
               exploitation is achieved. In this paper we propose a selection
               criterion based on the expected improvement measure, which
               allows relatively precise control of the scope of the search. We
               investigate its behavior through a set of artificial test
               functions and two structural optimization problems. We also look
               at another aspect of setting up search heuristics of this type:
               the choice of the size of the database that the initial
               approximation is built upon.",
  journal   = "J. Global Optimiz.",
  publisher = "Kluwer Academic Publishers",
  volume    =  33,
  number    =  1,
  pages     = "31--59",
  month     =  sep,
  year      =  2005,
  keywords  = "Expected improvement; Gaussian kernels; Radial basis
               functions;Acquisition/InfillFxns;BayesOpt;Computer Science-
               general;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language  = "en"
}

@ARTICLE{Zhou2011-eo,
  title    = "A Simple Approach to Emulation for Computer Models With
              Qualitative and Quantitative Factors",
  author   = "Zhou, Qiang and Qian, Peter Z G and Zhou, Shiyu",
  abstract = "We propose a flexible yet computationally efficient approach for
              building Gaussian process models for computer experiments with
              both qualitative and quantitative factors. This approach uses the
              hypersphere parameterization to model the correlations of the
              qualitative factors, thus avoiding the need of directly solving
              optimization problems with positive definite constraints. The
              effectiveness of the proposed method is successfully illustrated
              by several examples.",
  journal  = "Technometrics",
  volume   =  53,
  number   =  3,
  pages    = "266--273",
  year     =  2011,
  keywords = "computer experiment; hypersphere decomposition;
              kriging;BayesOpt;Important;TALAF;BayesOpt"
}

@ARTICLE{Ma2015-kx,
  title     = "Active Pointillistic Pattern Search",
  author    = "Ma, Y and Sutherland, D J and Garnett, R and Schneider, J G",
  abstract  = "Abstract We introduce the problem of active pointillistic
               pattern search (APPS), which seeks to discover regions of a
               domain exhibiting desired behavior with limited observations.
               Unusually, the patterns we consider are defined by large-scale
               properties of an underlying",
  journal   = "AISTATS",
  publisher = "jmlr.org",
  pages     = "672--680",
  year      =  2015,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{Kushner1964-le,
  title    = "A new method of locating the maximal point of an arbitrary
              multipeak curve in the presence of noise",
  author   = "Kushner, Harold J",
  journal  = "J. Basic Eng.",
  volume   =  86,
  number   =  1,
  pages    = "97--106",
  year     =  1964,
  keywords = "BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{Wang2013-lg,
  title     = "Adaptive Hamiltonian and Riemann manifold Monte Carlo samplers",
  author    = "Wang, Z and Mohamed, S and De Freitas, N",
  abstract  = "Abstract In this paper we address the widelyexperienced
               difficulty in tuning Monte Carlo sampler based on simulating
               Hamiltonian dynamics. We develop an algorithm that allows for
               the adaptation of Hamiltonian and Riemann manifold Hamiltonian
               Monte Carlo samplers using Bayesian optimization that allows for
               infinite adaptation of the parameters of these samplers. We show
               that the resulting samplers are ergodic, and that the use of our
               ...",
  journal   = "International Conference on Machine",
  publisher = "jmlr.org",
  volume    =  28,
  pages     = "10",
  year      =  2013,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{Picheny2013-uu,
  title     = "A benchmark of kriging-based infill criteria for noisy
               optimization",
  author    = "Picheny, Victor and Wagner, Tobias and Ginsbourger, David",
  abstract  = "Responses of many real-world problems can only be evaluated
               perturbed by noise. In order to make an efficient optimization
               of these problems possible, intelligent optimization strategies
               successfully coping with noisy evaluations are required. In this
               article, a comprehensive review of existing kriging-based
               methods for the optimization of noisy functions is provided. In
               summary, ten methods for choosing the sequential samples are
               described using a unified formalism. They are compared on
               analytical benchmark problems, whereby the usual assumption of
               homoscedastic Gaussian noise made in the underlying models is
               meet. Different problem configurations (noise level, maximum
               number of observations, initial number of observations) and
               setups (covariance functions, budget, initial sample size) are
               considered. It is found that the choices of the initial sample
               size and the covariance function are not critical. The choice of
               the method, however, can result in significant differences in
               the performance. In particular, the three most intuitive
               criteria are found as poor alternatives. Although no criterion
               is found consistently more efficient than the others, two
               specialized methods appear more robust on average.",
  journal   = "Struct. Multidiscip. Optim.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  48,
  number    =  3,
  pages     = "607--626",
  month     =  apr,
  year      =  2013,
  keywords  = "EGO; Metamodeling;
               Noise;Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language  = "en"
}

@INPROCEEDINGS{Martinez-Cantin2007-zu,
  title     = "Active Policy Learning for Robot Planning and Exploration under
               Uncertainty",
  booktitle = "Robotics: Science and Systems",
  author    = "Martinez-Cantin, Ruben and de Freitas, Nando and Doucet, Arnaud
               and Castellanos, Jos{\'e} A",
  pages     = "321--328",
  year      =  2007,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@ARTICLE{Hennig2011-wa,
  title         = "Entropy Search for {Information-Efficient} Global
                   Optimization",
  author        = "Hennig, Philipp and Schuler, Christian J",
  abstract      = "Contemporary global optimization algorithms are based on
                   local measures of utility, rather than a probability measure
                   over location and value of the optimum. They thus attempt to
                   collect low function values, not to learn about the optimum.
                   The reason for the absence of probabilistic global
                   optimizers is that the corresponding inference problem is
                   intractable in several ways. This paper develops desiderata
                   for probabilistic optimization algorithms, then presents a
                   concrete algorithm which addresses each of the computational
                   intractabilities with a sequence of approximations and
                   explicitly adresses the decision problem of maximizing
                   information gain from each evaluation.",
  pages         = "1809--1837",
  month         =  dec,
  year          =  2011,
  keywords      = "BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1112.1217"
}

@ARTICLE{Kuindersma2013-oq,
  title     = "Variable risk control via stochastic optimization",
  author    = "Kuindersma, Scott R and Grupen, Roderic A and Barto, Andrew G",
  abstract  = "We present new global and local policy search algorithms
               suitable for problems with policy-dependent cost variance (or
               risk), a property present in many robot control tasks. These
               algorithms exploit new techniques in non-parametric
               heteroscedastic regression to directly model the
               policy-dependent distribution of cost. For local search, the
               learned cost model can be used as a critic for performing
               risk-sensitive gradient descent. Alternatively,
               decision-theoretic criteria can be applied to globally select
               policies to balance exploration and exploitation in a principled
               way, or to perform greedy minimization with respect to various
               risk-sensitive criteria. This separation of learning and policy
               selection permits variable risk control, where risk-sensitivity
               can be flexibly adjusted and appropriate policies can be
               selected at runtime without relearning. We describe experiments
               in dynamic stabilization and manipulation with a mobile
               manipulator that demonstrate learning of flexible,
               risk-sensitive policies in...",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  32,
  number    =  7,
  pages     = "806--825",
  month     =  jul,
  year      =  2013,
  keywords  = "bayesian optimization; dynamic mobile manipulation; policy
               search; risk-sensitive; robot
               learning;BayesOpt;Important;NotRead;TALAF;BayesOpt",
  language  = "en"
}

@ARTICLE{Brochu2010-tj,
  title         = "A Tutorial on Bayesian Optimization of Expensive Cost
                   Functions, with Application to Active User Modeling and
                   Hierarchical Reinforcement Learning",
  author        = "Brochu, Eric and Cora, Vlad M and de Freitas, Nando",
  abstract      = "We present a tutorial on Bayesian optimization, a method of
                   finding the maximum of expensive cost functions. Bayesian
                   optimization employs the Bayesian technique of setting a
                   prior over the objective function and combining it with
                   evidence to get a posterior function. This permits a
                   utility-based selection of the next observation to make on
                   the objective function, which must take into account both
                   exploration (sampling from areas of high uncertainty) and
                   exploitation (sampling areas likely to offer improvement
                   over the current best observation). We also present two
                   detailed extensions of Bayesian optimization, with
                   experiments---active user modelling with preferences, and
                   hierarchical reinforcement learning---and a discussion of
                   the pros and cons of Bayesian optimization based on our
                   experiences.",
  month         =  dec,
  year          =  2010,
  keywords      = "BayesOpt;Important;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1012.2599"
}

@ARTICLE{Bergstra2012-tg,
  title     = "Random Search for Hyper-parameter Optimization",
  author    = "Bergstra, James and Bengio, Yoshua",
  abstract  = "Grid search and manual search are the most widely used
               strategies for hyper-parameter optimiza- tion. This paper shows
               empirically and theoretically that randomly chosen trials are
               more efficient for hyper-parameter optimization than trials on a
               grid. Empirical evidence comes from a compar- ison with a large
               previous study that used grid search and manual search to
               configure neural net- works and deep belief networks. Compared
               with neural networks configured by a pure grid search, we find
               that random search over the same domain is able to find models
               that are as good or better within a small fraction of the
               computation time. Granting random search the same computational
               budget, random search finds better models by effectively
               searching a larger, less promising con- figuration space.
               Compared with deep belief networks configured by a thoughtful
               combination of manual search and grid search, purely random
               search over the same 32-dimensional configuration space found
               statistically equal performance on four of seven data sets, and
               superior performance on one of seven. A Gaussian process
               analysis of the function from hyper-parameters to validation set
               performance reveals that for most data sets only a few of the
               hyper-parameters really matter, but that different
               hyper-parameters are important on different data sets. This
               phenomenon makes grid search a poor choice for configuring
               algorithms for new data sets. Our analysis casts some light on
               why recent ``High Throughput''methods achieve surprising
               success---they appear to search through a large number of
               hyper-parameters because most hyper-parameters do not matter
               much. We anticipate that growing interest in large hierarchical
               models will place an increasing burden on techniques for
               hyper-parameter optimization; this work shows that randomsearch
               is a natural base- line against which to judge progress in the
               development of adaptive (sequential) hyper-parameter
               optimization algorithms.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  13,
  number    =  1,
  pages     = "281--305",
  month     =  feb,
  year      =  2012,
  keywords  = "deep learning; global optimization; model selection; neural
               networks; response surface; response surface
               modeling;BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@MISC{noauthor_undated-uo,
  title        = "machine learning - Marginalization of {GP} regression
                  hyperparameters with Laplace approximation - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/173216/marginalization-of-gp-regression-hyperparameters-with-laplace-approximation}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Perrin2009-oo,
  title    = "Sex differences in the growth of white matter during adolescence",
  author   = "Perrin, J S and Leonard, G and Perron, M and Pike, G B and
              Pitiot, A and Richer, L and Veillette, S and Pausova, Z and Paus,
              T",
  abstract = "The purpose of this study was to examine sex differences in the
              maturation of white matter during adolescence (12 to 18 years of
              age). We measured lobular volumes of white matter and
              white-matter ``density'' throughout the brain using T1-weighted
              images, and estimated the myelination index using
              magnetisation-transfer ratio (MTR). In male adolescents, we
              observed age-related increases in white-matter lobular volumes
              accompanied by decreases in the lobular values of white-matter
              MTR. White-matter density in the putative cortico-spinal tract
              (pCST) decreased with age. In female adolescents, on the other
              hand, we found only small age-related increase in white-matter
              volumes and no age-related changes in white-matter MTR, with the
              exception of the frontal lobe where MTR increased. White-matter
              density in the pCST also increased with age. These results
              suggest that sex-specific mechanisms may underlie the growth of
              white matter during adolescence. We speculate that these
              mechanisms involve primarily age-related increases in axonal
              calibre in males and increased myelination in females.",
  journal  = "Neuroimage",
  volume   =  45,
  number   =  4,
  pages    = "1055--1066",
  month    =  may,
  year     =  2009,
  keywords = "BayesOpt;NotRead;TALAF;BayesOpt",
  language = "en"
}

@ARTICLE{Boyle2007-oo,
  title     = "Gaussian processes for regression and optimisation",
  author    = "Boyle, Phillip",
  abstract  = "Gaussian processes have proved to be useful and powerful
               constructs for the purposes of regression. The classical method
               proceeds by parameterising a covariance function, and then
               infers the parameters given the training data. In this thesis,
               the classical approach is augmented by interpreting Gaussian
               processes as the outputs of linear filters excited by white
               noise. This enables a straightforward definition of dependent
               Gaussian processes as the outputs of a multiple output linear
               filter excited by multiple noise sources. We show how dependent
               Gaussian processes defined in this way can also be used for the
               purposes of system identification. Onewell known
               problemwithGaussian process regression is that the compu-
               tational complexity scales poorlywith the amount of training
               data.We review one approximate solution that alleviates this
               problem, namely reduced rank Gaussian processes. We then show
               how the reduced rank approximation can be applied to allow for
               the efficient computation of dependent Gaussian pro- cesses. We
               then examine the application ofGaussian processes to the
               solution of other machine learning problems. To do so, we review
               methods for the parameter- isation of full covariance matrices.
               Furthermore, we discuss how improve- ments can be made by
               marginalising over alternative models, and introduce methods to
               perform these computations efficiently. In particular, we intro-
               duce sequential annealed importance sampling as a method for
               calculating model evidence in an on-line fashion as newdata
               arrives. Gaussian process regression can also be applied to
               optimisation. An algo- rithm is described that uses model
               comparison between multiple models to find the optimumof a
               function while taking as fewsamples as possible. This algorithm
               shows impressive performance on the standard control problem of
               double pole balancing. Finally, we describe how Gaussian
               processes can be used to efficiently estimate gradients of noisy
               functions, and numerically estimate integrals.",
  publisher = "Victoria University of Wellington",
  pages     = "190",
  year      =  2007,
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters",
  language  = "en\_NZ"
}

@PHDTHESIS{Huang2005-ps,
  title    = "Experimental planning and sequential kriging optimization using
              variable fidelity data",
  author   = "Huang, Deng",
  abstract = "Engineers in many industries routinely need to improve the
              product or process designs using data from the field, lab
              experiments, and computer experiments. Historically, designers
              have performed a calibration exercise to ``fix'' the lab system
              or computer model and then used an analysis method or
              optimization procedure that ignores the fact that systematic
              differences between products in the field and other environments
              necessarily exist. A new line of research is not based on the
              assumption that calibration is perfect and seeks to develop
              experimental planning and optimization schemes using data form
              multiple experimental sources. We use the term ``fidelity'' to
              refer to the extent to which a surrogate experimental system can
              reproduce results of the system of interest. For experimental
              planning, we present perhaps the first optimal designs for
              variable fidelity experimentation, using an extension of the
              Expected Integrated Mean Squared Error (EIMSE) criterion, where
              the Generalized Least Squares (GLS) method was used to generate
              the predictions. Numerical tests are used to compare the method
              performance with alternatives and to investigate the robustness
              to incorporated assumptions. The method is applied to automotive
              engine valve heat treatment process design in which real world
              data were mixed with data from two types of computer simulations.
              Sequential Kriging Optimization (SKO) is a method developed in
              recent years for solving expensive black-box problems in areas
              such as large-scale circuit board design and manufacturing
              process improvement. We propose an extension of the SKO method,
              named Multiple Fidelity Sequential Kriging Optimization (MFSKO),
              where surrogate systems are exploited to reduce the total
              evaluation cost. As a pre-step to MFSKO, we extended SKO to
              address stochastic black-box systems. In the empirical studies
              using numerical test functions, SKO compared favorably with
              alternatives in terms of consistency in finding global optima and
              efficiency as measured by number of evaluations. Also, in the
              presence of noise, the new expected improvement function for
              infill sample selection appears to achieve the desired balance
              between the need for global and local searches. In the proposed
              MFSKO method, data on all experimental systems are integrated to
              build a kriging meta-model that provides a global prediction of
              the system of interest and a measure of prediction uncertainty.
              The location and fidelity level of the next evaluation are
              selected by maximizing an augmented expected improvement
              function, which is connected with the evaluation costs. The
              proposed method was applied to test functions from the literature
              and metal-forming process design problems via Finite Element
              simulations. The method manifests sensible search patterns,
              robust performance, and appreciable reduction in total evaluation
              cost as compared to the original method.",
  year     =  2005,
  keywords = "0546:Industrial engineering; 0796:Operations research; Applied
              sciences; Engine valves; Industrial engineering; Kriging;
              Multiple fidelity; Operations
              research;Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@MISC{noauthor_undated-gj,
  title        = "Experiences with bayesian hyperparameter optimization?",
  abstract     = "I was checking the paper, [Practical Bayesian Optimization of
                  Machine Learning](http://arxiv.org/pdf/1206.2944) and i was
                  wondering if anyone here...",
  howpublished = "\url{https://www.reddit.com/r/MachineLearning/comments/2m1cad/experiences_with_bayesian_hyperparameter/}",
  keywords     = "BayesOpt;NotRead;TALAF;BayesOpt"
}

@INPROCEEDINGS{Bonilla2007-jw,
  title     = "Multi-task Gaussian process prediction",
  booktitle = "Advances in neural information processing systems",
  author    = "Bonilla, Edwin V and Chai, Kian M and Williams, Christopher",
  abstract  = "In this paper we investigate multi-task learning in the context
               of Gaussian Processes (GP). We propose a model that learns a
               shared covariance function on input-dependent features and a
               ``free-form'' covariance matrix over tasks. This allows for good
               flexibility when modelling inter-task dependencies while
               avoiding the need for large amounts of data for training. We
               show that under the assumption of noise-free observations and a
               block design, predictions for a given task only depend on its
               target values and therefore a cancellation of inter-task
               transfer occurs. We evaluate the benefits of our model on two
               practical applications: a compiler performance prediction
               problem and an exam score prediction task. Additionally, we make
               use of GP approximations and properties of our model in order to
               provide scalability to large data sets.",
  volume    =  20,
  pages     = "153--160",
  year      =  2007,
  keywords  = "learning; statistics \&
               optimisation;BayesOpt;NotRead;TALAF;BayesOpt"
}

@TECHREPORT{Seeger2005-dc,
  title    = "Semiparametric latent factor models",
  author   = "Seeger, Matthias and Teh, Yee-Whye and Jordan, Michael",
  abstract = "(x n , y n",
  year     =  2005,
  keywords = "BayesOpt;NotRead;TALAF;BayesOpt"
}

@ARTICLE{Schliep2012-en,
  title         = "Multilevel latent Gaussian process model for mixed discrete
                   and continuous multivariate response data",
  author        = "Schliep, Erin M and Hoeting, Jennifer A",
  abstract      = "We propose a Bayesian model for mixed ordinal and continuous
                   multivariate data to evaluate a latent spatial Gaussian
                   process. Our proposed model can be used in many contexts
                   where mixed continuous and discrete multivariate responses
                   are observed in an effort to quantify an unobservable
                   continuous measurement. In our example, the latent, or
                   unobservable measurement is wetland condition. While
                   predicted values of the latent wetland condition variable
                   produced by the model at each location do not hold any
                   intrinsic value, the relative magnitudes of the wetland
                   condition values are of interest. In addition, by including
                   point-referenced covariates in the model, we are able to
                   make predictions at new locations for both the latent random
                   variable and the multivariate response. Lastly, the model
                   produces ranks of the multivariate responses in relation to
                   the unobserved latent random field. This is an important
                   result as it allows us to determine which response variables
                   are most closely correlated with the latent variable. Our
                   approach offers an alternative to traditional indices based
                   on best professional judgment that are frequently used in
                   ecology. We apply our model to assess wetland condition in
                   the North Platte and Rio Grande River Basins in Colorado.
                   The model facilitates a comparison of wetland condition at
                   multiple locations and ranks the importance of in-field
                   measurements.",
  month         =  may,
  year          =  2012,
  keywords      = "BayesOpt;Important;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1205.4163"
}

@INPROCEEDINGS{Tamrat2004-be,
  title      = "The X-31: A {Post-Stall} Technology ({PST}) Fighter
                {Close-In-Combat} Results Assessment, And A Look At New {CIC}
                Performance Evaluation Metrics",
  booktitle  = "{AIAA} Atmospheric Flight Mechanics Conference and Exhibit",
  author     = "Tamrat, Befecadu",
  abstract   = "Key historical fighter performance metrics were reviewed, and
                their shortcomings and strengths were discussed. Flight time
                histories of a typical air-to-air combat engagement of the X-31
                against its F-18 adversary were assessed, and conclusions were
                drawn. Enough parameters were shown to help the reader make
                his/her own assessment as well. Given sufficient roll rate
                capability about flight path, the control of rate of descent
                was identified as key to X-31 superior close-in-combat
                performance. It was shown that open-loop turn reversal
                maneuvers, based on point-mass simulation, could be used to
                compare potential CIC out come of fighters. Based on this, the
                Dog-Fight-Metric (DFM) was formulated. DFM is a simple
                empirical metric that could be used to evaluate/ design CIC
                fighters. It emphasizes low wing-loading and high drag at
                moderate to high angles of attack. The use of this metric
                suggested that equal DFM could predict equal close- in-combat
                outcome. For example, the X-31 and the F-18 both flying at DFM
                25, had corresponding angles of attack of 43 and 35 degrees,
                respectively. It is concluded that dissimilar fighters could be
                made to have similar CIC outcome (equal DFM) when flown at
                different angles of attack. Conversely, two similar fighters
                could be configured as adversaries via angle of attack
                limiting.",
  publisher  = "American Institute of Aeronautics and Astronautics",
  pages      = "1--21",
  month      =  aug,
  year       =  2004,
  address    = "Reston, Virigina",
  keywords   = "TALAF;BayesOpt",
  language   = "en",
  conference = "AIAA Atmospheric Flight Mechanics Conference and Exhibit"
}

@PHDTHESIS{Pietilainen2010-ab,
  title    = "Approximations for Integration over the Hyperparameters in
              Gaussian Processes",
  author   = "Pietil{\"a}inen, Ville",
  abstract = "This thesis examines three numerical approximations for the
              analytically intractable in- tegral over the posterior
              distribution of the hyperparameters in Gaussian processes. The
              properties of the approximations are studied, and their
              performance is compared to each other and to a method using a
              point-estimate. Traditionally the integral over the posterior of
              the hyperparameters is computed using Markov chain Monte Carlo
              (MCMC) -methods. However, MCMC methods suffer from a heavy
              computational burden of Gaussian processes, because the
              complexity of Gaussian process models grows with the amount of
              the data used. An alternative approach has been to use only a
              point estimate for the hyperparameters instead of integrating
              over their posterior distribution. This is a computationally
              attractive approach, but it ignores the uncertainty related to
              the hyperparameters. The approximations discussed in this thesis
              attempt to take the uncertainty in the hyper- parameters into
              consideration better than does a point estimate method, and to be
              compu- tationally lighter than MCMC methods. The results
              demonstrate that the integration over the hyperparameters is
              beneficial in particular conditions. In addition, it is shown
              that a point estimate method yields equally accurate results with
              the integration methods in other situations. The amount of the
              data and the use of the models determine the need for the
              integration methods and the determining conditions are discussed
              in this work",
  year     =  2010,
  keywords = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@MISC{John_undated-qj,
  title    = "Making a singular matrix non-singular",
  author   = "{John}",
  abstract = "Someone asked me on Twitter Is there a trick to make an singular
              (non-invertible) matrix invertible? The only response I could
              think of in less than 140",
  keywords = "Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Qi2004-ot,
  title     = "Predictive automatic relevance determination by expectation
               propagation",
  author    = "Qi, Yuan (alan) and Minka, Thomas P and Picard, Rosalind W and
               Ghahramani, Zoubin",
  abstract  = "In many real-world classification problems the input contains a
               large \textbackslashr\textbackslashnnumber of potentially
               irrelevant features. This paper proposes a new Bayesian
               framework for determining the relevance of input features. This
               \textbackslashr\textbackslashnapproach extends one of
               the\textbackslashr\textbackslashnmost successful Bayesian
               methods for feature selection and sparse
               \textbackslashr\textbackslashnlearning, known as Automatic
               Relevance Determination (ARD). ARD finds
               \textbackslashr\textbackslashnthe relevance of features by
               optimizing the model marginal likelihood,
               also\textbackslashr\textbackslashn known as the evidence. We
               show that this can lead to overfitting. To
               \textbackslashr\textbackslashnaddress this problem, we propose
               Predictive ARD based on estimating
               \textbackslashr\textbackslashnthe predictive performance of the
               classifier. While the actual leave-one-out
               \textbackslashr\textbackslashnpredictive performance is
               generally very costly to compute, the
               expectation\textbackslashr\textbackslashn propagation (EP)
               algorithm proposed by Minka provides an estimate of
               \textbackslashr\textbackslashnthis predictive performance as a
               side-effect of its iterations. We exploit
               this\textbackslashr\textbackslashn in our algorithm to do
               feature selection, and to select data points in a
               \textbackslashr\textbackslashnsparse Bayesian kernel classifier.
               Moreover, we provide two other
               \textbackslashr\textbackslashnimprovements to previous
               algorithms, by replacing Laplace's
               \textbackslashr\textbackslashnapproximation with the generally
               more accurate EP, and by incorporating
               \textbackslashr\textbackslashnthe fast optimization algorithm
               proposed by Faul and Tipping. Our
               \textbackslashr\textbackslashnexperiments show that our method
               based on the EP estimate of predictive
               \textbackslashr\textbackslashnperformance is more accurate on
               test data than relevance determination
               by\textbackslashr\textbackslashnoptimizing the evidence.",
  journal   = "Twenty-first international conference on Machine learning - ICML
               '04",
  publisher = "ACM",
  pages     = "85",
  series    = "ICML '04",
  year      =  2004,
  address   = "New York, NY, USA",
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@INCOLLECTION{Swiler2014-ao,
  title     = "Surrogate Models for Mixed {Discrete-Continuous} Variables",
  booktitle = "Constraint Programming and Decision Making",
  author    = "Swiler, Laura P and Hough, Patricia D and Qian, Peter and Xu, Xu
               and Storlie, Curtis and Lee, Herbert",
  editor    = "Ceberio, Martine and Kreinovich, Vladik",
  abstract  = "Large-scale computational models have become common tools for
               analyzing complex man-made systems. However, when coupled with
               optimization or uncertainty quantification methods in order to
               conduct extensive model exploration and analysis, the
               computational expense quickly becomes intractable. Furthermore,
               these models may have both continuous and discrete parameters.
               One common approach to mitigating the computational expense is
               the use of response surface approximations. While well developed
               for models with continuous parameters, they are still new and
               largely untested for models with both continuous and discrete
               parameters. In this work, we describe and investigate the
               performance of three types of response surfaces developed for
               mixed-variable models: Adaptive Component Selection and
               Shrinkage Operator, Treed Gaussian Process, and Gaussian Process
               with Special Correlation Functions. We focus our efforts on test
               problems with a small number of parameters of interest, a
               characteristic of many physics-based engineering models. We
               present the results of our studies and offer some insights
               regarding the performance of each response surface approximation
               method.",
  publisher = "Springer International Publishing",
  pages     = "181--202",
  series    = "Studies in Computational Intelligence",
  year      =  2014,
  keywords  = "BayesOpt;NotRead;TALAF;BayesOpt",
  language  = "en"
}

@ARTICLE{Hoffman2014-dk,
  title     = "Modular mechanisms for Bayesian optimization",
  author    = "Hoffman, M W and Shahriari, B",
  abstract  = "Abstract The design of methods for Bayesian optimization
               involves a great number of choices that are often implicit in
               the overall algorithm design. In this work we argue for a
               modular approach to Bayesian optimization and present a Python
               implementation, pybo, that allows us to easily vary these
               choices. In particular this includes selection of the
               acquisition function, kernel, and hyperpriors as well as
               less-discussed components such ...",
  journal   = "NIPS Workshop on Bayesian Optimization",
  publisher = "Citeseer",
  pages     = "1--5",
  year      =  2014,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@INPROCEEDINGS{Laumanns2002-na,
  title      = "Bayesian Optimization Algorithms for Multi-objective
                Optimization",
  booktitle  = "Parallel Problem Solving from Nature --- {PPSN} {VII}",
  author     = "Laumanns, Marco and Ocenasek, Jiri",
  editor     = "Guerv{\'o}s, Juan Juli{\'a}n Merelo and Adamidis, Panagiotis
                and Beyer, Hans-Georg and Schwefel, Hans-Paul and
                Fern{\'a}ndez-Villaca{\~n}as, Jos{\'e}-Luis",
  abstract   = "In recent years, several researchers have concentrated on using
                probabilistic models in evolutionary algorithms. These
                Estimation Distribution Algorithms (EDA) incorporate methods
                for automated learning of correlations between variables of the
                encoded solutions. The process of sampling new individuals from
                a probabilistic model respects these mutual dependencies such
                that disruption of important building blocks is avoided, in
                comparison with classical recombination operators. The goal of
                this paper is to investigate the usefulness of this concept in
                multi-objective optimization, where the aim is to approximate
                the set of Pareto-optimal solutions. We integrate the model
                building and sampling techniques of a special EDA called
                Bayesian Optimization Algorithm, based on binary decision
                trees, into an evolutionary multi-objective optimizer using a
                special selection scheme. The behavior of the resulting
                Bayesian Multi-objective Optimization Algorithm (BMOA) is
                empirically investigated on the multi-objective knapsack
                problem.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "298--307",
  series     = "Lecture Notes in Computer Science",
  month      =  sep,
  year       =  2002,
  keywords   = "BayesOpt;NotRead;TALAF;BayesOpt",
  language   = "en",
  conference = "International Conference on Parallel Problem Solving from
                Nature"
}

@INPROCEEDINGS{Macdonald2009-yw,
  title     = "Comparison of sampling techniques on the performance of
               {Monte-Carlo} based sensitivity analysis",
  booktitle = "Eleventh International {IBPSA} Conference",
  author    = "Macdonald, Iain A",
  abstract  = "Sensitivity analysis is a key part of a comprehensive energy
               simulation study. Monte-Carlo techniques have been successfully
               applied to many simulation tools. Several sampling techniques
               have been proposed in the literature; however to date there has
               been no comparison of their performance for typical building
               simulation applications. This paper examines the performance of
               simple random, stratified and Latin Hypercube sampling when
               applied to a typical building simulation problem. An integrated
               natural ventilation problem was selected as it has an
               inexpensive calculation time thus allowing multiple sensitivity
               analyses to be undertaken, while being realistic as wind and
               temperature effects are both modeled. The research shows that
               compared to simple random sampling: LHS and stratified sampling
               produce results that are not significantly different (at a 5\%
               level) with increased robustness (less variance in the mean
               prediction). However, it should not be inferred from this that
               fewer simulation runs are required for LHS and stratified
               sampling. Given the results presented here and in previous work
               it would indicate that for practical purposes Monte-Carlo
               uncertainty analysis in typical building simulation applications
               should use about 100 runs and simple random sampling.
               INTRODUCTION The field of sensitivity analysis is becoming",
  pages     = "992--999",
  year      =  2009,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@MISC{Rasmussen2006-rs,
  title    = "{GPML} Toolbox",
  author   = "Rasmussen, Carl Edward and Williams, Christopher K I and {ebrary
              Inc.}",
  abstract = "The GPML toolbox provides a wide range of functionality for
              Gaussian process (GP) inference and prediction. GPs are specified
              by mean and covariance functions; we offer a library of simple
              mean and covariance functions and mechanisms to compose more
              complex ones. Several likelihood functions are supported
              including Gaussian and heavy-tailed for regression as well as
              others suitable for classification. Finally, a range of inference
              methods is provided, including exact and variational inference,
              Expectation Propagation, and Laplace's method dealing with
              non-Gaussian likelihoods and FITC for dealing with large
              regression tasks.",
  journal  = "Adaptive computation and machine learning",
  volume   =  11,
  pages    = "1 online resource xviii, 248 p.",
  month    =  dec,
  year     =  2006,
  keywords = "Gaussian processes Data processing.; Machine learning
              Mathematical models.;BayesOpt;Important;TALAF;Folder -
              NIPS2015;GPs;Machine learning;Variatinal
              Inference;Textbook;BayesOpt;TextBooks;GPs"
}

@ARTICLE{Garg2010-xm,
  title    = "Occam's razor",
  author   = "Garg, A",
  journal  = "A.Word.A.Day",
  pages    = "294--300",
  year     =  2010,
  keywords = "BayesOpt;TALAF;BayesOpt"
}

@PHDTHESIS{Gelbart2015-lg,
  title    = "Constrained Bayesian Optimization and Applications",
  author   = "Gelbart, Michael Adam",
  abstract = "Bayesian optimization is an approach for globally optimizing
              black-box functions that are expensive to evaluate, non-convex,
              and possibly noisy. Recently, Bayesian optimization has been used
              with great effectiveness for applications like tuning the
              hyperparameters of machine learning algorithms and automatic A/B
              testing for websites. This thesis considers Bayesian optimization
              in the presence of black-box constraints. Prior work on
              constrained Bayesian optimization consists of a variety of
              methods that can be used with some efficacy in specific contexts.
              Here, by forming a connection with multi-task Bayesian
              optimization, we formulate a more general class of constrained
              Bayesian optimization problems that we call Bayesian optimization
              with decoupled constraints. In this general framework, the
              objective and constraint functions are divided into tasks that
              can be evaluated independently of each other, and resources with
              which these tasks can be performed. We then present two methods
              for solving problems in this general class. The first method, an
              extension to a constrained variant of expected improvement, is
              fast and straightforward to implement but performs poorly in some
              circumstances and is not sufficiently flexible to address all
              varieties of decoupled problems. The second method, Predictive
              Entropy Search with Constraints (PESC), is highly effective and
              sufficiently flexible to address all problems in the general
              class of decoupled problems without any ad hoc modifications. The
              two weaknesses of PESC are its implementation difficulty and slow
              execution time. We address these issues by, respectively,
              providing a publicly available implementation within the popular
              Bayesian optimization software Spearmint, and developing an
              extension to PESC that achieves greater speed without significant
              performance losses. We demonstrate the effectiveness of these
              methods on real-world machine learning meta-optimization
              problems.",
  year     =  2015,
  keywords = "Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language = "en"
}

@INPROCEEDINGS{Hutter2011-yp,
  title      = "Sequential {Model-Based} Optimization for General Algorithm
                Configuration",
  booktitle  = "Learning and Intelligent Optimization",
  author     = "Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin",
  editor     = "Coello Coello, Carlos A",
  abstract   = "State-of-the-art algorithms for hard computational problems
                often expose many parameters that can be modified to improve
                empirical performance. However, manually exploring the
                resulting combinatorial space of parameter settings is tedious
                and tends to lead to unsatisfactory outcomes. Recently,
                automated approaches for solving this algorithm configuration
                problem have led to substantial improvements in the state of
                the art for solving various problems. One promising approach
                constructs explicit regression models to describe the
                dependence of target algorithm performance on parameter
                settings; however, this approach has so far been limited to the
                optimization of few numerical algorithm parameters on single
                instances. In this paper, we extend this paradigm for the first
                time to general algorithm configuration problems, allowing many
                categorical parameters and optimization for sets of instances.
                We experimentally validate our new algorithm configuration
                procedure by optimizing a local search and a tree search solver
                for the propositional satisfiability problem (SAT), as well as
                the commercial mixed integer programming (MIP) solver CPLEX. In
                these experiments, our procedure yielded state-of-the-art
                performance, and in many cases outperformed the previous best
                configuration approach.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "6683 LNCS",
  pages      = "507--523",
  series     = "Lecture Notes in Computer Science",
  month      =  jan,
  year       =  2011,
  keywords   = "BayesOpt;Important;NotRead;TALAF;BayesOpt",
  language   = "en",
  conference = "International Conference on Learning and Intelligent
                Optimization"
}

@ARTICLE{Shahriari2016-xh,
  title    = "Taking the Human Out of the Loop: A Review of Bayesian
              Optimization",
  author   = "Shahriari, B and Swersky, K and Wang, Z and Adams, R P and de
              Freitas, N",
  abstract = "Big Data applications are typically associated with systems
              involving large numbers of users, massive complex software
              systems, and large-scale heterogeneous computing and storage
              architectures. The construction of such systems involves many
              distributed design choices. The end products (e.g.,
              recommendation systems, medical analysis tools, real-time game
              engines, speech recognizers) thus involve many tunable
              configuration parameters. These parameters are often specified
              and hard-coded into the software by various developers or teams.
              If optimized jointly, these parameters can result in significant
              improvements. Bayesian optimization is a powerful tool for the
              joint optimization of design choices that is gaining great
              popularity in recent years. It promises greater automation so as
              to increase both product quality and human productivity. This
              review paper introduces Bayesian optimization, highlights some of
              its methodological aspects, and showcases a wide range of
              applications.",
  journal  = "Proc. IEEE",
  volume   =  104,
  number   =  1,
  pages    = "148--175",
  month    =  jan,
  year     =  2016,
  keywords = "Bayes methods;Big Data;optimisation;storage allocation;Bayesian
              optimization;Big data application;human productivity;large-scale
              heterogeneous computing;massive complex software system;product
              quality;storage architecture;Bayes methods;Big data;Decision
              making;Design of experiments;Genomes;Linear
              programming;Optimization;Statistical analysis;Decision
              making;decision making;design of experiments;genomic
              medicine;optimization;response surface methodology;statistical
              learning;Acquisition/InfillFxns;BayesOpt;Important;NotRead;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@MISC{Cook2010-zq,
  title        = "Don't invert that matrix",
  booktitle    = "Johndcook.com",
  author       = "Cook, John C",
  abstract     = "There is hardly ever a good reason to invert a matrix. What
                  do you do if you need to solve Ax = b where A is an n x n
                  matrix? Isn't the solution A-1 b? Yes,",
  year         =  2010,
  howpublished = "\url{http://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/}",
  keywords     = "Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@PHDTHESIS{Lizotte2008-ou,
  title     = "Practical Bayesian Optimization",
  author    = "Lizotte, Daniel James",
  abstract  = "Global optimization of non-convex functions over real vector
               spaces is a problem of widespread theoretical and practical
               interest............ Examples: AIBO Gait parameters Stereo
               Camera Parameters",
  publisher = "University of Alberta",
  year      =  2008,
  address   = "Edmonton, Alta., Canada",
  keywords  = "Bayes; Kernels; Learning; Optimization; Parameter optimization;
               local
               search;Acquisition/InfillFxns;BayesOpt;Important;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}
@INCOLLECTION{Ginsbourger2010-wo,
  title     = "Kriging Is {Well-Suited} to Parallelize Optimization",
  booktitle = "Computational Intelligence in Expensive Optimization Problems",
  author    = "Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent",
  editor    = "Tenne, Yoel and Goh, Chi-Keong",
  abstract  = "The optimization of expensive-to-evaluate functions generally
               relies on metamodel-based exploration strategies. Many
               deterministic global optimization algorithms used in the field
               of computer experiments are based on Kriging (Gaussian process
               regression). Starting with a spatial predictor including a
               measure of uncertainty, they proceed by iteratively choosing the
               point maximizing a criterion which is a compromise between
               predicted performance and uncertainty. Distributing the
               evaluation of such numerically expensive objective functions on
               many processors is an appealing idea. Here we investigate a
               multi-points optimization criterion, the multipoints expected
               improvement (\textbackslash(q-\{\textbackslashmathbb
               E\}I\textbackslash)), aimed at choosing several points at the
               same time. An analytical expression of the
               \textbackslash(q-\{\textbackslashmathbb E\}I\textbackslash) is
               given when q = 2, and a consistent statistical estimate is given
               for the general case. We then propose two classes of heuristic
               strategies meant to approximately optimize the
               \textbackslash(q-\{\textbackslashmathbb E\}I\textbackslash), and
               apply them to the classical Branin-Hoo test-case function. It is
               finally demonstrated within the covered example that the latter
               strategies perform as good as the best Latin Hypercubes and
               Uniform Designs ever found by simulation (2000 designs drawn at
               random for every q $\in$ [1,10]).",
  publisher = "Springer Berlin Heidelberg",
  pages     = "131--162",
  series    = "Adaptation Learning and Optimization",
  year      =  2010,
  keywords  = "Appl.Mathematics/Computational Methods of Enginee; Applications
               of Mathematics; Artificial Intelligence (incl.
               Robotics);Acquisition/InfillFxns;BayesOpt;TALAF;qEI;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language  = "en"
}

@ARTICLE{Goldberg2000-zp,
  title    = "Bayesian Optimization Algorithm, Population Sizing, and Time to
              Convergence Martin Pelikan, David. E. Goldberg, and Erick
              {Cantu-Paz}",
  author   = "Goldberg, David E and Pelikan, Martin",
  journal  = "Population",
  number   =  2000001,
  pages    = "275--282",
  year     =  2000,
  keywords = "BayesOpt; NotRead; TALAF;BayesOpt;NotRead;TALAF;BayesOpt"
}

@ARTICLE{Hensman2013-ai,
  title         = "Gaussian Processes for Big Data",
  author        = "Hensman, James and Fusi, Nicolo and Lawrence, Neil D",
  abstract      = "We introduce stochastic variational inference for Gaussian
                   process models. This enables the application of Gaussian
                   process (GP) models to data sets containing millions of data
                   points. We show how GPs can be vari- ationally decomposed to
                   depend on a set of globally relevant inducing variables
                   which factorize the model in the necessary manner to perform
                   variational inference. Our ap- proach is readily extended to
                   models with non-Gaussian likelihoods and latent variable
                   models based around Gaussian processes. We demonstrate the
                   approach on a simple toy problem and two real world data
                   sets.",
  pages         = "282--290",
  month         =  sep,
  year          =  2013,
  keywords      = "BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1309.6835"
}

@ARTICLE{Brochu2010-bp,
  title         = "Portfolio Allocation for Bayesian Optimization",
  author        = "Brochu, Eric and Hoffman, Matthew W and de Freitas, Nando",
  abstract      = "Bayesian optimization with Gaussian processes has become an
                   increasingly popular tool in the machine learning community.
                   It is efficient and can be used when very little is known
                   about the objective function, making it popular in expensive
                   black-box optimization scenarios. It uses Bayesian methods
                   to sample the objective efficiently using an acquisition
                   function which incorporates the model's estimate of the
                   objective and the uncertainty at any given point. However,
                   there are several different parameterized acquisition
                   functions in the literature, and it is often unclear which
                   one to use. Instead of using a single acquisition function,
                   we adopt a portfolio of acquisition functions governed by an
                   online multi-armed bandit strategy. We propose several
                   portfolio strategies, the best of which we call GP-Hedge,
                   and show that this method outperforms the best individual
                   acquisition function. We also provide a theoretical bound on
                   the algorithm's performance.",
  pages         = "327--336",
  month         =  sep,
  year          =  2010,
  keywords      = "BayesOpt; TALAF;BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1009.5419"
}

@INPROCEEDINGS{Ponweiser2008-zf,
  title     = "Clustered multiple generalized expected improvement: A novel
               infill sampling criterion for surrogate models",
  booktitle = "2008 {IEEE} Congress on Evolutionary Computation ({IEEE} World
               Congress on Computational Intelligence)",
  author    = "Ponweiser, W and Wagner, T and Vincze, M",
  abstract  = "Surrogate model-based optimization is a well-known technique for
               optimizing expensive black-box functions. By applying this
               function approximation, the number of real problem evaluations
               can be reduced because the optimization is performed on the
               model. In this case two contradictory targets have to be
               achieved: increasing global model accuracy and exploiting
               potentially optimal areas. The key to these targets is the
               criterion for selecting the next point, which is then evaluated
               on the expensive black-box function - the dasiainfill sampling
               criterionpsila. Therefore, a novel approach - the dasiaClustered
               Multiple Generalized Expected Improvementpsila (CMGEI) - is
               introduced and motivated by an empirical study. Furthermore,
               experiments benchmarking its performance compared to the state
               of the art are presented.",
  pages     = "3515--3522",
  month     =  jun,
  year      =  2008,
  keywords  = "function approximation;optimisation;clustered multiple
               generalized expected improvement;expensive black-box
               functions;function approximation;infill sampling
               criterion;surrogate model-based optimization;Accuracy;Distance
               measurement;Function approximation;Mathematical
               model;Optimization;Simulated
               annealing;Uncertainty;Acquisition/InfillFxns;BayesOpt;Mathematical
               model;neural\_networks;Optimization
               methods;TALAF;optimisation;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Jones1993-ua,
  title     = "Lipschitzian optimization without the Lipschitz constant",
  author    = "Jones, D R and Perttunen, C D and Stuckman, B E",
  abstract  = "We present a new algorithm for finding the global minimum of a
               multivariate function subject to simple bounds. The algorithm is
               a modification of the standard Lipschitzian approach that
               eliminates the need to specify a Lipschitz constant. This is
               done by carrying out simultaneous searches using all possible
               constants from zero to infinity. On nine standard test
               functions, the new algorithm converges in fewer function
               evaluations than most competing methods.The motivation for the
               new algorithm stems from a different way of looking at the
               Lipschitz constant. In particular, the Lipschitz constant is
               viewed as a weighting parameter that indicates how much emphasis
               to place on global versus local search. In standard Lipschitzian
               methods, this constant is usually large because it must equal or
               exceed the maximum rate of change of the objective function. As
               a result, these methods place a high emphasis on global search
               and exhibit slow convergence. In contrast, the new algorithm
               carries out simultaneous searches using all possible constants,
               and therefore operates at both the global and local level. Once
               the global part of the algorithm finds the basin of convergence
               of the optimum, the local part of the algorithm quickly and
               automatically exploits it. This accounts for the fast
               convergence of the new algorithm on the test functions.",
  journal   = "J. Optim. Theory Appl.",
  publisher = "Kluwer Academic Publishers-Plenum Publishers",
  volume    =  79,
  number    =  1,
  pages     = "157--181",
  month     =  oct,
  year      =  1993,
  keywords  = "1staff research scientist; covering; development center; general
               motors research and; global optimization; lipschitzian
               optimization; space; space partitioning;
               warren;BayesOpt;DIRECT;Important;TALAF;BayesOpt",
  language  = "en"
}

@ARTICLE{Alvarez2011-wg,
  title     = "Computationally Efficient Convolved Multiple Output Gaussian
               Processes",
  author    = "{\'A}lvarez, Mauricio A and Lawrence, Neil D",
  abstract  = "Recently there has been an increasing interest in regression
               methods that deal with multiple outputs. This has been motivated
               partly by frameworks like multitask learning, multisensor
               networks or structured output data. From a Gaussian processes
               perspective, the problem reduces to specifying an appropriate
               covariance function that, whilst being positive semi-definite,
               captures the dependencies between all the data points and across
               all the outputs. One approach to account for non-trivial
               correlations between outputs employs convolution processes.
               Under a latent function interpretation of the convolution
               transform we establish dependencies between output variables.
               The main drawbacks of this approach are the associated
               computational and storage demands. In this paper we address
               these issues. We present different efficient approximations for
               dependent output Gaussian processes constructed through the
               convolution formalism. We exploit the conditional independencies
               present naturally in the model. This leads to a form of the
               covariance similar in spirit to the so called PITC and FITC
               approximations for a single output. We show experimental results
               with synthetic and real data, in particular, we show results in
               school exams score prediction, pollution prediction and gene
               expression data. \copyright{} 2011 Mauricio A. {\'A}lvarez and
               Neil D. Lawrence.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  12,
  pages     = "1459--1500",
  month     =  jul,
  year      =  2011,
  keywords  = "convolution processes; efficient approximations; gaussian
               processes; ing; multitask learn-; multivariate processes;
               structured outputs;BayesOpt;TALAF;BayesOpt"
}

@MISC{noauthor_undated-cw,
  title        = "machine learning - How can you detect if a Gaussian process
                  is over-fitting? - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/47174/how-can-you-detect-if-a-gaussian-process-is-over-fitting}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Wang2014-sv,
  title         = "Theoretical Analysis of Bayesian Optimisation with Unknown
                   Gaussian Process {Hyper-Parameters}",
  author        = "Wang, Ziyu and de Freitas, Nando",
  abstract      = "Bayesian optimisation has gained great popularity as a tool
                   for optimising the parameters of machine learning algorithms
                   and models. Somewhat ironically, setting up the
                   hyper-parameters of Bayesian optimisation methods is
                   notoriously hard. While reasonable practical solutions have
                   been advanced, they can often fail to find the best optima.
                   Surprisingly, there is little theoretical analysis of this
                   crucial problem in the literature. To address this, we
                   derive a cumulative regret bound for Bayesian optimisation
                   with Gaussian processes and unknown kernel hyper-parameters
                   in the stochastic setting. The bound, which applies to the
                   expected improvement acquisition function and sub-Gaussian
                   observation noise, provides us with guidelines on how to
                   design hyper-parameter estimation methods. A simple
                   simulation demonstrates the importance of following these
                   guidelines.",
  pages         = "1--16",
  month         =  jun,
  year          =  2014,
  keywords      = "BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1406.7758"
}

@ARTICLE{Mahendran2011-tx,
  title         = "Bayesian Optimization for Adaptive {MCMC}",
  author        = "Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and de
                   Freitas, Nando",
  abstract      = "This paper proposes a new randomized strategy for adaptive
                   MCMC using Bayesian optimization. This approach applies to
                   non-differentiable objective functions and trades off
                   exploration and exploitation to reduce the number of
                   potentially costly objective function evaluations. We
                   demonstrate the strategy in the complex setting of sampling
                   from constrained, discrete and densely connected
                   probabilistic graphical models where, for each variation of
                   the problem, one needs to adjust the parameters of the
                   proposal mechanism automatically to ensure efficient mixing
                   of the Markov chains.",
  pages         = "152",
  month         =  oct,
  year          =  2011,
  keywords      = "BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1110.6497"
}

@INPROCEEDINGS{Bergstra2013-je,
  title     = "Making a Science of Model Search: Hyperparameter Optimization in
               Hundreds of Dimensions for Vision Architectures",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Bergstra, James and Yamins, Daniel and Cox, David",
  abstract  = "Many computer vision algorithms depend on configuration settings
               that are typically hand-tuned in the course of evaluating the
               algorithm for a particular data set. While such parameter tuning
               is often presented as being incidental to the algorithm,
               correctly setting these parameter choices is frequently critical
               to realizing a method's full potential. Compounding matters,
               these parameters often must be re-tuned when the algorithm is
               applied to a new problem domain, and the tuning process itself
               often depends on personal experience and intuition in ways that
               are hard to quantify or describe. Since the performance of a
               given technique depends on both the fundamental quality of the
               algorithm and the details of its tuning, it is sometimes
               difficult to know whether a given technique is genuinely better,
               or simply better tuned. In this work, we propose a meta-modeling
               approach to support automated hyperparameter optimization, with
               the goal of providing practical tools that replace hand-tuning
               with a reproducible and unbiased optimization process. Our
               approach is to expose the underlying expression graph of how a
               performance metric (e.g. classification accuracy on validation
               examples) is computed from hyperparameters that govern not only
               how individual processing steps are applied, but even which
               processing steps are included. A hyperparameter optimization
               algorithm transforms this graph into a program for optimizing
               that performance metric. Our approach yields state of the art
               results on three disparate computer vision problems: a
               face-matching verification task (LFW), a face identification
               task (PubFig83) and an object recognition task (CIFAR-10), using
               a single broad class of feed-forward vision architectures.",
  pages     = "115--123",
  year      =  2013,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@INCOLLECTION{Bergstra2011-mp,
  title     = "Algorithms for {Hyper-Parameter} Optimization",
  booktitle = "Advances in Neural Information Processing Systems 24",
  author    = "Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and
               K{\'e}gl, Bal{\'a}zs",
  editor    = "Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F
               and Weinberger, K Q",
  abstract  = "Several recent advances to the state of the art in image
               classification benchmarks have come from better configurations
               of existing techniques rather than novel ap- proaches to feature
               learning. Traditionally, hyper-parameter optimization has been
               the job of humans because they can be very efficient in regimes
               where only a few trials are possible. Presently, computer
               clusters and GPU processors make it pos- sible to run more
               trials and we show that algorithmic approaches can find better
               results. We present hyper-parameter optimization results on
               tasks of training neu- ral networks and deep belief networks
               (DBNs). We optimize hyper-parameters using random search and two
               new greedy sequential methods based on the ex- pected
               improvement criterion. Random search has been shown to be
               sufficiently efficient for learning neural networks for several
               datasets, but we show it is unreli- able for training DBNs. The
               sequential algorithms are applied to the most difficult DBN
               learning problems from [1] and find significantly better results
               than the best previously reported. This work contributes novel
               techniques for making response surface models P(y|x) in which
               many elements of hyper-parameter assignment (x) are known to be
               irrelevant given particular values of other elements.",
  publisher = "Curran Associates, Inc.",
  pages     = "2546--2554",
  year      =  2011,
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Gopalan2013-vt,
  title         = "Thompson Sampling for Complex Bandit Problems",
  author        = "Gopalan, Aditya and Mannor, Shie and Mansour, Yishay",
  editor        = "Sammut, Claude and Webb, Geoffrey I",
  abstract      = "We consider stochastic multi-armed bandit problems with
                   complex actions over a set of basic arms, where the decision
                   maker plays a complex action rather than a basic arm in each
                   round. The reward of the complex action is some function of
                   the basic arms' rewards, and the feedback observed may not
                   necessarily be the reward per-arm. For instance, when the
                   complex actions are subsets of the arms, we may only observe
                   the maximum reward over the chosen subset. Thus, feedback
                   across complex actions may be coupled due to the nature of
                   the reward function. We prove a frequentist regret bound for
                   Thompson sampling in a very general setting involving
                   parameter, action and observation spaces and a likelihood
                   function over them. The bound holds for discretely-supported
                   priors over the parameter space and without additional
                   structural properties such as closed-form posteriors,
                   conjugate prior structure or independence across arms. The
                   regret bound scales logarithmically with time but, more
                   importantly, with an improved constant that non-trivially
                   captures the coupling across complex actions due to the
                   structure of the rewards. As applications, we derive
                   improved regret bounds for classes of complex bandit
                   problems involving selecting subsets of arms, including the
                   first nontrivial regret bounds for nonlinear MAX reward
                   feedback from subsets.",
  pages         = "1--9",
  month         =  nov,
  year          =  2013,
  keywords      = "bandit problem;Acquisition/InfillFxns;BayesOpt;Machine
                   learning;NotRead;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1311.0466"
}

@ARTICLE{Qian2008-ip,
  title    = "Gaussian Process Models for Computer Experiments With Qualitative
              and Quantitative Factors",
  author   = "Qian, Peter Z G and Wu, Huaiqing and Wu, C F Jeff",
  abstract = "Modeling experiments with qualitative and quantitative factors is
              an important issue in computer modeling. We propose a framework
              for building Gaussian process models that incorporate both types
              of factors. The key to the development of these new models is an
              approach for constructing correlation functions with qualitative
              and quantitative factors. An iterative estimation procedure is
              developed for the proposed models. Modern optimization techniques
              are used in the estimation to ensure the validity of the
              constructed correlation functions. The proposed method is
              illustrated with an example involving a known function and a real
              example for modeling the thermal distribution of a data center.",
  journal  = "Technometrics",
  volume   =  50,
  number   =  3,
  pages    = "383--396",
  year     =  2008,
  keywords = "cokriging; design of experiments; kriging; multivariate gaussian
              processes; semi-definite programming;BayesOpt;TALAF;BayesOpt"
}

@MISC{Jones1998-tz,
  title    = "Efficient Global Optimization of {ExpensiveBlack-Box} Functions",
  author   = "Jones, Donald R and Schonlau, Matthias and Welch, William J",
  abstract = "Inmany engineering optimization problems, the number of function
              evaluations is severely limited by time or cost. These problems
              pose a special challenge to the field of global optimization,
              since existing methods often require more function evaluations
              than can be comfortably afforded. One way to address this
              challenge is to fit response surfaces to data collected by
              evaluating the objective and constraint functions at a few
              points. These surfaces can then be used for visualization,
              tradeoff analysis, and optimization. In this paper, we introduce
              the reader to a response surface methodology that is especially
              good at modeling the nonlinear, multimodal functions that often
              occur in engineering. We then show how these approximating
              functions can be used to construct an efficient global
              optimization algorithm with a credible stopping rule. The key to
              using response surfaces for global optimization lies in balancing
              the need to exploit the approximating surface (by sampling where
              it is minimized) with the need to improve the approximation (by
              sampling where prediction error may be high). Striking this
              balance requires solving certain auxiliary problems which have
              previously been considered intractable, but we show how these
              computational obstacles can be overcome.",
  journal  = "Journal of Global Optimization",
  volume   =  13,
  number   =  4,
  pages    = "455--492",
  year     =  1998,
  keywords = "bayesian global optimization; kriging; process; random function;
              response surface; stochastic;
              visualization;Acquisition/InfillFxns;BayesOpt;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@INPROCEEDINGS{Contal2013-ig,
  title      = "Parallel Gaussian Process Optimization with Upper Confidence
                Bound and Pure Exploration",
  booktitle  = "Machine Learning and Knowledge Discovery in Databases",
  author     = "Contal, Emile and Buffoni, David and Robicquet, Alexandre and
                Vayatis, Nicolas",
  editor     = "Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried
                and {\v Z}elezn{\'y}, Filip",
  abstract   = "In this paper, we consider the challenge of maximizing an
                unknown function f for which evaluations are noisy and are
                acquired with high cost. An iterative procedure uses the
                previous measures to actively select the next estimation of f
                which is predicted to be the most useful. We focus on the case
                where the function can be evaluated in parallel with batches of
                fixed size and analyze the benefit compared to the purely
                sequential procedure in terms of cumulative regret. We
                introduce the Gaussian Process Upper Confidence Bound and Pure
                Exploration algorithm (GP-UCB-PE) which combines the UCB
                strategy and Pure Exploration in the same batch of evaluations
                along the parallel iterations. We prove theoretical upper
                bounds on the regret with batches of size K for this procedure
                which show the improvement of the order of
                \textbackslash(\textbackslashsqrt\{K\}\textbackslash) for fixed
                iteration cost over purely sequential versions. Moreover, the
                multiplicative constants involved have the property of being
                dimension-free. We also confirm empirically the efficiency of
                GP-UCB-PE on real and synthetic problems compared to
                state-of-the-art competitors.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "8188 LNAI",
  pages      = "225--240",
  series     = "Lecture Notes in Computer Science",
  month      =  sep,
  year       =  2013,
  keywords   = "BayesOpt;TALAF;batch
                selection;BayesOpt;BayesOpt/Acquisition/InfillFxns",
  language   = "en",
  conference = "Joint European Conference on Machine Learning and Knowledge
                Discovery in Databases"
}

@ARTICLE{Shahriari2014-pu,
  title         = "An Entropy Search Portfolio for Bayesian Optimization",
  author        = "Shahriari, Bobak and Wang, Ziyu and Hoffman, Matthew W and
                   Bouchard-C{\^o}t{\'e}, Alexandre and de Freitas, Nando",
  abstract      = "Bayesian optimization is a sample-efficient method for
                   black-box global optimization. How- ever, the performance of
                   a Bayesian optimization method very much depends on its
                   exploration strategy, i.e. the choice of acquisition
                   function, and it is not clear a priori which choice will
                   result in superior performance. While portfolio methods
                   provide an effective, principled way of combining a
                   collection of acquisition functions, they are often based on
                   measures of past performance which can be misleading. To
                   address this issue, we introduce the Entropy Search
                   Portfolio (ESP): a novel approach to portfolio construction
                   which is motivated by information theoretic considerations.
                   We show that ESP outperforms existing portfolio methods on
                   several real and synthetic problems, including
                   geostatistical datasets and simulated control tasks. We not
                   only show that ESP is able to offer performance as good as
                   the best, but unknown, acquisition function, but
                   surprisingly it often gives better performance. Finally,
                   over a wide range of conditions we find that ESP is robust
                   to the inclusion of poor acquisition functions.",
  pages         = "10",
  month         =  jun,
  year          =  2014,
  keywords      = "BayesOpt;TALAF;BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1406.4625"
}

@ARTICLE{Ebisu1975-qe,
  title    = "Isolation and purification of Flavobacterium
              alpha-1,3-glucanase-hydrolyzing, insoluble, sticky glucan of
              Streptococcus mutans",
  author   = "Ebisu, S and Kato, K and Kotani, S and Misaki, A",
  abstract = "Studies were made on the physical and chemical properties of
              polysaccharides synthesized by cell-free extracts of
              Streptococcus mutans, Streptococcus sanguis, and Streptococcus
              sp. and their susceptibilities to dextranases. Among the
              polysaccharides examined, insoluble glucans were rather resistant
              to available dextranase preparations, and the insoluble, sticky
              glucan produced by S. mutans OMZ 176, which could be important in
              formation of dental plaques, was the most resistant. By
              enrichment culture of soil specimens, using OMZ 176 glucans as
              the sole carbon source, an organism was isolated that produced
              colonies surrounded by a clear lytic zone on opaque agar plates
              containing the OMZ 176 glucan. The organism was identified as a
              strain of Flavobacterium and named the Ek-14 bacterium. EK-14
              bacterium was grown in Trypticase soy broth, and an enzyme
              capable of hydrolyzing the OMZ 176 glucan was concentrated from
              the culture supernatant and purified by negative adsorption on a
              diethylaminoethyl-cellulose (DE-32) column and gradient elution
              chromatography with a carboxymethyl-cellulose (CM-32) column. The
              enzyme was a basic protein with an isoelectric point of pH 8.5
              and molecular weight of 65,000. Its optimum pH was 6.3 and its
              optimal temperature was 42 C. The purified enzyme released 11\%
              of the total glucose residues of the OMZ 176 glucan as reducing
              sugars and solubilized about half of the substrate glucan. The
              products were found to be isomaltose, nigerose, and nigerotriose,
              with some oligosaccharides. The purified enzyme split the
              alpha-1,3-glucan endolytically and was inactive toward glucans
              containing alpha-1,6, alpha-1,4, beta-1,3, beta-1,4, and/or
              beta-1,6 bonds as the main linkages.",
  journal  = "J. Bacteriol.",
  volume   =  124,
  number   =  3,
  pages    = "1489--1501",
  month    =  dec,
  year     =  1975,
  keywords = "classification; cross-validation bootstrap; prediction
              rule;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters",
  language = "en"
}

@INPROCEEDINGS{Domingos1998-xw,
  title     = "A {Process-Oriented} Heuristic for Model Selection",
  booktitle = "{ICML}",
  author    = "Domingos, Pedro M",
  abstract  = "Current methods to avoid overfitting are either data-oriented
               (using separate data for validation) or representation-oriented
               (penalizing complexity in the model). This paper proposes
               process-oriented evaluation, where a model's expected
               generalization error is computed as a function of the search
               process that led to it. The paper develops the necessary
               theoretical framework, and applies it to one type of learning:
               rule induction. A process-oriented version of the CN2 rule
               learner is empirically compared with the default CN2. The
               process-oriented version is more accurate in a large majority of
               the datasets, with high significance, and also produces simpler
               models. Experiments in artificial domains suggest that
               processoriented evaluation is particularly useful in
               high-dimensional domains. 1 INTRODUCTION Overfitting avoidance
               is often considered the central problem of machine learning
               (e.g., (Cheeseman \& Oldford, 1994)). If a learner is
               sufficiently powerful, it must guard against selec...",
  pages     = "127--135",
  year      =  1998,
  keywords  = "imported;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@INPROCEEDINGS{Feurer2014-xr,
  title     = "Using meta-learning to initialize Bayesian optimization of
               hyperparameters",
  booktitle = "{CEUR} Workshop Proceedings",
  author    = "Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank",
  abstract  = "Model selection and hyperparameter optimization is cru-cial in
               applying machine learning to a novel dataset. Recently, a
               sub-community of machine learning has focused on solving this
               prob-lem with Sequential Model-based Bayesian Optimization
               (SMBO), demonstrating substantial successes in many
               applications. However, for expensive algorithms the
               computational overhead of hyperpa-rameter optimization can still
               be prohibitive. In this paper we ex-plore the possibility of
               speeding up SMBO by transferring knowl-edge from previous
               optimization runs on similar datasets; specifi-cally, we propose
               to initialize SMBO with a small number of config-urations
               suggested by a metalearning procedure. The resulting simple
               MI-SMBO technique can be trivially applied to any SMBO method,
               allowing us to perform experiments on two quite different SMBO
               methods with complementary strengths applied to optimize two
               ma-chine learning frameworks on 57 classification datasets. We
               find that our initialization procedure mildly improves the state
               of the art in low-dimensional hyperparameter optimization and
               substantially im-proves the state of the art in the more complex
               problem of combined model selection and hyperparameter
               optimization.",
  volume    =  1201,
  pages     = "3--10",
  year      =  2014,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Rodriguez2010-ss,
  title    = "Sensitivity analysis of kappa-fold cross validation in prediction
              error estimation",
  author   = "Rodr{\'\i}guez, Juan Diego and P{\'e}rez, Aritz and Lozano, Jose
              Antonio",
  abstract = "In the machine learning field, the performance of a classifier is
              usually measured in terms of prediction error. In most real-world
              problems, the error cannot be exactly calculated and it must be
              estimated. Therefore, it is important to choose an appropriate
              estimator of the error. This paper analyzes the statistical
              properties, bias and variance, of the kappa-fold cross-validation
              classification error estimator (kappa-cv). Our main contribution
              is a novel theoretical decomposition of the variance of the
              kappa-cv considering its sources of variance: sensitivity to
              changes in the training set and sensitivity to changes in the
              folds. The paper also compares the bias and variance of the
              estimator for different values of kappa. The experimental study
              has been performed in artificial domains because they allow the
              exact computation of the implied quantities and we can rigorously
              specify the conditions of experimentation. The experimentation
              has been performed for two classifiers (naive Bayes and nearest
              neighbor), different numbers of folds, sample sizes, and training
              sets coming from assorted probability distributions. We conclude
              by including some practical recommendation on the use of
              kappa-fold cross validation.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  32,
  number   =  3,
  pages    = "569--575",
  month    =  mar,
  year     =  2010,
  keywords = "Bayes methods;BayesOpt;GPs;Machine
              learning;NotRead;OptimizingHyperparameters;STATISTICAL
              ANALYSIS;TALAF;estimation theory;learning (artificial
              intelligence);pattern classification;probability;sensitivity
              analysis;BayesOpt;OptimizingHyperparameters",
  language = "en"
}

@INPROCEEDINGS{Ester1996-xn,
  title     = "A density-based algorithm for discovering clusters in large
               spatial databases with noise",
  booktitle = "Proceedings of the Second International Conference on Knowledge
               Discovery and Data Mining ({KDD-96)}, Portland, Oregon, {USA}",
  author    = "Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and
               Xu, Xiaowei",
  abstract  = "Clustering algorithms are attractive for the task of class
               identification in spatial databases. However, the application to
               large spatial databases rises the following requirements for
               clustering algorithms: minimal requirements of domain knowledge
               to determine the input parameters, discovery of clusters with
               arbitrary shape and good efficiency on large databases. The
               well-known clustering algorithms offer no solution to the
               combination of these requirements. In this paper, we present the
               new clustering algorithm DBSCAN relying on a density-based
               notion of clusters which is designed to discover clusters of
               arbitrary shape. DBSCAN requires only one input parameter and
               supports the user in determining an appropriate value for it. We
               performed an experimental evaluation of the effectiveness and
               efficiency of DBSCAN using synthetic data and real data of the
               SEQUOIA 2000 benchmark. The results of our experiments
               demonstrate that (1) DBSCAN is significantly more effective in
               discovering clusters of arbitrary shape than the well-known
               algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a
               factor of more than 100 in terms of efficiency.",
  publisher = "AAAI Press",
  pages     = "226--231",
  year      =  1996,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@INCOLLECTION{Golovin2010-sr,
  title     = "{Near-Optimal} Bayesian Active Learning with Noisy Observations",
  booktitle = "Advances in Neural Information Processing Systems 23",
  author    = "Golovin, Daniel and Krause, Andreas and Ray, Debajyoti",
  editor    = "Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel,
               R S and Culotta, A",
  abstract  = "We tackle the fundamental problem of Bayesian active learning
               with noise, where we need to adaptively select from a number of
               expensive tests in order to identify an unknown hypothesis
               sampled from a known prior distribution. In the case of
               noise-free observations, a greedy algorithm called generalized
               binary search (GBS) is known to perform near-optimally. We show
               that if the observations are noisy, perhaps surprisingly, GBS
               can perform very poorly. We develop EC2, a novel, greedy active
               learning algorithm and prove that it is competitive with the
               optimal policy, thus obtaining the first competitiveness
               guarantees for Bayesian active learning with noisy observations.
               Our bounds rely on a recently discovered diminishing returns
               property called adaptive submodularity, generalizing the
               classical notion of submodular set functions to adaptive
               policies. Our results hold even if the tests have non--uniform
               cost and their noise is correlated. We also propose EffECXtive,
               a particularly fast approximation of EC2, and evaluate it on a
               Bayesian experimental design problem involving human subjects,
               intended to tease apart competing economic theories of how
               people make decisions under uncertainty.",
  publisher = "Curran Associates, Inc.",
  pages     = "766--774",
  year      =  2010,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@INCOLLECTION{Rao2006-vr,
  title     = "On the Dangers of {Cross-Validation}. An Experimental Evaluation",
  booktitle = "Proceedings of the 2008 {SIAM} International Conference on Data
               Mining",
  author    = "Rao, R Bharat and Fung, Glenn and Rosales, Romer",
  abstract  = "Abstract Cross validation allows models to be tested using the
               full training set by means of repeated resampling; thus,
               maximizing the total number of points used for testing and
               potentially, helping to protect against overfitting.
               Improvements in computational power, recent reductions in the
               (computational) cost of classification algorithms, and the
               development of closed-form solutions (for performing cross
               validation in certain classes of learning algorithms) makes it
               possible to test thousand or millions of variants of learning
               models on the data. Thus, it is now possible to calculate cross
               validation performance on a much larger number of tuned models
               than would have been possible otherwise. However, we empirically
               show how under such large number of models the risk for
               overfitting increases and the performance estimated by cross
               validation is no longer an effective estimate of generalization;
               hence, this paper provides an empirical reminder of the dangers
               of cross validation. We use a closed-form solution that makes
               this evaluation possible for the cross validation problem of
               interest. In addition, through extensive experiments we expose
               and discuss the effects of the overuse/misuse of cross
               validation in various aspects, including model selection,
               feature selection, and data dimensionality. This is illustrated
               on synthetic, benchmark, and real-world data sets.",
  publisher = "SIAM",
  pages     = "588--596",
  year      =  2006,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Snelson2004-kh,
  title     = "Warped gaussian processes",
  author    = "Snelson, E and Rasmussen, C E and {others}",
  abstract  = "Abstract We generalise the Gaussian process (GP) framework for
               regression by learning a nonlinear transformation of the GP
               outputs. This allows for non-Gaussian processes and non-
               Gaussian noise. The learning algorithm chooses a nonlinear
               transformation such that",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "books.google.com",
  volume    =  16,
  pages     = "337--344",
  year      =  2004,
  keywords  = "Computational; Information-Theoretic Learning with;
               Learning/Statistics \& Optimisation; Theory \&
               Algorithms;BayesOpt;Important;NotRead;TALAF;BayesOpt"
}

@INPROCEEDINGS{Gardner2014-qw,
  title     = "Bayesian Optimization with Inequality Constraints",
  booktitle = "Proceedings of The 31st International Conference on Machine
               Learning",
  author    = "Gardner, Jacob and Kusner, Matt and {Zhixiang} and Weinberger,
               Kilian and Cunningham, John",
  abstract  = "Bayesian optimization is a powerful framework for minimizing
               expensive objective functions while using very few function
               evaluations. It has been successfully applied to a variety of
               problems, including hyperparameter tuning and experimental
               design. However, this framework has not been extended to the
               inequality-constrained optimization setting, particularly the
               setting in which evaluating feasibility is just as expensive as
               evaluating the objective. Here we present constrained Bayesian
               optimization, which places a prior distribution on both the
               objective and the constraint functions. We evaluate our method
               on simulated and real data, demonstrating that constrained
               Bayesian optimization can quickly find optimal and feasible
               points, even when small feasible regions cause standard methods
               to fail.",
  volume    =  32,
  pages     = "937--945",
  year      =  2014,
  keywords  = "BayesOpt;TALAF;BayesOpt"
}

@INPROCEEDINGS{Wang2013-yv,
  title     = "Bayesian Optimization in High Dimensions via Random Embeddings",
  booktitle = "Proceedings of the {Twenty-Third} International Joint Conference
               on Artificial Intelligence",
  author    = "Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson,
               David and De Freitas, Nando",
  abstract  = "Bayesian optimization techniques have been successfully applied
               to robotics, planning, sensor placement, recommendation,
               advertising, intelligent user interfaces and automatic algorithm
               configuration. Despite these successes, the approach is
               restricted to problems of moderate dimension, and several
               workshops on Bayesian optimization have identified its scaling
               to high-dimensions as one of the holy grails of the field. In
               this paper, we introduce a novel random embedding idea to attack
               this problem. The resulting Random EMbedding Bayesian
               Optimization (REMBO) algorithm is very simple, has important
               invariance properties, and applies to domains with both
               categorical and continuous variables. We present a thorough
               theoretical analysis of REMBO, including regret bounds that only
               depend on the problem's intrinsic dimensionality. Empirical
               results confirm that REMBO can effectively solve problems with
               billions of dimensions, provided the intrinsic dimensionality is
               low. They also show that REMBO achieves state-of-the-art
               performance in optimizing the 47 discrete parameters of a
               popular mixed integer linear programming solver.",
  publisher = "AAAI Press",
  pages     = "1778--1784",
  series    = "IJCAI '13",
  year      =  2013,
  address   = "Beijing, China",
  keywords  = "BayesOpt;Important;TALAF;BayesOpt"
}

@ARTICLE{Cawley2007-he,
  title     = "Preventing {Over-Fitting} During Model Selection via Bayesian
               Regularisation of the {Hyper-Parameters}",
  author    = "Cawley, Gavin C and Talbot, Nicola L C",
  abstract  = "While the model parameters of a kernel machine are typically
               given by the solution of a convex optimisation problem, with a
               single global optimum, the selection of good values for the
               regularisation and kernel parameters is much less
               straightforward. Fortunately the leave-one-out cross-validation
               procedure can be performed or a least approximated very
               efficiently in closed form for a wide variety of kernel learning
               methods, providing a convenient means for model selection.
               Leave-one-out cross-validation based estimates of performance,
               however, generally exhibit a relatively high variance and are
               therefore prone to over-fitting. In this paper, we investigate
               the novel use of Bayesian regularisation at the second level of
               inference, adding a regularisation term to the model selection
               criterion corresponding to a prior over the hyper-parameter
               values, where the additional regularisation parameters are
               integrated out analytically. Results obtained on a suite of
               thirteen real-world and synthetic benchmark data sets clearly
               demonstrate the benefit of this approach.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  8,
  pages     = "841--861",
  month     =  may,
  year      =  2007,
  keywords  = "bayesian regularisation; kernel methods; model
               selection;BayesOpt;GPs;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@ARTICLE{Girosi1995-td,
  title    = "Regularization Theory and Neural Networks Architectures",
  author   = "Girosi, Federico and Jones, Michael and Poggio, Tomaso",
  abstract = "We had previously shown that regularization principles lead to
              approximation schemes that are equivalent to networks with one
              layer of hidden units, called regularization networks. In
              particular, standard smoothness functionals lead to a subclass of
              regularization networks, the well known radial basis functions
              approximation schemes. This paper shows that regularization
              networks encompass a much broader range of approximation schemes,
              including many of the popular general additive models and some of
              the neural networks. In particular, we introduce new classes of
              smoothness functionals that lead to different classes of basis
              functions. Additive splines as well as some tensor product
              splines can be obtained from appropriate classes of smoothness
              functionals. Furthermore, the same generalization that extends
              radial basis functions (RBF) to hyper basis functions (HBF) also
              leads from additive models to ridge approximation models,
              containing as special cases Breiman's hinge functions, some forms
              of projection pursuit regression, and several types of neural
              networks. We propose to use the term generalized regularization
              networks for this broad class of approximation schemes that
              follow from an extension of regularization. In the probabilistic
              interpretation of regularization, the different classes of basis
              functions correspond to different classes of prior probabilities
              on the approximating function spaces, and therefore to different
              types of smoothness assumptions. In summary, different multilayer
              networks with one hidden layer, which we collectively call
              generalized regularization networks, correspond to different
              classes of priors and associated smoothness functionals in a
              classical regularization principle. Three broad classes are (1)
              radial basis functions that can be generalized to hyper basis
              functions, (2) some tensor product splines, and (3) additive
              splines that can be generalized to schemes of the type of ridge
              approximation, hinge functions, and several perceptron-like
              neural networks with one hidden layer.",
  journal  = "Neural Comput.",
  volume   =  7,
  number   =  2,
  pages    = "219--269",
  month    =  mar,
  year     =  1995,
  keywords = "MLTheory"
}

@INCOLLECTION{Bertero1986-vf,
  title     = "Regularization methods for linear inverse problems",
  booktitle = "Inverse Problems",
  author    = "Bertero, M",
  editor    = "Talenti, Giorgio",
  publisher = "Springer Berlin Heidelberg",
  pages     = "52--112",
  series    = "Lecture Notes in Mathematics",
  year      =  1986,
  keywords  = "NotRead;Textbook;TextBooks;MLTheory",
  language  = "en"
}

@ARTICLE{Tarantola2005-jt,
  title     = "Inverse Problem Theory - A . Tarantola - Siam 2004",
  author    = "Tarantola, Albert",
  publisher = "Society for Industrial and Applied Mathematics",
  number    = "July",
  pages     = "2--3",
  year      =  2005,
  address   = "Philadelphia, PA",
  keywords  = "NotRead;Textbook;TextBooks;MLTheory"
}

@BOOK{Vapnik2000-bm,
  title     = "The Nature of Statistical Learning Theory",
  author    = "Vapnik, Vladimir Naoumovitch",
  abstract  = "The aim of this book is to discuss the fundamental ideas which
               lie behind the statistical theory of learning and
               generalization. It considers learning as a general problem of
               function estimation based on empirical data. Omitting proofs and
               technical details, the author concentrates on discussing the
               main results of learning theory and their connections to
               fundamental problems in statistics. These include: * the setting
               of learning problems based on the model of minimizing the risk
               functional from empirical data * a comprehensive analysis of the
               empirical risk minimization principle including necessary and
               sufficient conditions for its consistency * non-asymptotic
               bounds for the risk achieved using the empirical risk
               minimization principle * principles for controlling the
               generalization ability of learning machines using small sample
               sizes based on these bounds * the Support Vector methods that
               control the generalization ability when estimating function
               using small sample size. The second edition of the book contains
               three new chapters devoted to further development of the
               learning theory and SVM techniques. These include: * the theory
               of direct method of learning based on solving multidimensional
               integral equations for density, conditional probability, and
               conditional density estimation * a new inductive principle of
               learning. Written in a readable and concise style, the book is
               intended for statisticians, mathematicians, physicists, and
               computer scientists. Vladimir N. Vapnik is Technology Leader
               AT\&T Labs-Research and Professor of London University. He is
               one of the founders of statistical learning theory, and the
               author of seven books published in English, Russian, German, and
               Chinese. Written for: Researchers",
  publisher = "Springer New York",
  volume    =  8,
  pages     = "i--xix 1--314",
  year      =  2000,
  address   = "New York, NY",
  keywords  = "Computational learning theory.; Reasoning.; Various-Artists;
               book; juergen; learning; learning-theory svm; statistical
               learning theory; statistical-learning-theory; svm; svm/ support
               vector machines/ learning theory;
               svms;NotRead;Textbook;TextBooks;MLTheory"
}

@MISC{noauthor_undated-td,
  title    = "References for Machine Learning Theory",
  keywords = "MLTheory"
}

@ARTICLE{Cohen2014-nc,
  title    = "Using Gambling to Entice {Low-Income} Families to Save",
  author   = "Cohen, Patricia",
  abstract = "A growing number of credit unions and nonprofit groups are using
              lotteries to encourage low-income families to save.",
  journal  = "The New York Times",
  month    =  aug,
  year     =  2014,
  keywords = "Discounting"
}

@ARTICLE{Watkins1992-ah,
  title    = "Q-learning",
  author   = "Watkins, Christopher J C H and Dayan, Peter",
  abstract = "Q-learning (Watkins, 1989) is a simple way for agents to learn
              how to act optimally in controlled Markovian domains. It amounts
              to an incremental method for dynamic programming which imposes
              limited computational demands. It works by successively improving
              its evaluations of the quality of particular actions at
              particular states. This paper presents and proves in detail a
              convergence theorem forQ-learning based on that outlined in
              Watkins (1989). We show thatQ-learning converges to the optimum
              action-values with probability 1 so long as all actions are
              repeatedly sampled in all states and the action-values are
              represented discretely. We also sketch extensions to the cases of
              non-discounted, but absorbing, Markov environments, and where
              manyQ values can be changed each iteration, rather than just one.",
  journal  = "Mach. Learn.",
  volume   =  8,
  number   = "3-4",
  pages    = "279--292",
  month    =  may,
  year     =  1992,
  keywords = "Artificial Intelligence (incl. Robotics);Q-learning;reinforcement
              learning;Discounting",
  language = "en"
}

@ARTICLE{Yen2006-ru,
  title    = "Agents with Shared Mental Models for",
  author   = "Yen, John and Fan, Xiaocong and Sun, Shuang and Hanratty, Timothy
              and Dumer, John",
  journal  = "Decis. Support Syst.",
  volume   =  41,
  number   =  3,
  pages    = "1--31",
  year     =  2006,
  keywords = "agent teamwork; homeland security; information overload; team
              decision-making;Discounting"
}

@ARTICLE{Bartels2015-gq,
  title     = "To Know and to Care: How Awareness and Valuation of the Future
               Jointly Shape Consumer Spending",
  author    = "Bartels, Daniel M and Urminsky, Oleg",
  abstract  = "Reducing spending in the present requires the combination of
               being both motivated to provide for one's future self (valuing
               the future) and actively considering long-term implications of
               one's choices (awareness of the future). Feeling more connected
               to the future self---thinking that the important psychological
               properties that define your current self are preserved in the
               person you will be in the future---helps motivate consumers to
               make far-sighted choices by changing their valuation of future
               outcomes (e.g., discount factors). However, this change only
               reduces spending when opportunity costs are considered.
               Correspondingly, cues that highlight opportunity costs reduce
               spending primarily when people discount the future less or are
               more connected to their future selves. Implications for the
               efficacy of behavioral interventions and for research on time
               discounting are discussed.",
  journal   = "J. Consum. Res.",
  publisher = "The Oxford University Press",
  volume    =  41,
  number    =  6,
  pages     = "1469--1485",
  month     =  apr,
  year      =  2015,
  keywords  = "Discounting",
  language  = "en"
}

@INCOLLECTION{Cooper2013-qy,
  title     = "On Computable Numbers, with an Application to the
               Entscheidungsproblem -- A Correction",
  booktitle = "Alan Turing: His Work and Impact",
  author    = "Cooper, S Barry and Leeuwen, Jan Van",
  abstract  = "Turing, A. M. ``On Computable Numbers with an Application to the
               Entscheidungsproblem.'' ,",
  publisher = "Elsevier",
  volume    =  58,
  pages     = "13--115",
  year      =  2013,
  keywords  = "Folder - HistoricalCSPapers;NotRead;Reading List/Historical"
}

@MISC{noauthor_undated-xo,
  title    = "Five Surprises from My Computer Science Academic Job Search",
  abstract = "Next in series: The job talk is a performance I've just about
              settled into a rhythm at Princeton --- classes started two weeks
              ago --- and next year's academic job search cycle is already
              underway! Ind...",
  keywords = "Folder - AcademicJobs;Other"
}

@MISC{Duffy2014-rt,
  title    = "You do not need to work 80 hours a week to succeed in academia",
  author   = "Duffy, Meghan",
  abstract = "There is a persistent myth (some might even call it a zombie
              idea) that getting tenure in academia requires working 80 hours a
              week. There's even a joke along the lines of ``The great thing
              about ac...",
  month    =  feb,
  year     =  2014,
  keywords = "Folder - AcademicJobs;Other"
}

@ARTICLE{Shannon1997-oy,
  title    = "The mathematical theory of communication. 1963",
  author   = "Shannon, C E",
  abstract = "The recent development of various methods of modulation such as
              PCM and PPM which exchange bandwidth for signal-to-noise ratio
              has intensified the interest in a general theory of
              communication. A basis for such a theory is contained in the
              important papers of Nyquist and Hartley on this subject. In the
              present paper we will extend the theory to include a number of
              new factors, in particular the effect of noise in the channel,
              and the savings possible due to the statistical structure of the
              original message and due to the nature of the final destination
              of the information. The fundamental problem of communication is
              that of reproducing at one point either exactly or approximately
              a message selected at another point. Frequently the messages have
              meaning; that is they refer to or are correlated according to
              some system with certain physical or conceptual entities. These
              semantic aspects of communication are irrelevant to the
              engineering problem. The significant aspect is that the actual
              message is one selected from a set of possible messages. The
              system must be designed to operate for each possible selection,
              not just the one which will actually be chosen since this is
              unknown at the time of design. If the number of messages in the
              set is finite then this number or any monotonic function of this
              number can be regarded as a measure of the information produced
              when one message is chosen from the set, all choices being
              equally likely. As was pointed out by Hartley the most natural
              choice is the logarithmic function. Although this definition must
              be generalized considerably when we consider the influence of the
              statistics of the message and when we have a continuous range of
              messages, we will in all cases use an essentially logarithmic
              measure.",
  journal  = "MD Comput.",
  volume   =  14,
  number   =  4,
  pages    = "306--317",
  month    =  jul,
  year     =  1997,
  keywords = "Information theory;NotRead;Reading List/Historical",
  language = "en"
}

@MISC{noauthor_undated-pz,
  title        = "theorems - Define proof environment in thmtools - {TeX} -
                  {LaTeX} Stack Exchange",
  howpublished = "\url{http://tex.stackexchange.com/questions/154883/define-proof-environment-in-thmtools}",
  keywords     = "Other"
}

@ARTICLE{Titman2008-ct,
  title    = "A general goodness-of-fit test for Markov and hidden Markov
              models",
  author   = "Titman, Andrew C and Sharples, Linda D",
  abstract = "Markov models are a convenient and useful method of estimating
              transition rates between levels of a categorical response
              variable, such as a disease stage, which changes over time. In
              medical applications the response variable is typically observed
              at irregular intervals. A Pearson-type goodness-of-fit test for
              such models was proposed by Aguirre-Hernandez and Farewell
              (Statist. Med. 2002; 21:1899-1911), but this test is not
              applicable in the common situation where the process includes an
              absorbing state, such as death, for which the time of entry is
              known precisely nor when the data include censored state
              observations. This paper presents a modification to the
              Pearson-type test to allow for these cases. An extension of the
              method, to allow for the class of hidden Markov models where the
              response variable is subject to misclassification error, is
              given. The method is applied to data on cardiac allograft
              vasculopathy in post-heart-transplant patients.",
  journal  = "Stat. Med.",
  volume   =  27,
  number   =  12,
  pages    = "2177--2195",
  month    =  may,
  year     =  2008,
  keywords = "goodness-of-fit; hidden markov model; markov model;Folder -
              HMMGoodnessOfFit;NotRead;goodness-of-fit;hidden Markov
              model;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language = "en"
}

@ARTICLE{Laskey1995-jp,
  title    = "Sensitivity analysis for probability assessments in Bayesian
              networks",
  author   = "Laskey, K B",
  journal  = "IEEE Trans. Syst. Man Cybern.",
  volume   =  25,
  number   =  6,
  pages    = "901--909",
  year     =  1995,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Sinharay2006-yc,
  title    = "Model Diagnostics for Bayesian Networks",
  author   = "Sinharay, S",
  abstract = "Bayesian networks are frequently used in educational assessments
              primarily for learning about students' knowledge and skills.
              There is a lack of works on assessing fit of Bayesian networks.
              This article employs the posterior predictive model checking
              method, a popular Bayesian model checking tool, to assess fit of
              simple Bayesian networks. A number of aspects of model fit, those
              of usual interest to practitioners, are assessed using various
              diagnostic tools. This article suggests a direct data display for
              assessing overall fit, suggests several diagnostics for assessing
              item fit, suggests a graphical approach to examine if the model
              can explain the association among the items, and suggests a
              version of the Mantel-Haenszel statistic for assessing
              differential item functioning. Limited simulation studies and a
              real data application demonstrate the effectiveness of the
              suggested model diagnostics. [PUBLICATION ABSTRACT]",
  journal  = "J. Educ. Behav. Stat.",
  volume   =  31,
  number   =  1,
  pages    = "1--33",
  month    =  jan,
  year     =  2006,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@MISC{Titman2012-zw,
  title    = "Assessing Goodness-of-fit in Hidden Markov Models",
  author   = "Titman, Andrew",
  year     =  2012,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@INPROCEEDINGS{De_Villiers2014-ch,
  title     = "A {URREF} interpretation of Bayesian network information fusion",
  booktitle = "17th International Conference on Information Fusion ({FUSION})",
  author    = "de Villiers, J P and Pavlin, G and Costa, P and Laskey, K and
               Jousselme, A L",
  abstract  = "In order for the uncertainty representation and reasoning
               evaluation framework (URREF) ontology for the evaluation of
               information fusion systems to have maximum value, it must be
               generally applicable irrespective of the application,
               uncertainty representation, reasoning scheme or data format.
               Since the URREF ontology is still an evolving framework, it is
               the focus of ongoing refinement through the efforts of the
               Evaluation of Techniques for Uncertainty Representation Working
               Group (ETURWG). Recent efforts by the authors to apply the URREF
               definitions to the evaluation of Bayesian network (BN) fusion
               systems have identified a need to translate and map the
               terminology of the URREF to the Bayesian network paradigm. The
               BN-to-URREF mapping is addressed in this paper within the
               context of the atomic decision procedure (ADP) and the latest
               view of the URREF ontology. The atomic decision procedure
               describes the information fusion process from input to output
               and consists of information sources (ADP-1), the interpretation
               and processing of this information and the qualification of the
               uncertainty it contains (ADP-2), the fusion of and reasoning
               with this uncertain information (ADP-3) and finally the decision
               scheme and output information (ADP-4). The URREF evaluation of
               the BN information fusion process allows for evaluation
               according to (1) representation criteria which relate to ADP-2
               and evaluate modeling and model parameterization, (2) reasoning
               criteria which relate to ADP-3 and evaluate the reasoning
               scheme, which in the case of a BN entails the computation of
               marginal probability densities over hypothesis variables and (3)
               data criteria which relate to ADP-1 and evaluate the sources and
               the information generated by said sources, together with the
               qualification and representation of the uncertainty inherent to
               the infor",
  pages     = "1--8",
  month     =  jul,
  year      =  2014,
  keywords  = "belief networks;inference mechanisms;ontologies (artificial
               intelligence);sensor fusion;ADP;ADP-1 source;ADP-2 source;BN
               fusion systems;BN-to-URREF mapping;Bayesian network information
               fusion;ETURWG;Evaluation of Techniques for Uncertainty
               Representation Working Group;URREF interpretation;URREF
               ontology;atomic decision procedure;data criteria;decision
               criteria;information fusion process;information fusion systems
               evaluation;model parameterization;representation
               criteria;uncertainty representation and reasoning evaluation
               framework;Bayes methods;Cognition;Data
               models;Joints;Ontologies;Probability
               distribution;Uncertainty;Assurances"
}

@ARTICLE{Laskey2015-gz,
  title    = "Uncertainty representation , quantification and evaluation for
              data and information fusion",
  author   = "Laskey, K and De Villiers, J P and Blasch, Erik and Jousselme, A
              L and Pavlin, G and de Waal, A and Costa, P",
  abstract = "Mathematical and uncertainty modelling is an important component
              of data fusion (the fusion of unprocessed sensor data) and
              information fusion (the fusion of processed or interpreted data).
              If uncertainties in the modelling process are not or are
              incorrectly accounted for, fusion processes may provide under- or
              overconfident results, or in some cases incorrect results. These
              are often owing to incorrect or invalid simplifying assumptions
              during the modelling process. The authors investigate the sources
              of uncertainty in the modelling process. In particular, four
              processes of abstraction are identified where uncertainty may
              enter the modelling process. These are isolation abstraction
              (where uncertainty is introduced by isolating a portion of the
              real world to be modelled), datum uncertainty (where uncertainty
              is introduced by representing real world information by a
              mathematical quantity), data generation abstraction (where
              uncertainty is introduced through a mathematical representation
              of the mapping between a real-world process and an observable
              datum), and process abstraction (where uncertainty is introduced
              through a mathematical representation of real world entities and
              processes). The uncertainties associated with these abstraction
              processes are characterised according to the uncertainty
              representation and reasoning evaluation framework (URREF)
              ontology. A Bayesian network information fusion use case that
              models the rhino poaching problem is utilised to demonstrate the
              taxonomies introduced in this paper.",
  journal  = "International Conference on Information Fusion",
  pages    = "50--57",
  month    =  jul,
  year     =  2015,
  keywords = "URREF
              ontology;trust\_informal\_treatment;assurance\_implicit;Assurances"
}

@ARTICLE{MacKay_Altman2004-fl,
  title    = "Assessing the goodness-of-fit of hidden Markov models",
  author   = "MacKay Altman, Rachel",
  abstract = "In this article, we propose a graphical technique for assessing
              the goodness-of-fit of a stationary hidden Markov model (HMM). We
              show that plots of the estimated distribution against the
              empirical distribution detect lack of fit with high probability
              for large sample sizes. By considering plots of the univariate
              and multidimensional distributions, we are able to examine the
              fit of both the assumed marginal distribution and the correlation
              structure of the observed data. We provide general conditions for
              the convergence of the empirical distribution to the true
              distribution, and demonstrate that these conditions hold for a
              wide variety of time-series models. Thus, our method allows us to
              compare not only the fit of different HMMs, but also that of
              other models as well. We illustrate our technique using a
              multiple sclerosis data set.",
  journal  = "Biometrics",
  volume   =  60,
  number   =  2,
  pages    = "444--450",
  month    =  jun,
  year     =  2004,
  keywords = "Goodness-of-fit; Hidden Markov model; Model selection; Multiple
              sclerosis; Probability plot; Stationary time series;Folder -
              HMMGoodnessOfFit;Model selection;goodness-of-fit;hidden Markov
              model;assurance\_implicit;trust\_informal\_treatment;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language = "en"
}

@ARTICLE{Dannemann2008-ch,
  title     = "Likelihood Ratio Testing for Hidden Markov Models Under
               Non-standard Conditions",
  author    = "Dannemann, J{\"o}rn and Holzmann, Hajo",
  abstract  = "Abstract. In practical applications, when testing parametric
               restrictions for hidden Markov models (HMMs), one frequently
               encounters non-standard situations such as testing for zero
               entries in the transition matrix, one-sided tests for the
               parameters of the transition matrix or for the components of the
               stationary distribution of the underlying Markov chain, or
               testing boundary restrictions on the parameters of the
               state-dependent distributions. In this paper, we briefly discuss
               how the relevant asymptotic distribution theory for the
               likelihood ratio test (LRT) when the true parameter is on the
               boundary extends from the independent and identically
               distributed situation to HMMs. Then we concentrate on discussing
               a number of relevant examples. The finite-sample performance of
               the LRT in such situations is investigated in a simulation
               study. An application to series of epileptic seizure counts
               concludes the paper.",
  journal   = "Scand. Stat. Theory Appl.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  35,
  number    =  2,
  pages     = "309--321",
  month     =  jun,
  year      =  2008,
  keywords  = "Bimodality; Boundary; Hidden Markov model; Likelihood ratio
               test; Marginal distribution; Maximum-likelihood estimation;
               Overdispersion;Folder -
               HMMGoodnessOfFit;NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@ARTICLE{Titman2010-qx,
  title    = "Model diagnostics for multi-state models",
  author   = "Titman, Andrew C and Sharples, Linda D",
  abstract = "Multi-state models are a popular method of describing medical
              processes that can be represented as discrete states or stages.
              They have particular use when the data are panel-observed,
              meaning they consist of discrete snapshots of disease status at
              irregular time points which may be unique to each patient.
              However, due to the difficulty of inference in more complicated
              cases, strong assumptions such as the Markov property, patient
              homogeneity and time homogeneity are applied. It is important
              that the validity of these assumptions is tested. A review of
              methods for diagnosing model fit for panel-observed
              continuous-time Markov and misclassification-type hidden Markov
              models is given, with illustrative application to a dataset on
              cardiac allograft vasculopathy progression in post-heart
              transplant patients.",
  journal  = "Stat. Methods Med. Res.",
  volume   =  19,
  number   =  6,
  pages    = "621--651",
  month    =  dec,
  year     =  2010,
  keywords = "Folder -
              HMMGoodnessOfFit;trust\_informal\_treatment;assurance\_implicit;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language = "en"
}

@ARTICLE{Shenoy1989-tw,
  title    = "A valuation-based language for expert systems",
  author   = "Shenoy, Prakash P",
  abstract = "A new language based on valuations is proposed as an alternative
              to rule-based languages for constructing knowledge-based systems.
              Valuation-based languages are superior to rule-based languages
              for maintaining consistency in the knowledge base, for caching
              inferences, for managing uncertainty, and for nonmonotonic
              reasoning. An abstract description of a valuation-based language
              is given. Two specific instances of valuation-based languages are
              described. The first is designed to represent categorical
              knowledge. The ability of such a language to maintain consistency
              and cache inferences is demonstrated with an example. The second
              is an evidential language---a valuation-based language in which
              valuations are belief functions. The ability of evidential
              languages to perform nonmonotonic reasoning and manage
              uncertainty is demonstrated with an example.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  3,
  number   =  5,
  pages    = "383--411",
  year     =  1989,
  keywords = "caching inferences; consistency in knowledge bases; evidential
              systems; knowledge-based system; management of uncertainty;
              nonmonotonic reasoning; rule-based language; rule-based system;
              truth maintenance systems; valuation system; valuation-based
              language;Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Johnson2004-mv,
  title    = "A Bayesian $\chi$ 2 test for goodness-of-fit",
  author   = "Johnson, Valen E",
  abstract = "This article describes an extension of classical $\chi$2
              goodness-of-fit tests to Bayesian model assessment. The
              extension, which essentially involves evaluating Pearson's
              goodness-of-fit statistic at a parameter value drawn from its
              posterior distribution, has the important property that it is
              asymptotically distributed as a $\chi$2 random variable on K−1
              degrees of freedom, independently of the dimension of the
              underlying parameter vector. By examining the posterior
              distribution of this statistic, global goodness-of-fit
              diagnostics are obtained. Advantages of these diagnostics include
              ease of interpretation, computational convenience and favorable
              power properties. The proposed diagnostics can be used to assess
              the adequacy of a broad class of Bayesian models, essentially
              requiring only a finite-dimensional parameter vector and
              conditionally independent observations.",
  journal  = "Ann. Stat.",
  volume   =  32,
  number   =  6,
  pages    = "2361--2384",
  month    =  dec,
  year     =  2004,
  keywords = "Bayes factor; Bayesian model assessment; Discrepancy functions;
              Intrinsic Bayes factor; Pearson's chi-squared statistic;
              Posterior-predictive diagnostics;
              p-value;trust\_informal\_treatment;assurance\_implicit;Assurances",
  language = "en"
}

@INPROCEEDINGS{Costa2012-fa,
  title     = "Towards unbiased evaluation of uncertainty reasoning: The
               {URREF} ontology",
  booktitle = "2012 15th International Conference on Information Fusion",
  author    = "Costa, P C G and Laskey, K B and Blasch, E and Jousselme, A L",
  abstract  = "Current advances in technology, sensor collection, data storage,
               and data distribution have afforded more complex, distributed,
               and operational information fusion systems (IFSs). IFSs
               notionally consist of low-level (data collection, registration,
               and association in time and space) and high-level information
               fusion (user coordination, situational awareness, and mission
               control), which require a common ontology for effective
               communication and data processing. In this paper, we describe
               the ontology reference model developed as part of the
               uncertainty representation and reasoning evaluation framework
               (URREF). The URREF ontology is intended to provide guidance for
               defining the actual concepts and criteria that together comprise
               the comprehensive uncertainty evaluation framework being
               developed by the Evaluation of Technologies for Uncertainty
               Representation Working Group (ETURWG).",
  pages     = "2301--2308",
  month     =  jul,
  year      =  2012,
  keywords  = "component; e; evaluation; i; information fusion; knowledge
               representation; level 1 fusion; measures of effectiveness;
               ontology; performance; report updates; situation and threat;
               uncertainty reasoning; ontologies (artificial intelligence);
               sensor fusion; Evaluation of Technologies for Uncertainty
               Representation Working Group ETURWG; IFS; URREF ontology;
               comprehensive uncertainty evaluation framework; data collection;
               data distribution; data processing; data storage; high-level
               information fusion; information fusion systems; mission control;
               ontology reference model; reasoning evaluation framework;
               registration; sensor collection; situational awareness; unbiased
               evaluation; uncertainty representation; user coordination;
               Accuracy; Cognition; Measurement; Ontologies; Quality of
               service; Standards; Uncertainty; Performance
               Evaluation;Measurement;Ontologies;URREF
               ontology;Uncertainty;information fusion;ontologies (artificial
               intelligence);sensor
               fusion;trust\_informal\_treatment;assurance\_implicit;Assurances"
}

@ARTICLE{Yuan2012-tb,
  title    = "Goodness-of-fit diagnostics for Bayesian hierarchical models",
  author   = "Yuan, Ying and Johnson, Valen E",
  abstract = "This article proposes methodology for assessing goodness of fit
              in Bayesian hierarchical models. The methodology is based on
              comparing values of pivotal discrepancy measures (PDMs), computed
              using parameter values drawn from the posterior distribution, to
              known reference distributions. Because the resulting diagnostics
              can be calculated from standard output of Markov chain Monte
              Carlo algorithms, their computational costs are minimal. Several
              simulation studies are provided, each of which suggests that
              diagnostics based on PDMs have higher statistical power than
              comparable posterior-predictive diagnostic checks in detecting
              model departures. The proposed methodology is illustrated in a
              clinical application; an application to discrete data is
              described in supplementary material.",
  journal  = "Biometrics",
  volume   =  68,
  number   =  1,
  pages    = "156--164",
  month    =  mar,
  year     =  2012,
  keywords = "Discrepancy measures; Markov chain Monte Carlo; Model checking;
              Model criticism; Model hierarchy; Posterior-predictive
              density;NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances",
  language = "en"
}

@ARTICLE{Spiegelhalter2002-ia,
  title     = "Bayesian measures of model complexity and fit",
  author    = "Spiegelhalter, David J and Best, Nicola G and Carlin, Bradley P
               and Van Der Linde, Angelika",
  abstract  = "Summary. We consider the problem of comparing complex
               hierarchical models in which the number of parameters is not
               clearly defined. Using an information theoretic argument we
               derive a measure pD for the effective number of parameters in a
               model as the difference between the posterior mean of the
               deviance and the deviance at the posterior means of the
               parameters of interest. In general pD approximately corresponds
               to the trace of the product of Fisher's information and the
               posterior covariance, which in normal models is the trace of the
               `hat' matrix projecting observations onto fitted values. Its
               properties in exponential families are explored. The posterior
               mean deviance is suggested as a Bayesian measure of fit or
               adequacy, and the contributions of individual observations to
               the fit and complexity can give rise to a diagnostic plot of
               deviance residuals against leverages. Adding pD to the posterior
               mean deviance gives a deviance information criterion for
               comparing models, which is related to other information criteria
               and has an approximate decision theoretic justification. The
               procedure is illustrated in some examples, and comparisons are
               drawn with alternative Bayesian and classical proposals.
               Throughout it is emphasized that the quantities required are
               trivial to compute in a Markov chain Monte Carlo analysis.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Blackwell Publishers",
  volume    =  64,
  number    =  4,
  pages     = "583--639",
  month     =  oct,
  year      =  2002,
  keywords  = "Bayesian model comparison; Decision theory; Deviance information
               criterion; Effective number of parameters; Hierarchical models;
               Information theory; Leverage; Markov chain Monte Carlo methods;
               Model dimension;Decision theory;Information
               theory;NotRead;trust\_informal\_treatment;assurance\_implicit;Assurances",
  language  = "en"
}

@ARTICLE{Yanjie_Li2011-jj,
  title    = "Partially Observable Markov Decision Processes and Performance
              Sensitivity Analysis",
  author   = "{Yanjie Li} and {Baoqun Yin} and {Hongsheng Xi}",
  abstract = "The sensitivity-based optimization of Markov systems has become
              an increasingly important area. From the perspective of
              performance sensitivity analysis, policy-iteration algorithms and
              gradient estimation methods can be directly obtained for Markov
              decision processes (MDPs). In this correspondence, the
              sensitivity-based optimization is extended to average reward
              partially observable MDPs (POMDPs). We derive the
              performance-difference and performance-derivative formulas of
              POMDPs. On the basis of the performance-derivative formula, we
              present a new method to estimate the performance gradients. From
              the performance-difference formula, we obtain a sufficient
              optimality condition without the discounted reward formulation.
              We also propose a policy-iteration algorithm to obtain a nearly
              optimal finite-state-controller policy.",
  journal  = "IEEE Trans. Syst. Man Cybern. B Cybern.",
  volume   =  38,
  number   =  6,
  pages    = "1645--1651",
  month    =  dec,
  year     =  2011,
  keywords = "Markov processes;estimation theory;optimisation;sensitivity
              analysis;meh..;Assurances"
}

@ARTICLE{Krishnan2015-qb,
  title         = "Deep Kalman Filters",
  author        = "Krishnan, Rahul G and Shalit, Uri and Sontag, David",
  abstract      = "Kalman Filters are one of the most influential models of
                   time-varying phenomena. They admit an intuitive
                   probabilistic interpretation, have a simple functional form,
                   and enjoy widespread adoption in a variety of disciplines.
                   Motivated by recent variational methods for learning deep
                   generative models, we introduce a unified algorithm to
                   efficiently learn a broad spectrum of Kalman filters. Of
                   particular interest is the use of temporal generative models
                   for counterfactual inference. We investigate the efficacy of
                   such models for counterfactual inference, and to that end we
                   introduce the ``Healing MNIST'' dataset where long-term
                   structure, noise and actions are applied to sequences of
                   digits. We show the efficacy of our method for modeling this
                   dataset. We further show how our model can be used for
                   counterfactual inference for patients, based on electronic
                   health record data of 8,000 patients over 4.5 years.",
  number        =  2000,
  pages         = "1--7",
  month         =  nov,
  year          =  2015,
  keywords      = "Folder - NIPS2015;Variatinal Inference",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.05121"
}

@BOOK{Bar-Shalom2001-tg,
  title     = "Estimation with Applications to Tracking and Navigation: Theory
               Algorithms and Software",
  author    = "Bar-Shalom, Yaakov and Rong Li, X and Kirubarajan, Thiagalingam",
  abstract  = "Expert coverage of the design and implementation of state
               estimation algorithms for tracking and navigation Estimation
               with Applications to Tracking and Navigation treats the
               estimation of various quantities from inherently inaccurate
               remote observations. It explains state estimator design using a
               balanced combination of linear systems, probability, and
               statistics. The authors provide a review of the necessary
               background mathematical techniques and offer an overview of the
               basic concepts in estimation. They then provide detailed
               treatments of all the major issues in estimation with a focus on
               applying these techniques to real systems. Other features
               include: Problems that apply theoretical material to real-world
               applications In-depth coverage of the Interacting Multiple Model
               (IMM) estimator Companion DynaEst(TM) software for MATLAB(TM)
               implementation of Kalman filters and IMM estimators Design
               guidelines for tracking filters Suitable for graduate
               engineering students and engineers working in remote sensors and
               tracking, Estimation with Applications to Tracking and
               Navigation provides expert coverage of this important area.",
  publisher = "Wiley",
  volume    =  9,
  pages     = "584",
  month     =  jun,
  year      =  2001,
  keywords  = "Textbook;TextBooks",
  language  = "en"
}

@ARTICLE{Schueller1997-al,
  title    = "A state-of-the-art report on computational stochastic mechanics",
  author   = "Schu{\"e}ller, G I",
  abstract = "This state-of-the-art report assesses the current state of
              development of computational procedures as utilized in stochastic
              mechanics. The theoretical developments and aspects of practical
              applications are discussed in this report, which is structured in
              four sections. The first section is concerned with the latest
              developments in Monte Carlo simulation (MCS)-including parallel
              processing and various types of variance reduction techniques. In
              the second section, various possibilities for representing
              stochastic processes and random fields (including discrete and
              conditional representations) and wavelets are reviewed. The third
              section is concerned with various methods for prediction of the
              response of structural systems under stochastic excitation, e.g.
              by using the FEM method to solve the Fokker-Planck equation
              (FPE), path integral method, moment closure schemes, maximum
              entropy considerations, etc. This section also includes a brief
              subsection on computational aspects of stochastic stability. Tbe
              final section is concerned with the treatment of stochastic
              uncertainties of system properties, such as material and
              geometric variation, by applying stochastic finite element
              methods (SFEMs). All sections include statements on the current
              limitations and also of the future potential of the discussed
              procedures. \copyright{} 1997 Elsevier Science Ltd.",
  journal  = "Probab. Eng. Mech.",
  volume   =  12,
  number   =  4,
  pages    = "197--321",
  month    =  oct,
  year     =  1997,
  keywords = "Textbook;TextBooks"
}

@ARTICLE{Tanaka1988-ic,
  title     = "Sensitivity analysis in principal component analysis:influence
               on the subspace spanned by principal components",
  author    = "Tanaka, Yukata",
  abstract  = "This tutorial is designed to give the reader an understanding of
               Principal Components Analysis (PCA). PCA is a useful statistical
               technique that has found application in fields such as face
               recognition and image compression, and is a common technique for
               finding patterns in data of high dimension. Before getting to a
               description of PCA, this tutorial first introduces mathematical
               concepts that will be used in PCA. It covers standard deviation,
               covariance, eigenvec- tors and eigenvalues. This background
               knowledge is meant to make the PCA section very straightforward,
               but can be skipped if the concepts are already familiar. There
               are examples all the way through this tutorial that are meant to
               illustrate the concepts being discussed. If further information
               is required, the mathematics textbook Elementary Linear Algebra
               5e by Howard Anton, Publisher JohnWiley \& Sons Inc, ISBN
               0-471-85223-6 is a good source of information regarding the
               mathematical back- ground. 1",
  journal   = "Communications in Statistics - Theory and Methods",
  publisher = "Wiley",
  volume    =  17,
  number    =  9,
  pages     = "3157--3175",
  month     =  jan,
  year      =  1988,
  address   = "New York",
  keywords  = "Textbook;TextBooks",
  language  = "English"
}

@ARTICLE{Bubeck2015-va,
  title     = "Convex Optimization: Algorithms and Complexity",
  author    = "Bubeck, S{\'e}bastien",
  abstract  = "This monograph presents the main complexity theorems in convex
               optimization and their corresponding algorithms. Starting from
               the fundamental theory of black-box optimization, the material
               progresses towards recent advances in structural optimization
               and stochastic optimization. Our presentation of black-box
               optimization, strongly influenced by Nesterov's seminal book and
               Nemirovski's lecture notes, includes the analysis of cutting
               plane methods, as well as (accelerated) gradient descent
               schemes. We also pay special attention to non- Euclidean
               settings (relevant algorithms include Frank-Wolfe, mirror
               descent, and dual averaging) and discuss their relevance in
               machine learning. We provide a gentle introduction to structural
               optimization with FISTA (to optimize a sum of a smooth and a
               simple non-smooth term), saddle-point mirror prox (Nemirovski's
               alternative to Nesterov's smoothing), and a concise description
               of interior point methods. In stochastic optimization we discuss
               stochastic gradient descent, minibatches, random coordinate
               descent, and sublinear algorithms. We also briefly touch upon
               convex relaxation of combinatorial problems and the use of
               randomness to round solutions, as well as random walks based
               methods.",
  journal   = "Foundations and Trends\textregistered{} in Machine Learning",
  publisher = "Now Publishers",
  volume    =  8,
  number    = "3-4",
  pages     = "231--357",
  year      =  2015,
  keywords  = "Optimization;NotRead;Textbook;TextBooks",
  language  = "en"
}

@ARTICLE{Lehman2014-ky,
  title    = "Mathematics for Computer Science. 2011",
  author   = "Lehman, Eric and Leighton, F T and Meyer, A R",
  journal  = "URL: http://courses. csail. mit. edu/6. 042/spring12/mcs. pdf
              [cited 2012-09-06]",
  pages    = "1--912",
  year     =  2014,
  keywords = "Important;Textbook;TextBooks"
}

@BOOK{Cover2006-wt,
  title     = "Elements of Information Theory",
  author    = "Cover, Thomas M and Thomas, Joy A",
  abstract  = "The latest edition of this classic is updated with new problem
               sets and material The Second Edition of this fundamental
               textbook maintains the book's tradition of clear,
               thought-provoking instruction. Readers are provided once again
               with an instructive mix of mathematics, physics, statistics, and
               information theory. All the essential topics in information
               theory are covered in detail, including entropy, data
               compression, channel capacity, rate distortion, network
               information theory, and hypothesis testing. The authors provide
               readers with a solid understanding of the underlying theory and
               applications. Problem sets and a telegraphic summary at the end
               of each chapter further assist readers. The historical notes
               that follow each chapter recap the main points. The Second
               Edition features: * Chapters reorganized to improve teaching *
               200 new problems * New material on source coding, portfolio
               theory, and feedback capacity * Updated references Now current
               and enhanced, the Second Edition of Elements of Information
               Theory remains the ideal textbook for upper-level undergraduate
               and graduate courses in electrical engineering, statistics, and
               telecommunications. An Instructor's Manual presenting detailed
               solutions to all the problems in the book is available from the
               Wiley editorial department.",
  publisher = "Wiley",
  pages     = "1--748",
  edition   = "2nd ed",
  month     =  jul,
  year      =  2006,
  address   = "Hoboken, N.J",
  keywords  = "Information theory;NotRead;Textbook;TextBooks",
  language  = "en"
}

@BOOK{Konig2014-ud,
  title     = "Eigenvalue Distribution of Compact Operators",
  author    = "K{\"o}nig, H",
  publisher = "Birkh{\"a}user Basel",
  volume    =  16,
  pages     = "262",
  series    = "Operator Theory: Advances and Applications",
  month     =  apr,
  year      =  2014,
  address   = "Basel",
  keywords  = "Textbook;TextBooks",
  language  = "en"
}

@ARTICLE{Hastie2009-ho,
  title    = "The Elements of Statistical Learning",
  author   = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract = "During the past decade there has been an explosion in computation
              and information technology. With it has come vast amounts of data
              in a variety of fields such as medicine, biology, finance, and
              marketing. The challenge of understanding these data has led to
              the development of new tools in the field of statistics, and
              spawned new areas such as data mining, machine learning, and
              bioinformatics. Many of these tools have common underpinnings but
              are often expressed with different terminology. This book
              describes the important ideas in these areas in a common
              conceptual framework. While the approach is statistical, the
              emphasis is on concepts rather than mathematics. Many examples
              are given, with a liberal use of color graphics. It should be a
              valuable resource for statisticians and anyone interested in data
              mining in science or industry. The book's coverage is broad, from
              supervised learning (prediction) to unsupervised learning. The
              many topics include neural networks, support vector machines,
              classification trees and boosting-the first comprehensive
              treatment of this topic in any book. Trevor Hastie, Robert
              Tibshirani, and Jerome Friedman are professors of statistics at
              Stanford University. They are prominent researchers in this area:
              Hastie and Tibshirani developed generalized additive models and
              wrote a popular book of that title. Hastie wrote much of the
              statistical modeling software in S-PLUS and invented principal
              curves and surfaces. Tibshirani proposed the Lasso and is
              co-author of the very successful An Introduction to the
              Bootstrap. Friedman is the co-inventor of many data-mining tools
              including CART, MARS, and projection pursuit. FROM THE REVIEWS:
              TECHNOMETRICS ``This is a vast and complex book. Generally, it
              concentrates on explaining why and how the methods work, rather
              than how to use them. Examples and especially the visualizations
              are principle features...As a source for the methods of
              statistical learning...it will probably be a long time before
              there is a competitor to this book.''",
  journal  = "Elements",
  volume   =  1,
  pages    = "337--387",
  year     =  2009,
  keywords = "Textbook;TextBooks"
}

@BOOK{Olver2010-me,
  title     = "{NIST} Handbook of Mathematical Functions",
  author    = "Olver, F W J (editor)",
  publisher = "Cambridge University Press, Cambridge, London and New York",
  year      =  2010,
  keywords  = "TALAF;Textbook;TextBooks"
}

@MISC{noauthor_undated-le,
  title        = "plt - File Exchange - {MATLAB} Central",
  abstract     = "Finding the nearest positive definite matrix",
  howpublished = "\url{http://www.mathworks.de/matlabcentral/fileexchange/4936-plt}",
  keywords     = "TALAF;AFRL\_STTR"
}

@INPROCEEDINGS{Oddi2012-jt,
  title           = "A distributed multi-path algorithm for wireless ad-hoc
                     networks based on Wardrop routing",
  booktitle       = "21st Mediterranean Conference on Control and Automation",
  author          = "Oddi, G and Pietrabissa, A",
  abstract        = "Reinforcement learning, one of the most active research
                     areas in artificial intelligence, is a computational
                     approach to learning whereby an agent tries to maximize
                     the total amount of reward it receives when interacting
                     with a complex, uncertain environment. In Reinforcement
                     Learning, Richard Sutton and Andrew Barto provide a clear
                     and simple account of the key ideas and algorithms of
                     reinforcement learning. Their discussion ranges from the
                     history of the field's intellectual foundations to the
                     most recent developments and applications. The only
                     necessary mathematical background is familiarity with
                     elementary concepts of probability. The book is divided
                     into three parts. Part I defines the reinforcement
                     learning problem in terms of Markov decision processes.
                     Part II provides basic solution methods: dynamic
                     programming, Monte Carlo methods, and temporal-difference
                     learning. Part III presents a unified view of the solution
                     methods and incorporates artificial neural networks,
                     eligibility traces, and planning; the two final chapters
                     present case studies and consider the future of
                     reinforcement learning.",
  publisher       = "IEEE",
  volume          =  3,
  pages           = "930--935",
  year            =  2012,
  keywords        = "Textbook;TextBooks",
  conference      = "2013 21st Mediterranean Conference on Control \&
                     Automation (MED)"
}

@ARTICLE{Bubeck2012-xo,
  title     = "Regret Analysis of Stochastic and Nonstochastic Multi-armed
               Bandit Problems",
  author    = "Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicol{\`o}",
  abstract  = "Multi-armed bandit problems are the most basic examples of
               sequential decision problems with an exploration-exploitation
               trade-off. This is the balance between staying with the option
               that gave highest payoffs in the past and exploring new options
               that might give higher payoffs in the future. Although the study
               of bandit problems dates back to the Thirties,
               exploration-exploitation trade-offs arise in several modern
               applications, such as ad placement, website optimization, and
               packet routing. Mathematically, a multi-armed bandit is defined
               by the payoff process associated with each option. In this
               survey, we focus on two extreme cases in which the analysis of
               regret is particularly simple and elegant: i.i.d. payoffs and
               adversarial payoffs. Besides the basic setting of finitely many
               actions, we also analyze some of the most important variants and
               extensions, such as the contextual bandit model.",
  journal   = "Foundations and Trends\textregistered{} in Machine Learning",
  publisher = "Now Publishers",
  volume    =  5,
  number    =  1,
  pages     = "1--122",
  year      =  2012,
  keywords  = "Learning and statistical methods; Game theoretic learning;
               Online learning; Optimization; Reinforcement
               learning;NotRead;Textbook;TextBooks",
  language  = "en"
}

@BOOK{Wasserman2013-bh,
  title     = "All of Statistics: A Concise Course in Statistical Inference",
  author    = "Wasserman, Larry",
  abstract  = "Taken literally, the title ``All of Statistics'' is an
               exaggeration. But in spirit, the title is apt, as the book does
               cover a much broader range of topics than a typical introductory
               book on mathematical statistics. This book is for people who
               want to learn probability and statistics quickly. It is suitable
               for graduate or advanced undergraduate students in computer
               science, mathematics, statistics, and related disciplines. The
               book includes modern topics like nonparametric curve estimation,
               bootstrapping, and clas sification, topics that are usually
               relegated to follow-up courses. The reader is presumed to know
               calculus and a little linear algebra. No previous knowledge of
               probability and statistics is required. Statistics, data mining,
               and machine learning are all concerned with collecting and
               analyzing data. For some time, statistics research was con
               ducted in statistics departments while data mining and machine
               learning re search was conducted in computer science
               departments. Statisticians thought that computer scientists were
               reinventing the wheel. Computer scientists thought that
               statistical theory didn't apply to their problems. Things are
               changing. Statisticians now recognize that computer scientists
               are making novel contributions while computer scientists now
               recognize the generality of statistical theory and methodology.
               Clever data mining algo rithms are more scalable than
               statisticians ever thought possible. Formal sta tistical theory
               is more pervasive than computer scientists had realized.",
  publisher = "Springer Science \& Business Media",
  month     =  dec,
  year      =  2013,
  keywords  = "Textbook;TextBooks",
  language  = "en"
}

@BOOK{Zwillinger2012-ue,
  title     = "Standard mathematical tables and formulae",
  author    = "Zwillinger, Daniel",
  publisher = "CRC press",
  year      =  2012,
  keywords  = "Textbook;TextBooks"
}

@BOOK{Khalil2002-hc,
  title     = "Nonlinear Systems",
  author    = "Khalil, Hassan K",
  abstract  = "This book is written is such a way that the level of
               mathematical sophistication builds up from chapter to chapter.
               It has been reorganized into four parts: basic analysis,
               analysis of feedback systems, advanced analysis, and nonlinear
               feedback control. Updated content includes subjects which have
               proven useful in nonlinear control design in recent years---new
               in the 3rd edition are: expanded treatment of passivity and
               passivity-based control; integral control, high-gain feedback,
               recursive methods, optimal stabilizing control, control Lyapunov
               functions, and observers. For use as a self-study or reference
               guide by engineers and applied mathematicians.",
  publisher = "Prentice Hall",
  pages     = "767",
  edition   = "3rd ed",
  year      =  2002,
  address   = "Upper Saddle River, N.J",
  keywords  = "Textbook;TextBooks",
  language  = "en"
}

@BOOK{MacKay2003-ty,
  title     = "Information Theory, Inference and Learning Algorithms",
  author    = "MacKay, David J C",
  abstract  = "Information theory and inference, often taught separately, are
               here united in one entertaining textbook. These topics lie at
               the heart of many exciting areas of contemporary science and
               engineering - communication, signal processing, data mining,
               machine learning, pattern recognition, computational
               neuroscience, bioinformatics, and cryptography. This textbook
               introduces theory in tandem with applications. Information
               theory is taught alongside practical communication systems, such
               as arithmetic coding for data compression and sparse-graph codes
               for error-correction. A toolbox of inference techniques,
               including message-passing algorithms, Monte Carlo methods, and
               variational approximations, are developed alongside applications
               of these tools to clustering, convolutional codes, independent
               component analysis, and neural networks. The final part of the
               book describes the state of the art in error-correcting codes,
               including low-density parity-check codes, turbo codes, and
               digital fountain codes -- the twenty-first century standards for
               satellite communications, disk drives, and data broadcast.
               Richly illustrated, filled with worked examples and over 400
               exercises, some with detailed solutions, David MacKay's
               groundbreaking book is ideal for self-learning and for
               undergraduate or graduate courses. Interludes on crosswords,
               evolution, and sex provide entertainment along the way. In sum,
               this is a textbook on information, communication, and coding for
               a new generation of students, and an unparalleled entry point
               into these subjects for professionals in areas as diverse as
               computational biology, financial engineering, and machine
               learning.",
  publisher = "Cambridge University Press",
  volume    =  100,
  pages     = "1--640",
  month     =  sep,
  year      =  2003,
  keywords  = "NotRead;Textbook;Information theory;TextBooks;CurrentStudy",
  language  = "en"
}

@BOOK{Dasgupta_undated-cn,
  title    = "Algorithms",
  author   = "Dasgupta, S and Papadimitriou, C H and Vazirani, U V",
  keywords = "Folder - CSCI5454-Algorithms;Textbook;TextBooks"
}

@ARTICLE{OCallaghan2012-up,
  title    = "Gaussian process occupancy maps",
  author   = "O'Callaghan, S T and Ramos, F T",
  journal  = "Int. J. Rob. Res.",
  volume   =  31,
  number   =  1,
  pages    = "42--62",
  month    =  jan,
  year     =  2012,
  keywords = "BayesOpt;GPs;TALAF;BayesOpt;GPs",
  language = "en"
}

@INPROCEEDINGS{Guestrin2005-rw,
  title     = "Near-optimal Sensor Placements in Gaussian Processes",
  booktitle = "Proceedings of the 22Nd International Conference on Machine
               Learning",
  author    = "Guestrin, Carlos and Krause, Andreas and Singh, Ajit Paul",
  publisher = "ACM",
  pages     = "265--272",
  series    = "ICML '05",
  year      =  2005,
  address   = "New York, NY, USA",
  keywords  = "BayesOpt;GPs;NotRead;TALAF;BayesOpt;GPs"
}

@INPROCEEDINGS{Zagorecki2015-qy,
  title     = "An Approximation of Surprise Index as a Measure of Confidence",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Zagorecki, Adam and Kozniewski, Marcin and Druzdzel, Marek",
  abstract  = "Probabilistic graphical models, such as Bayesian networks, are
               intuitive and theoretically sound tools for modeling
               uncertainty. A major problem with applying Bayesian networks in
               practice is that it is hard to judge whether a model fits well a
               case that it is supposed to solve. One way of expressing a
               possible dissonance between a model and a case is the
               \{\textbackslashem surprise index\}, proposed by Habbema, which
               expresses the degree of surprise by the evidence given the
               model. While this measure reflects the intuition that the
               probability of a case should be judged in the context of a
               model, it is computationally intractable. In this paper, we
               propose an efficient way of approximating the surprise index.",
  month     =  sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;model\_check;Supplemental
               Assurance;Quantify Uncertainty;Assurances",
  language  = "en"
}

@ARTICLE{Lake2015-xn,
  title    = "Human-level concept learning through probabilistic program
              induction",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  abstract = "People learning new concepts can often generalize successfully
              from just a single example, yet machine learning algorithms
              typically require tens or hundreds of examples to perform with
              similar accuracy. People can also use learned concepts in richer
              ways than conventional algorithms-for action, imagination, and
              explanation. We present a computational model that captures these
              human learning abilities for a large class of simple visual
              concepts: handwritten characters from the world's alphabets. The
              model represents concepts as simple programs that best explain
              observed examples under a Bayesian criterion. On a challenging
              one-shot classification task, the model achieves human-level
              performance while outperforming recent deep learning approaches.
              We also present several ``visual Turing tests'' probing the
              model's creative generalization abilities, which in many cases
              are indistinguishable from human behavior.",
  journal  = "Science",
  volume   =  350,
  number   =  6266,
  pages    = "1332--1338",
  month    =  dec,
  year     =  2015,
  keywords = "Folder - Spring2016;NotRead;MLTheory",
  language = "en"
}

@INPROCEEDINGS{Hutchins2015-if,
  title     = "Representing Autonomous Systems' {Self-Confidence} through
               Competency Boundaries",
  author    = "Hutchins, Andrew R and Cummings, M L and Draper, Mark and
               Hughes, Thomas",
  publisher = "SAGE Publications",
  volume    =  59,
  pages     = "279--283",
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;perf\_prediction;vis\_dr;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@INPROCEEDINGS{Laskey1991-mf,
  title     = "Conflict and Surprise: Heuristics for Model Revision",
  booktitle = "Proceedings of the Seventh Conference on Uncertainty in
               Artificial Intelligence",
  author    = "Laskey, Kathryn Blackmond",
  publisher = "Morgan Kaufmann Publishers Inc.",
  pages     = "197--204",
  series    = "UAI'91",
  year      =  1991,
  address   = "San Francisco, CA, USA",
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;model\_check;Supplemental
               Assurance;Quantify Uncertainty;Assurances"
}

@BOOK{Rasmussen2004-vo,
  title     = "Gaussian processes in machine learning",
  author    = "Rasmussen, Carl Edward",
  publisher = "Springer",
  pages     = "63--71",
  year      =  2004,
  keywords  = "BayesOpt;GPs;TALAF;BayesOpt;GPs"
}

@INCOLLECTION{Sweet2016-tz,
  title     = "Towards {Self-Confidence} in Autonomous Systems",
  booktitle = "{AIAA} Infotech @ Aerospace",
  author    = "Sweet, Nicholas and Ahmed, Nisar R and Kuter, Ugur and Miller,
               Christopher",
  pages     = "1651",
  year      =  2016,
  keywords  = "Assurances"
}

@INPROCEEDINGS{Kaipa2015-hy,
  title     = "Toward Estimating Task Execution Confidence for Robotic
               {Bin-Picking} Applications",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Kaipa, Krishnanand N and Kankanhalli-Nagendra, Akshaya S and
               Gupta, Satyandra K",
  abstract  = "We present an approach geared toward estimating task execution
               confidence for robotic bin-picking applications. This requires
               estimating execution confidence for all constituent subtasks
               including part recognition and pose estimation, singulation,
               transport, and fine positioning. This paper is focussed on
               computing associated confidence parameters for the part
               recognition and pose estimation subtask. In particular, our
               approach allows a robot to evaluate how good the part
               recognition and pose estimation is, based on a
               confidence-measure, and thereby determine whether to proceed
               with the task execution (part singulation) or to request help
               from a human in order to resolve the associated failure. The
               value of a mean-square distance metric at a local minimum where
               the part matching solution is found is used as a surrogate for
               the confidence parameter. Experiments with a Baxter robot are
               used illustrate our approach.",
  month     =  sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;Supplemental
               Assurance;Quantify Uncertainty;Assurances",
  language  = "en"
}

@BOOK{Shawe-Taylor2004-jq,
  title     = "Kernel Methods for Pattern Analysis",
  author    = "Shawe-Taylor, John and Cristianini, Nello",
  abstract  = "This book fulfils two major roles: firstly it provides
               practitioners with a large toolkit of algorithms, kernels and
               solutions ready to be implemented, suitable for standard pattern
               discovery problems in field such as bioinformatics, text
               analysis, image analysis. Secondly it provides an easy
               introduction for students and researchers to the growing field
               of kernel-based pattern analysis, demonstrating with examples
               how to handcraft an algorithm or a kernel for a new specific
               application, and covering all the necessary conceptual and
               mathematical tools to do so.",
  publisher = "Cambridge University Press",
  volume    = "XXXIII",
  pages     = "81--87",
  month     =  jun,
  year      =  2004,
  keywords  = "BayesOpt;GPs;TALAF;Textbook;TextBooks;BayesOpt;GPs",
  language  = "en"
}

@INPROCEEDINGS{Ng1997-kg,
  title     = "Preventing`` overfitting'' of cross-validation data",
  booktitle = "{ICML}",
  author    = "Ng, Andrew Y",
  abstract  = "Suppose that, for a learning task, we have to select one
               hypothesis out of a set of hypotheses (that may, for example,
               have been generated by multiple applications of a randomized
               learning algorithm). A common approach is to evaluate each
               hypothesis in the set on some previously unseen cross-validation
               data, and then to select the hypothesis that had the lowest
               cross-validation error. But when the cross-validation data is
               partially corrupted such as by noise, and if the set of
               hypotheses we are selecting from is large, then ``folklore''
               also warns about ``overfitting'' the crossvalidation data
               [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953]. In this
               paper, we explain how this ``overfitting'' really occurs, and
               show the surprising result that it can be overcome by selecting
               a hypothesis with a higher cross-validation error, over others
               with lower cross-validation errors. We give reasons for not
               selecting the hypothesis with the lowest cross-validation error,
               and propose a new algorithm, L...",
  publisher = "Morgan Kaufmann",
  volume    =  97,
  pages     = "245--253",
  year      =  1997,
  keywords  = "BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;BayesOpt;OptimizingHyperparameters"
}

@MISC{Kaipa_undated-hw,
  title        = "Kaipa Webpage",
  author       = "Kaipa, Krishnanand N",
  howpublished = "\url{http://www.terpconnect.umd.edu/~kkrishna/research.html}",
  keywords     = "Assurances"
}

@INCOLLECTION{Murray2010-rv,
  title     = "Slice sampling covariance hyperparameters of latent Gaussian
               models",
  booktitle = "Advances in Neural Information Processing Systems 23",
  author    = "Murray, Iain and Adams, Ryan P",
  editor    = "Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel,
               R S and Culotta, A",
  abstract  = "The Gaussian process (GP) is a popular way to specify
               dependencies between random variables in a probabilistic model.
               In the Bayesian framework the covariance structure can be
               specified using unknown hyperparameters. Integrating over these
               hyperparameters considers different possible explanations for
               the data when making predictions. This integration is often
               performed using Markov chain Monte Carlo (MCMC) sampling.
               However, with non-Gaussian observations standard hyperparameter
               sampling approaches require careful tuning and may converge
               slowly. In this paper we present a slice sampling approach that
               requires little tuning while mixing well in both strong- and
               weak-data regimes.",
  publisher = "Curran Associates, Inc.",
  volume    =  2,
  pages     = "1732--1740",
  year      =  2010,
  keywords  = "Acquisition/InfillFxns;BayesOpt;NotRead;TALAF;BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@BOOK{Bishop2006-cl,
  title     = "Pattern Recognition and Machine Learning",
  author    = "Bishop, Christopher M",
  abstract  = "This is the first textbook on pattern recognition to present the
               Bayesian viewpoint. The book presents approximate inference
               algorithms that permit fast approximate answers in situations
               where exact answers are not feasible. It uses graphical models
               to describe probability distributions when no other books apply
               graphical models to machine learning. No previous knowledge of
               pattern recognition or machine learning concepts is assumed.
               Familiarity with multivariate calculus and basic linear algebra
               is required, and some experience in the use of probabilities
               would be helpful though not essential as the book includes a
               self-contained introduction to basic probability theory.",
  publisher = "Springer",
  volume    =  4,
  pages     = "738",
  series    = "Information science and statistics",
  month     =  aug,
  year      =  2006,
  address   = "New York",
  keywords  = "BayesOpt;GPs;Important;Machine
               learning;TALAF;Textbook;TextBooks;MLTheory;BayesOpt;GraphicalModels;GPs",
  language  = "en"
}

@PHDTHESIS{Hutter2009-og,
  title    = "Automated configuration of algorithms for solving hard
              computational problems",
  author   = "Hutter, Frank",
  year     =  2009,
  school   = "University of British Columbia",
  keywords = "BayesOpt;GPs;NotRead;TALAF;BayesOpt;GPs"
}

@ARTICLE{Sutton1999-cj,
  title    = "Between {MDPs} and {semi-MDPs}: A framework for temporal
              abstraction in reinforcement learning",
  author   = "Sutton, Richard S and Precup, Doina and Singh, Satinder",
  abstract = "Learning, planning, and representing knowledge at multiple levels
              of temporal abstraction are key, longstanding challenges for AI.
              In this paper we consider how these challenges can be addressed
              within the mathematical framework of reinforcement learning and
              Markov decision processes (MDPs). We extend the usual notion of
              action in this framework to include options---closed-loop
              policies for taking action over a period of time. Examples of
              options include picking up an object, going to lunch, and
              traveling to a distant city, as well as primitive actions such as
              muscle twitches and joint torques. Overall, we show that options
              enable temporally abstract knowledge and action to be included in
              the reinforcement learning framework in a natural and general
              way. In particular, we show that options may be used
              interchangeably with primitive actions in planning methods such
              as dynamic programming and in learning methods such as
              Q-learning. Formally, a set of options defined over an MDP
              constitutes a semi-Markov decision process (SMDP), and the theory
              of SMDPs provides the foundation for the theory of options.
              However, the most interesting issues concern the interplay
              between the underlying MDP and the SMDP and are thus beyond SMDP
              theory. We present results for three such cases: (1) we show that
              the results of planning with options can be used during execution
              to interrupt options and thereby perform even better than
              planned, (2) we introduce new intra-option methods that are able
              to learn about an option from fragments of its execution, and (3)
              we propose a notion of subgoal that can be used to improve the
              options themselves. All of these results have precursors in the
              existing literature; the contribution of this paper is to
              establish them in a simpler and more general setting with fewer
              changes to the existing reinforcement learning framework. In
              particular, we show that these results can be obtained without
              committing to (or ruling out) any particular approach to state
              abstraction, hierarchy, function approximation, or the
              macro-utility problem.",
  journal  = "Artif. Intell.",
  volume   =  112,
  number   = "1--2",
  pages    = "181--211",
  year     =  1999,
  keywords = "Temporal abstraction; Reinforcement learning; Markov decision
              processes; Options; Macros; Macroactions; Subgoals; Intra-option
              learning; Hierarchical planning; Semi-Markov decision
              processes;BayesOpt;NotRead;TALAF;reinforcement learning;BayesOpt"
}

@ARTICLE{Rasmussen2006-lz,
  title    = "Advances in Gaussian processes",
  author   = "Rasmussen, Carl Edward",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  19,
  year     =  2006,
  keywords = "BayesOpt;GPs;TALAF;BayesOpt;GPs"
}

@ARTICLE{Giudici2000-sl,
  title    = "Likelihood-ratio tests for hidden Markov models",
  author   = "Giudici, P and Ryd{\'e}n, T and Vandekerkhove, P",
  abstract = "We consider hidden Markov models as a versatile class of models
              for weakly dependent random phenomena. The topic of the present
              paper is likelihood-ratio testing for hidden Markov models, and
              we show that, under appropriate conditions, the standard
              asymptotic theory of likelihood-ratio tests is valid. Such tests
              are crucial in the specification of multivariate Gaussian hidden
              Markov models, which we use to illustrate the applicability of
              our general results. Finally, the methodology is illustrated by
              means of a real data set.",
  journal  = "Biometrics",
  volume   =  56,
  number   =  3,
  pages    = "742--747",
  month    =  sep,
  year     =  2000,
  keywords = "Gaussian hidden Markov model; Likelihood-ratio
              test;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language = "en"
}

@BOOK{Zucchini2009-ay,
  title     = "Hidden Markov Models for Time Series: An Introduction Using {R}",
  author    = "Zucchini, Walter and MacDonald, Iain L",
  abstract  = "Reveals How HMMs Can Be Used as General-Purpose Time Series
               Models Implements all methods in RHidden Markov Models for Time
               Series: An Introduction Using R applies hidden Markov models
               (HMMs) to a wide range of time series types, from
               continuous-valued, circular, and multivariate series to binary
               data, bounded and unbounded counts, and categorical
               observations. It also discusses how to employ the freely
               available computing environment R to carry out computations for
               parameter estimation, model selection and checking, decoding,
               and forecasting. Illustrates the methodology in actionAfter
               presenting the simple Poisson HMM, the book covers estimation,
               forecasting, decoding, prediction, model selection, and Bayesian
               inference. Through examples and applications, the authors
               describe how to extend and generalize the basic model so it can
               be applied in a rich variety of situations. They also provide R
               code for some of the examples, enabling the use of the codes in
               similar applications. Effectively interpret data using HMMs This
               book illustrates the wonderful flexibility of HMMs as
               general-purpose models for time series data. It provides a broad
               understanding of the models and their uses.",
  publisher = "CRC Press",
  pages     = "288",
  month     =  apr,
  year      =  2009,
  keywords  = "Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@TECHREPORT{Afrl_undated-fx,
  title    = "Constructive Entity Behavior v2",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-ex,
  title    = "Interface Design Document for the Not So Grand Challenge ( {NSGC}
              ) Project",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-nd,
  title    = "{PETS} interface design document",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-jz,
  title    = "Group definitions and behaviors",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@INPROCEEDINGS{Lara-Cabrera2013-yn,
  title     = "A review of computational intelligence in {RTS} games",
  booktitle = "2013 {IEEE} Symposium on Foundations of Computational
               Intelligence ({FOCI})",
  author    = "Lara-Cabrera, R and Cotta, C and Fern{\'a}ndez-Leiva, A J",
  abstract  = "Real-time strategy games offer a wide variety of fundamental AI
               research challenges. Most of these challenges have applications
               outside the game domain. This paper provides a review on
               computational intelligence in real-time strategy games (RTS). It
               starts with challenges in real-time strategy games, then it
               reviews different tasks to overcome this challenges. Later, it
               describes the techniques used to solve this challenges and it
               makes a relationship between techniques and tasks. Finally, it
               presents a set of different frameworks used as test-beds for the
               techniques employed. This paper is intended to be a starting
               point for future researchers on this topic.",
  pages     = "114--121",
  month     =  apr,
  year      =  2013,
  keywords  = "Buildings; Computational intelligence; Evolutionary computation;
               Games; NotRead; Planning; RTS games; artificial intelligence;
               computational intelligence; computer games; game domain; real
               time strategy games; real-time strategy games; real-time
               systems; review;Evolutionary
               computation;GameAI;Games;NotRead;ai\_planning;TALAF;artificial
               intelligence;computational intelligence;computer games;real-time
               strategy games;real-time systems;AFRL\_STTR"
}

@ARTICLE{Nguyen2013-wv,
  title    = "Monte Carlo Tree Search for Collaboration Control of Ghosts in
              Ms. {Pac-Man}",
  author   = "Nguyen, K Q and Thawonmas, R",
  abstract = "In this paper, we present an application of Monte Carlo tree
              search (MCTS) to control ghosts in the game called Ms. Pac-Man.
              Our proposed ghost team consists of a ghost controlled by rules
              and three ghosts controlled individually by different MCTS. Given
              a limited time response, in order to increase the reliability of
              MCTS results, we introduce a mechanism for predicting Ms.
              Pac-Man's future movements and use this mechanism for simulating
              Ms. Pac-Man during Monte Carlo simulations. Our ghost team won
              the first Ms. Pac-Man Versus Ghost Team Competition at the 2011
              IEEE Congress on Evolutionary Computation (CEC). Its performances
              for a variety of design choices are also shown and discussed.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  5,
  number   =  1,
  pages    = "57--68",
  month    =  mar,
  year     =  2013,
  keywords = "2011 IEEE CEC; 2011 IEEE Congress on Evolutionary Computation;
              Collaboration; Computers; Games; Ghosts; MCTS results
              reliability; Monte Carlo; Monte Carlo methods; Monte Carlo
              simulations; Monte Carlo tree search; Monte Carlo tree search
              (MCTS); Ms. Pac-Man; Ms. Pac-Man Versus Ghost Team Competition;
              Pac-Man; Prediction algorithms; Reliability; Time factors;
              computer games; ghost collaboration control; groupware; movement
              prediction; tree searching; video
              game;Computers;GameAI;Games;monte\_carlo;Monte Carlo tree search
              (MCTS);TALAF;computer games;tree searching;AFRL\_STTR"
}

@INPROCEEDINGS{Ha2015-xe,
  title     = "A stochastic game-theoretic approach for analysis of multiple
               cooperative air combat",
  booktitle = "2015 American Control Conference ({ACC})",
  author    = "Ha, J S and Chae, H J and Choi, H L",
  abstract  = "This paper presents a game theory-based methodology to analyze
               multiple beyond-visual-range (BVR) air combat scenarios. The
               combat scenarios are formalized as a stochastic game consisting
               of a sequence of normal-form games with a continuous sub-game.
               The proposed game formulation improves the scalability by
               reducing the decision space while taking advantage of some
               underlying symmetry structures of the combat scenario. The
               equilibrium strategy and the value functions of the game are
               computed through some dynamic programming procedure; the impact
               of the aircraft's velocity and the cooperation scheme for the
               combat is analyzed based on the equilibrium strategies.",
  pages     = "3728--3733",
  month     =  jul,
  year      =  2015,
  keywords  = "Aircraft; BVR air combat scenarios; Games; Missiles; Nash
               equilibrium; NotRead; Resource management; Stochastic processes;
               aircraft velocity; autonomous aerial vehicles;
               beyond-visual-range air combat scenarios; continuous subgame;
               cooperation scheme; decision space; dynamic programming;
               equilibrium strategies; equilibrium strategy; game formulation;
               game theory-based methodology; game value functions; military
               aircraft; multiple cooperative air combat analysis; normal-form
               games; stochastic game-theoretic approach; stochastic games;
               symmetry structures; unmanned air combat system; velocity
               control;Aircraft;Games;Missiles;NotRead;Stochastic
               processes;TALAF;AFRL\_STTR"
}

@MISC{Synnaeve2012-uc,
  title    = "A Bayesian Tactician",
  author   = "Synnaeve, Gabriel and Bessiere, Pierre",
  abstract = "We describe a generative Bayesian model of tactical attacks in
              strategy games, which can be used both to predict attacks and to
              take tactical decisions. This model is designed to easily
              integrate and merge information from other (probabilistic)
              estimations and heuristics. In particular, it handles uncertainty
              in enemy units' positions as well as their probable tech tree. We
              claim that learning, being it supervised or through
              reinforcement, adapts to skewed data sources. We evaluated our
              approach on StarCraft1: the parameters are learned on a new
              (freely available) dataset of game states, deterministically
              re-created from replays, and the whole model is evaluated for
              prediction in realistic conditions. It is also the tactical
              decision-making component of a competitive StarCraft AI.",
  journal  = "Computer Games Workshop at ECAI 2012",
  pages    = "114--125",
  year     =  2012,
  keywords = "Bayesian modeling; RTS games; game AI; machine learning;
              tactics;GameAI;TALAF;AFRL\_STTR"
}

@ARTICLE{Ciavarelli1980-of,
  title     = "Operational performance measures for air combat: Development and
               application",
  author    = "Ciavarelli, A P and Williams, A M and {others}",
  abstract  = "Abstract The content of this paper summarizes four years of
               research designed to develop valid and reliable performance
               criteria for the Navy's Tactical Aircrew Combat Training System
               (TACTS). Performance measurement methods for assessing missile
               envelope recognition and air combat engagements have been
               developed and applied in an operational setting. TACTS measures
               used in performance assessment were selected on ...",
  journal   = "Proc. Hum. Fact. Ergon. Soc. Annu. Meet.",
  publisher = "pro.sagepub.com",
  volume    =  24,
  number    =  1,
  pages     = "560--564",
  year      =  1980,
  keywords  = "NotRead;NotRead;TALAF;AFRL\_STTR",
  language  = "en"
}

@TECHREPORT{Moore1979-kv,
  title       = "Air Combat Training: Good Stick Index Validation",
  author      = "Moore, Samuel B and Madison, Walker G and Sepp, George D and
                 Stracener, Jerrell T and Coward, Robert E",
  institution = "DTIC Document",
  year        =  1979,
  keywords    = "*FLIGHT TRAINING; AERIAL WARFARE; Automation; FLIGHT
                 MANEUVERS; FLIGHT SIMULATORS; GSI(GOOD STICK INDEX);
                 Humanities and History; MATHEMATICAL PREDICTION; Measurement;
                 PE62205F; PERFORMANCE TESTS; PERFORMANCE(HUMAN); PERSONNEL
                 SELECTION; PILOTS; PROFICIENCY; SCORING; SKILLS; STATISTICAL
                 ANALYSIS; STUDENTS; Statistics and Probability; TEST
                 CONSTRUCTION(PSYCHOLOGY); VALIDATION;
                 WUAFHRL11231216;Measurement;STATISTICAL
                 ANALYSIS;TALAF;AFRL\_STTR",
  language    = "en"
}

@INPROCEEDINGS{Luo2005-gw,
  title     = "Air Combat {Decision-Making} for Cooperative Multiple Target
               Attack Using Heuristic Adaptive Genetic Algorithm",
  booktitle = "2005 International Conference on Machine Learning and
               Cybernetics",
  author    = "Luo, De-Lin and Shen, Chun-Lin and Wang, Biao and Wu, Wen-Hai",
  abstract  = "The Decision-Making (DM) problem is investigated for Cooperative
               Multiple Target Attack in air combat. It is to search for a
               proper attack assignment of M friendly fighters, with multiple
               target attack capability, to N hostile fighters called targets
               to achieve an optimal missile-target attack effect. Thus,
               Missile-Target Assignment (MTA) is regarded as the main part of
               the DM problem and has to be solved firstly. Then, the DM
               solution is derived from the optimal MTA solution. To the MTA
               problem, a Heuristic Adaptive Genetic Algorithm (HAGA) is
               proposed to search for its optimal solution. The HAGA utilizes
               specific heuristic knowledge to improve the search capability of
               the Adaptive Genetic Algorithm (AGA). Simulation results show
               that the HAGA is effective and has much better performance than
               the AGA.",
  volume    =  1,
  pages     = "473--478",
  month     =  aug,
  year      =  2005,
  keywords  = "Aerospace engineering; Automation; Decision making; Delta
               modulation; Educational institutions; Genetic engineering;
               Heuristic algorithms; Missiles; Multiple target attack; Neural
               networks; cooperative; cooperative air combat; decision-making;
               genetic algorithm; genetic algorithms; heuristic; multiple
               target attack;Combat Optimization;Decision
               making;Missiles;neural\_networks;TALAF;genetic algorithm;genetic
               algorithms;AFRL\_STTR"
}

@BOOK{Shaw1985-db,
  title     = "Fighter Combat: Tactics and Maneuvering",
  author    = "Shaw, Robert L",
  abstract  = "This book provides a detailed discussion of one-on-one
               dog-fights and multi-fighter team work tactics. Full discussions
               of fighter aircraft and weapons systems performance are provided
               along with an explanation of radar intercept tactics and an
               analysis of the elements involved in the performance of fighter
               missions.",
  publisher = "Naval Institute Press",
  pages     = "428",
  year      =  1985,
  address   = "Annapolis, Md",
  keywords  = "Fighter plane combat;TALAF;AFRL\_STTR",
  language  = "en"
}

@ARTICLE{Chaslot2008-pn,
  title     = "{Monte-Carlo} Tree Search: A New Framework for Game {AI}",
  author    = "Chaslot, G and Bakkes, S and Szita, I and Spronck, P",
  abstract  = "Abstract Classic approaches to game AI require either a high
               quality of domain knowledge, or a long time to generate
               effective AI behaviour. These two characteristics hamper the
               goal of establishing challenging game AI. In this paper, we put
               forward Monte-Carlo Tree Search as a novel, unified framework to
               game AI. In the framework, randomized explorations of the search
               space are used to predict the most promising game actions. We
               will demonstrate ...",
  journal   = "AIIDE",
  publisher = "aaai.org",
  pages     = "216--217",
  year      =  2008,
  keywords  = "NotRead; demonstration papers;NotRead;TALAF;AFRL\_STTR"
}

@INCOLLECTION{Teach1981-nf,
  title     = "An Analysis of Physician Attitudes Regarding {Computer-Based}
               Clinical Consultation Systems",
  booktitle = "Use and Impact of Computers in Clinical Medicine",
  author    = "Teach, Randy L and Shortliffe, Edward H",
  editor    = "Anderson, James G and Jay, Stephen J",
  abstract  = "Physician attitudes regarding computer-based clinical decision
               aids and the effect of a 2-day tutorial on medical computing are
               studied. The results indicate that physicians are accepting of
               applications that enhance their patient management capabilities,
               but tend to oppose applications in which they perceive an
               infringement on their management role. Expectations about the
               effect of computing on current medical practices are found to be
               generally favorable, although considerable individual
               differences exist among subgroups. The study participants place
               substantial demands on the performance capabilities of
               acceptable consultations systems, and emphasize the need for
               humanlike interactive capabilities. The tutorial had no effect
               on attitudes regarding appropriate clinical uses of computers
               nor on expectations about the effect of the technology on
               medical practice. However, it did increase the participants'
               knowledge of medical computing and led to more informed demands
               on system performance. We discuss the implications of the study
               and offer suggestions for developing and implementing
               computer-based clinical decision aids.",
  publisher = "Springer New York",
  volume    =  558,
  pages     = "68--85",
  series    = "Computers and Medicine",
  year      =  1981,
  keywords  = "ConsultationSystems;TALAF;AFRL\_STTR;AFRL\_STTR/ConsultationSystems",
  language  = "en"
}

@INPROCEEDINGS{Bakkes2004-lk,
  title      = "{TEAM}: The {Team-Oriented} Evolutionary Adaptability Mechanism",
  booktitle  = "Entertainment Computing -- {ICEC} 2004",
  author     = "Bakkes, Sander and Spronck, Pieter and Postma, Eric",
  editor     = "Rauterberg, Matthias",
  abstract   = "Many commercial computer games allow a team of players to match
                their skills against another team, controlled by humans or by
                the computer. Most players prefer human opponents, since the
                artificial intelligence of a computer-controlled team is in
                general inferior. An adaptive mechanism for team-oriented
                artificial intelligence would allow computer-controlled
                opponents to adapt to human player behaviour, thereby providing
                a means of dealing with weaknesses in the game AI. Current
                commercial computer games lack challenging adaptive mechanisms.
                This paper proposes ``TEAM'', a novel team-oriented adaptive
                mechanism which is inspired by evolutionary algorithms. The
                performance of TEAM is evaluated in an experiment involving an
                actual commercial computer game (the Capture The Flag
                team-based game mode of the popular commercial computer game
                Quake III). The experimental results indicate that TEAM
                succeeds in endowing computer-controlled opponents with
                successful adaptive performance. We therefore conclude that
                TEAM can be successfully applied to generate challenging
                adaptive opponent behaviour in team-oriented commercial
                computer games.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "273--282",
  series     = "Lecture Notes in Computer Science",
  month      =  sep,
  year       =  2004,
  keywords   = "Artificial Intelligence (incl. Robotics); Computer Appl. in
                Arts and Humanities; Computer Applications; Information Systems
                Applications (incl. Internet); Multimedia Information Systems;
                User Interfaces and Human Computer Interaction;Artificial
                Intelligence (incl. Robotics);GameAI;TALAF;AFRL\_STTR",
  language   = "en",
  conference = "International Conference on Entertainment Computing"
}

@TECHREPORT{Wooldridge1982-et,
  title       = "Air combat maneuvering performance measurement state space
                 analysis",
  author      = "Wooldridge, Lee and Kelly, Michael J and Obermayer, Richard W
                 and Vreuls, Donald and Nelson, William H",
  institution = "DTIC Document",
  year        =  1982,
  keywords    = "NotRead;NotRead;TALAF;AFRL\_STTR"
}

@ARTICLE{Browne2012-lj,
  title    = "A Survey of Monte Carlo Tree Search Methods",
  author   = "Browne, C B and Powley, E and Whitehouse, D and Lucas, S M and
              Cowling, P I and Rohlfshagen, P and Tavener, S and Perez, D and
              Samothrakis, S and Colton, S",
  abstract = "Monte Carlo tree search (MCTS) is a recently proposed search
              method that combines the precision of tree search with the
              generality of random sampling. It has received considerable
              interest due to its spectacular success in the difficult problem
              of computer Go, but has also proved beneficial in a range of
              other domains. This paper is a survey of the literature to date,
              intended to provide a snapshot of the state of the art after the
              first five years of MCTS research. We outline the core
              algorithm's derivation, impart some structure on the many
              variations and enhancements that have been proposed, and
              summarize the results from the key game and nongame domains to
              which MCTS methods have been applied. A number of open research
              questions indicate that the field is ripe for future work.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  4,
  number   =  1,
  pages    = "1--43",
  month    =  mar,
  year     =  2012,
  keywords = "Artificial intelligence (AI); Computers; Decision theory; Game
              theory; Games; MCTS research; Markov processes; Monte Carlo
              methods; Monte Carlo tree search (MCTS); Monte carlo tree search
              methods; NotRead; artificial intelligence; bandit-based methods;
              computer Go; game search; key game; nongame domains; random
              sampling generality; tree searching; upper confidence bounds
              (UCB); upper confidence bounds for trees (UCT);Computers;Decision
              theory;Game theory;GameAI;Games;Markov
              processes;monte\_carlo;Monte Carlo tree search
              (MCTS);NotRead;TALAF;artificial intelligence;tree
              searching;AFRL\_STTR"
}

@TECHREPORT{L3_undated-ea,
  title    = "{WARFIGHTER} {READINESS} {SCIENCE} Contract No .:
              {FA8650-05-D-6502} Task Order 0013 : Operational Development and
              Validation of {Competency-Based} Methods and Tools for Enhancing
              Human Performance in Air and Space Warfighting Systems Prepared
              by : {AFRL-RH-WP-TR-20}",
  author   = "{L3}",
  keywords = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@ARTICLE{Mulgund2001-sp,
  title    = "{Large-Scale} Air Combat Tactics Optimization Using Genetic
              Algorithms",
  author   = "Mulgund, Sandeep and Harper, Karen and Zacharias, Greg",
  journal  = "J. Guid. Control Dyn.",
  volume   =  24,
  number   =  1,
  pages    = "140--142",
  year     =  2001,
  keywords = "Combat Metrics;Combat Optimization;TALAF;AFRL\_STTR",
  language = "en"
}

@ARTICLE{Liaw2013-wp,
  title    = "{EVOLVING} A {TEAM} {IN} A {FIRST-PERSON} {SHOOTER} {GAME} {BY}
              {USING} A {GENETIC} {ALGORITHM}",
  author   = "Liaw, Chishyan and Wang, Wei-Hua and Tsai, Ching-Tsorng and Ko,
              Chao-Hui and Hao, Gorden",
  abstract = "Evolving game agents in a first-person shooter game is important
              to game developers and players. Choosing a proper set of
              parameters in a multiplayer game is not a straightforward process
              because consideration must be given to a large number of
              parameters, and therefore requires effort and thorough knowledge
              of the game. Thus, numerous artificial intelligence (AI)
              techniques are applied in the designing of game characters'
              behaviors. This study applied a genetic algorithm to evolve a
              team in the mode of One Flag CTF in Quake III Arena to behave
              intelligently. The source code of the team AI is modified, and
              the progress of the game is represented as a finite state
              machine. A fitness function is used to evaluate the effect of a
              team's tactics in certain circumstances during the game. The team
              as a whole evolves intelligently, and consequently, effective
              strategies are discovered and applied in various situations. The
              experimental results have demonstrated that the proposed
              evolution method is capable of evolving a team's behaviors and
              optimizing the commands in a shooter game. The evolution strategy
              enhances the original game AI and assists game designers in
              tuning the parameters more effectively. In addition, this
              adaptive capability increases the variety of a game and makes
              gameplay more interesting and challenging.",
  journal  = "Appl. Artif. Intell.",
  volume   =  27,
  number   =  3,
  pages    = "199--212",
  year     =  2013,
  keywords = "NotRead;GameAI;NotRead;TALAF;AFRL\_STTR"
}

@TECHREPORT{McManus2003-og,
  title       = "A Parallel Distributed System for Aircraft Tactical Decision
                 Generation",
  author      = "McManus, John W",
  abstract    = "A research program investigating the use of artificial
                 intelligence (AI) techniques to aid in the development of a
                 Tactical Decision Generator (TDG) for Within Visual Range
                 (WVR) air combat engagements is discussed. The application of
                 AI programming and problem solving methods in the development
                 and implementation of a concurrent version of the Computerized
                 Logic For Air-to-Air Warfare Simulations (CLAWS) program, a
                 second generation TDG, is presented. Concurrent computing
                 environments and programming approaches are discussed and the
                 design and performance of a prototype concurrent TDG system
                 are presented.",
  publisher   = "NASA Langley Technical Report Server",
  institution = "NASA Langley Technical Report Server",
  year        =  2003,
  keywords    = "NotRead;NotRead;TALAF;AFRL\_STTR"
}

@ARTICLE{Busemeyer1993-gd,
  title    = "Decision field theory: a dynamic-cognitive approach to decision
              making in an uncertain environment",
  author   = "Busemeyer, J R and Townsend, J T",
  abstract = "Decision field theory provides for a mathematical foundation
              leading to a dynamic, stochastic theory of decision behavior in
              an uncertain environment. This theory is used to explain (a)
              violations of stochastic dominance, (b) violations of strong
              stochastic transitivity, (c) violations of independence between
              alternatives, (d) serial position effects on preference, (e)
              speed-accuracy trade-off effects in decision making, (f) the
              inverse relation between choice probability and decision time,
              (g) changes in the direction of preference under time pressure,
              (h) slower decision times for avoidance as compared with approach
              conflicts, and (i) preference reversals between choice and
              selling price measures of preference. The proposed theory is
              compared with 4 other theories of decision making under
              uncertainty.",
  journal  = "Psychol. Rev.",
  volume   =  100,
  number   =  3,
  pages    = "432--459",
  month    =  jul,
  year     =  1993,
  keywords = "AI benchmarks; AI-assisted game design; Artificial intelligence;
              Computational modeling; Evolutionary computation; Games;
              Important; NPC behavior learning; Planning; Seminars; artificial
              intelligence; commercial games; computational intelligence;
              computational narrative believable agents; computer games; human
              computer interaction; human-computer interaction; player
              modeling; player-game interaction; procedural content generation;
              search and planning;Computational modeling;Evolutionary
              computation;GameAI;Games;ai\_planning;TALAF;artificial
              intelligence;computational intelligence;computer games;player
              modeling;AFRL\_STTR",
  language = "en"
}

@INPROCEEDINGS{Gonsalves2004-hk,
  title      = "Software Toolkit for Optimizing Mission Plans ({STOMP})",
  booktitle  = "{AIAA} 1st Intelligent Systems Technical Conference",
  author     = "Gonsalves, Paul and Burge, Janet",
  abstract   = "Recent military actions have demonstrated the need for
                addressing time-sensitive and time-critical targeting. The
                capabilities of precision guided munitions and the further
                development of strike warfare platforms and tactics portend a
                huge increase in effectiveness and lethality of air operations
                and achieving the vision of the US Air Force's Global Strike
                Task Force concept. To fully realize the benefits of these
                strike capable assets and to address the challenges inherent in
                time-sensitive targeting, decision support systems are needed
                to assist warfighters in optimal allocation and near real-time
                re-deployment of air assets, and to support predictive
                battlespace awareness. While meeting a specific operational
                need, additional benefits can accrue through the employment of
                such decision support systems for virtual and constructive
                simulation based training, experimentation, and Command and
                Control (C2) system evaluation and acquisition. Here, we detail
                a Software Toolkit for Optimizing Mission Plans (STOMP). STOMP
                integrates a genetic algorithm-based mechanism to rapidly
                generate, analyze, and visualize mission plans in tandem with
                software interoperability to provide the requisite interface
                and connectivity with Air Force C2 systems and synthetic
                battlespace environments. Copyright \copyright{} 2004 by
                Charles River Analytics, Inc.",
  publisher  = "American Institute of Aeronautics and Astronautics",
  volume     =  1,
  pages      = "391--399",
  month      =  sep,
  year       =  2004,
  address    = "Reston, Virigina",
  keywords   = "Combat Optimization;TALAF;AFRL\_STTR",
  language   = "en",
  conference = "AIAA 1st Intelligent Systems Technical Conference"
}

@ARTICLE{Powlson2008-sj,
  title     = "When does nitrate become a risk for humans?",
  author    = "Powlson, David S and Addiscott, Tom M and Benjamin, Nigel and
               Cassman, Ken G and de Kok, Theo M and van Grinsven, Hans and
               L'Hirondel, Jean-Louis and Avery, Alex A and van Kessel, Chris",
  editor    = "Sch{\"o}lkopf, B and Platt, J C and Hoffman, T",
  abstract  = "Is nitrate harmful to humans? Are the current limits for nitrate
               concentration in drinking water justified by science? There is
               substantial disagreement among scientists over the
               interpretation of evidence on the issue. There are two main
               health issues: the linkage between nitrate and (i) infant
               methaemoglobinaemia, also known as blue baby syndrome, and (ii)
               cancers of the digestive tract. The evidence for nitrate as a
               cause of these serious diseases remains controversial. On one
               hand there is evidence that shows there is no clear association
               between nitrate in drinking water and the two main health issues
               with which it has been linked, and there is even evidence
               emerging of a possible benefit of nitrate in cardiovascular
               health. There is also evidence of nitrate intake giving
               protection against infections such as gastroenteritis. Some
               scientists suggest that there is sufficient evidence for
               increasing the permitted concentration of nitrate in drinking
               water without increasing risks to human health. However,
               subgroups within a population may be more susceptible than
               others to the adverse health effects of nitrate. Moreover,
               individuals with increased rates of endogenous formation of
               carcinogenic N-nitroso compounds are likely to be susceptible to
               the development of cancers in the digestive system. Given the
               lack of consensus, there is an urgent need for a comprehensive,
               independent study to determine whether the current nitrate limit
               for drinking water is scientifically justified or whether it
               could safely be raised.",
  journal   = "J. Environ. Qual.",
  publisher = "MIT Press",
  volume    =  37,
  number    =  2,
  pages     = "291--295",
  month     =  mar,
  year      =  2008,
  keywords  = "NotRead; bayesian learning; dynamic difficulty adjustment;
               match-making;NotRead;TALAF;AFRL\_STTR",
  language  = "en"
}

@MISC{Aikins1983-tw,
  title        = "Prototypical knowledge for expert systems - {ScienceDirect}",
  author       = "Aikins, Janice S",
  abstract     = "Knowledge of situations typically encountered in performing a
                  task is an important and useful source of information for
                  solving that task. This paper presents a system that uses a
                  representation of prototypical knowledge to guide computer
                  consultations, and to focus the application of production
                  rules used to represent inferential knowledge in the domain.
                  The explicit representation of control knowledge for each
                  prototypical situation is also emphasized. ?? 1983.",
  month        =  feb,
  year         =  1983,
  howpublished = "\url{http://www.sciencedirect.com/science/article/pii/0004370283900176}",
  note         = "Accessed: 2017-1-17",
  keywords     = "ConsultationSystems;TALAF;AFRL\_STTR;AFRL\_STTR/ConsultationSystems"
}

@ARTICLE{Togelius2016-oq,
  title    = "Why video games are essential for inventing artificial
              intelligence",
  author   = "Togelius, Julian",
  month    =  jan,
  year     =  2016,
  keywords = "GameAI;TALAF;AFRL\_STTR"
}

@TECHREPORT{Thomas1989-ys,
  title       = "Performance Measurement Development for Air Combat",
  author      = "Thomas, Gary S and Miller, David C",
  institution = "DTIC Document",
  year        =  1989,
  keywords    = "NotRead;NotRead;TALAF;AFRL\_STTR"
}

@ARTICLE{Munos2014-bk,
  title    = "From Bandits to {Monte-Carlo} Tree Search: The Optimistic
              Principle Applied to Optimization and Planning",
  author   = "Munos, Remi",
  abstract = "This work covers several aspects of the optimism in the face of
              uncertainty principle applied to large scale optimization
              problems under finite numerical budget. The initial motivation
              for the research reported here originated from the empirical
              success of the so-called Monte-Carlo Tree Search method
              popularized in computer-go and further extended to many other
              games as well as optimization and planning problems. Our
              objective is to contribute to the development of theoretical
              foundations of the field by characterizing the complexity of the
              underlying optimization problems and designing efficient
              algorithms with performance guarantees. The main idea presented
              here is that it is possible to decompose a complex decision
              making problem (such as an optimization problem in a large search
              space) into a sequence of elementary decisions, where each
              decision of the sequence is solved using a (stochastic)
              multi-armed bandit (simple mathematical model for decision making
              in stochastic environments). This so-called hierarchical bandit
              approach (where the reward observed by a bandit in the hierarchy
              is itself the return of another bandit at a deeper level)
              possesses the nice feature of starting the exploration by a
              quasi-uniform sampling of the space and then focusing
              progressively on the most promising area, at different scales,
              according to the evaluations observed so far, and eventually
              performing a local search around the global optima of the
              function. The performance of the method is assessed in terms of
              the optimality of the returned solution as a function of the
              number of function evaluations. Our main contribution to the
              field of function optimization is a class of hierarchical
              optimistic algorithms designed for general search spaces (such as
              metric spaces, trees, graphs, Euclidean spaces, ...) with
              different algorithmic instantiations depending on whether the
              evaluations are noisy or noiseless and whether some measure of
              the ''smoothness'' of the function is known or unknown. The
              performance of the algorithms depend on the local behavior of the
              function around its global optima expressed in terms of the
              quantity of near-optimal states measured with some metric. If
              this local smoothness of the function is known then one can
              design very efficient optimization algorithms (with convergence
              rate independent of the space dimension), and when it is not
              known, we can build adaptive techniques that can, in some cases,
              perform almost as well as when it is known.",
  journal  = "FNT in Machine Learning",
  volume   =  7,
  number   =  1,
  pages    = "1--129",
  year     =  2014,
  keywords = "Bandit theory; Important; Monte-Carlo Tree Search; NotRead;
              Optimism in the face of uncertainty; Upper Confidence
              Bounds;GameAI;TALAF;Important;NotRead;AFRL\_STTR"
}

@BOOK{Johansson2010-jx,
  title    = "Evaluating the performance of {TEWA} systems",
  author   = "Johansson, Fredrik",
  abstract = "It is in military engagements the task of the air defense to
              protect valuable assets such as air bases from being destroyed by
              hostile aircrafts and missiles. In order to fulfill this mission,
              the ...",
  year     =  2010,
  keywords = "TALAF;AFRL\_STTR",
  language = "eng"
}

@INPROCEEDINGS{Othman2012-fw,
  title     = "Simulation-based optimization of {StarCraft} tactical {AI}
               through evolutionary computation",
  booktitle = "2012 {IEEE} Conference on Computational Intelligence and Games
               ({CIG})",
  author    = "Othman, N and Decraene, J and Cai, W and Hu, N and Low, M Y H
               and Gouaillard, A",
  abstract  = "The development of competent AI for real-time strategy games
               such as StarCraft is made difficult by the myriad of strategic
               and tactical reasonings which must be performed concurrently. A
               significant portion of StarCraft gameplay is in managing
               tactical conflict with opposing forces. We present a modular
               framework for simulating AI vs. AI conflicts through an XML
               specification, whereby the behavioural and tactical components
               for each force can be varied. Evolutionary computation can be
               employed on aspects of the scenario to yield superior solutions.
               Through evolution, a StarCraft AI tournament bot achieved a
               success rate of 68\% against its original version. We also
               demonstrate the use of evolutionary computation to yield a
               tactical attack path to maximise enemy casualties. We believe
               that our framework can be used to perform automatic refinement
               on AI bots in StarCraft.",
  pages     = "394--401",
  year      =  2012,
  keywords  = "AI bots; Computational modeling; Evolutionary computation;
               Force; Games; NotRead; StarCraft gameplay; StarCraft tactical
               AI; XML; XML specification; artificial intelligence; computer
               games; inference mechanisms; real-time strategy games; real-time
               systems; simulation-based optimization; software agents;
               strategic reasoning; tactical conflict management; tactical
               reasoning;Computational modeling;Evolutionary
               computation;GameAI;Games;NotRead;TALAF;artificial
               intelligence;computer games;inference mechanisms;real-time
               strategy games;real-time systems;software agents;AFRL\_STTR"
}

@ARTICLE{Vallim2013-jb,
  title    = "Online behavior change detection in computer games",
  author   = "Vallim, Rosane M M and Andrade Filho, Jos{\'e} A and de Mello,
              Rodrigo F and de Carvalho, Andr{\'e} C P L F",
  abstract = "Abstract Player Modelling has been receiving much attention from
              the game community in the recent years. The ability to build
              accurate models of player behavior can be useful in many aspects
              of a game. One important aspect is the tracking of a player's
              behavior along time, informing every time a change is perceived.
              This way, the game Artificial Intelligence can adapt itself to
              better respond to this new behavior. In order to build models of
              player behavior, researchers frequently resort to Machine
              Learning techniques. Such methods work on previously recorded
              game metrics representing player's interactions with the game
              environment. However, if the player changes styles over time, the
              constructed models get out of date. In order to address this
              drawback, this work proposes the use of and incremental learning
              technique to track a player's behavior during his/her interaction
              with the game environment. Our approach attempts to automatically
              detect the moments in time when the player changes behavior. We
              apply a change detection technique from the area of Data Stream
              Mining that is based on incremental clustering and novelty
              detection. We also propose three modifications to the original
              technique, in order to formalize change detection, improve
              detection rate and reduce detection delay. Simulations were
              performed considering data produced by the Unreal Tournament
              game, showing the applicability of the method to online tracking
              of a player's behavior and informing whenever behavior changes
              occur.",
  journal  = "Expert Syst. Appl.",
  volume   =  40,
  number   =  16,
  pages    = "6258--6265",
  month    =  nov,
  year     =  2013,
  keywords = "Behavior change detection; Data; Data Stream Mining; Mining;
              NotRead; Online player modeling;
              Stream;GameAI;NotRead;TALAF;AFRL\_STTR"
}

@INPROCEEDINGS{Mora2012-xl,
  title     = "Dealing with noisy fitness in the design of a {RTS} game bot",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture
               Notes in Artificial Intelligence and Lecture Notes in
               Bioinformatics)",
  author    = "Mora, Antonio M and Fern{\'a}ndez-Ares, Antonio and
               Merelo-Guerv{\'o}s, Juan Juli{\'a}n and Garc{\'\i}a-S{\'a}nchez,
               Pablo",
  abstract  = "This work describes an evolutionary algorithm (EA) for evolving
               the constants, weights and probabilities of a rule-based
               decision engine of a bot designed to play the Planet Wars game.
               The evaluation of the individuals is based on the result of some
               non-deterministic combats, whose outcome depends on random draws
               as well as the enemy action, and is thus noisy. This noisy
               fitness is addressed in the EA and then, its effects are deeply
               analysed in the experimental section. The conclusions shows that
               reducing randomness via repeated combats and re-evaluations
               reduces the effect of the noisy fitness, making then the EA an
               effective approach for solving the problem.",
  publisher = "Springer Berlin Heidelberg",
  volume    = "7248 LNCS",
  pages     = "234--244",
  series    = "Lecture Notes in Computer Science",
  month     =  apr,
  year      =  2012,
  keywords  = "Artificial Intelligence (incl. Robotics); Computation by
               Abstract Devices; Computer Communication Networks; Image
               Processing and Computer Vision; Math Applications in Computer
               Science; NotRead; Programming Techniques;Artificial Intelligence
               (incl. Robotics);GameAI;NotRead;TALAF;AFRL\_STTR",
  language  = "en"
}

@INPROCEEDINGS{Mulgund1998-ad,
  title     = "Air combat tactics optimization using stochastic genetic
               algorithms",
  booktitle = "Systems, Man, and Cybernetics, 1998. 1998 {IEEE} International
               Conference on",
  author    = "Mulgund, S and Harper, K and Krishnakumar, K and Zacharias, G",
  abstract  = "Describes the development of a software tool for optimizing
               large-scale air combat tactics using stochastic genetic
               algorithms. The tool integrates four key components: (1)
               autonomous blue/red player agents, with their individual
               aircraft and tactics; (2) an engagement simulator used to play
               out a tactical scenario; (3) performance metrics reflecting
               engagement outcome and tactical advantage; and (4) a GA
               ``engine'' for performance-based optimization of blue team
               tactics. The tool's capabilities are demonstrated through the
               optimization of blue team formation and intercept geometry in a
               series of tactical engagements. The tactics implementation uses
               a hierarchical concept that builds large formation tactics from
               small conventional fighting units, facilitating the design of
               tactics compatible with existing air combat principles",
  volume    =  4,
  pages     = "3136--3141 vol.4",
  month     =  oct,
  year      =  1998,
  keywords  = "Aircraft; Algorithm design and analysis; Genetic algorithms;
               Important; Measurement; Optimization methods; Rivers; Search
               methods; Software tools; Stochastic processes; USA Councils; air
               combat tactics optimization; autonomous blue/red player agents;
               engagement simulator; genetic algorithms; intercept geometry;
               military computing; performance metrics; stochastic genetic
               algorithms; tactical engagements; tactical
               scenario;Aircraft;Combat Metrics;Combat
               Optimization;Measurement;Optimization methods;Stochastic
               processes;TALAF;genetic algorithms;AFRL\_STTR"
}

@MISC{Paper2015-lw,
  title        = "{ResearchGate} - Share and discover research",
  author       = "Paper, Conference",
  abstract     = "Find over 100+ million publications, 11+ million researchers
                  and 1 million answers to research questions. ResearchGate is
                  a network dedicated to science and research. Connect,
                  collaborate and discover scientific publications, jobs and
                  conferences. All for free.",
  year         =  2015,
  howpublished = "\url{http://www.researchgate.net/profile/Frederik{_}Schadd/publication/221024452{_}Opponent{_}Modeling{_}}",
  note         = "Accessed: 2017-1-17",
  keywords     = "NotRead;GameAI;NotRead;TALAF;AFRL\_STTR"
}

@ARTICLE{Kelly1988-wt,
  title    = "Performance Measurement during Simulated {Air-to-Air} Combat",
  author   = "Kelly, Michael J",
  abstract = "Measurement and assessment of operator performance of complex
              tasks with decisional and psychomotor components of which only
              ultimate outcome measures are well defined provide difficult
              methodological challenges. One such task is the practice of
              air-to-air combat skills in simulated combat environments such as
              the Air Force Air Combat Maneuvering Instrumentation (ACMI) or
              ground-based visual flight simulators. Measurement and assessment
              of pilot performance in these environments are especially
              challenging because of the special cognitive and psychomotor
              skills involved in the freeplay and gamesmanship of two or more
              competing pilots. A review of the existing approaches to
              automated aircrew performance measurement was performed. Although
              the most common measurement models, using positional advantage or
              disadvantage, provide an adequate performance metric, other
              measures, such as control manipulation and the management of
              kinetic and potential energy, must be added to provide a refined
              performance measurement algorithm.",
  journal  = "Human Factors: The Journal of the Human Factors and Ergonomics
              Society",
  volume   =  30,
  number   =  4,
  pages    = "495--506",
  year     =  1988,
  keywords = "Important; NotRead;Combat Metrics;TALAF;AFRL\_STTR"
}

@ARTICLE{Clough2002-as,
  title    = "Metrics, schmetrics! How the heck do you determine a {UAV's}
              autonomy anyway?",
  author   = "Clough, Bt",
  abstract = "The recently released DoD Unmanned Aerial Vehicles Roadmap
              discusses advancements in UAV autonomy in terms of autonomous
              control levels (ACL). The ACL concept was pioneered by
              researchers in the Air Force Research Laboratory's Air Vehicles
              Directorate who are charged with developing autonomous air
              vehicles. In the process of developing intelligent autonomous
              agents for UAV control systems we were constantly challenged to
              ``tell us how autonomous a UAV is, and how do you think it can be
              measured?'' Usually we hand-waved away the argument and hoped the
              questioner will go away since this is a very subjective, and
              complicated, subject, but within the last year we've been
              directed to develop national intelligent autonomous UAV control
              metrics - an IQ test for the flyborgs, if you will. The ACL chart
              is the result. We've done this via intense discussions with other
              government labs and industry, and this paper covers the agreed
              metrics (an extension of the OODA - observe, orient, decide, and
              act - loop) as well as the precursors, ``dead-ends'', and
              out-and-out flops investigated to get there.",
  journal  = "Security",
  number   =  990,
  pages    = "313--319",
  month    =  aug,
  year     =  2002,
  keywords = "*ACL(AUTONOMOUS CONTROL LEVELS); *AUTONOMY METRICS; *DETECTORS;
              *DRONES; *MACHINE INTELLIGENCE METRICS; *MEASUREMENT; *PLANNING
              PROGRAMMING BUDGETING; *WORKSHOPS; AERONAUTICAL LABORATORIES; AIR
              FORCE RESEARCH; Adjustable autonomy; CONTROL SYSTEMS; METRICS;
              Pilotless Aircraft; SELF OPERATION; SITUATIONAL AWARENESS;
              SURVIVAL SPACE;AFRL;CONTROL
              SYSTEMS;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL",
  language = "en"
}

@BOOK{Caserta2010-eo,
  title     = "Applications of Evolutionary Computation",
  author    = "Caserta, Marco and Ramirez, Adriana and Vo{\ss}, Stefan",
  editor    = "Di Chio, Cecilia and Brabazon, Anthony and Di Caro, Gianni A and
               Ebner, Marc and Farooq, Muddassar and Fink, Andreas and Grahl,
               J{\"o}rn and Greenfield, Gary and Machado, Penousal and O'Neill,
               Michael and Tarantino, Ernesto and Urquhart, Neil",
  abstract  = "We present a math-heuristic algorithm for the lot sizing problem
               with carryover. The proposed algorithm uses mathematical\$\$n
               programming techniques in a metaheuristic fashion to iteratively
               solve smaller portions of the original problem. More
               specifically,\$\$n we draw ideas from the corridor method to
               design and impose exogenous constraints on the original problem
               and, subsequently,\$\$n we solve to optimality the constrained
               problem using a MIP solver. The algorithm iteratively builds new
               corridors around the\$\$n best solution found within each
               corridor and, therefore, explores adjacent portions of the
               search space. In the attempt of\$\$n fostering diversification
               while exploring the original search space, we generate a pool of
               incumbent solutions for the corridor\$\$n method and, therefore,
               we reapply the corridor method using different starting points.
               The algorithm has been tested on instances\$\$n of a standard
               benchmark library and the reported results show the robustness
               and effectiveness of the proposed scheme.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  6025,
  pages     = "462--471",
  series    = "Lecture Notes in Computer Science",
  year      =  2010,
  address   = "Berlin, Heidelberg",
  keywords  = "GameAI; TALAF; agent-based model; conformity; emergent; game
               theory;GameAI;TALAF;AFRL\_STTR"
}

@INPROCEEDINGS{Smith2011-aa,
  title     = "An inclusive view of player modeling",
  booktitle = "Proceedings of the 6th International Conference on Foundations
               of Digital Games",
  author    = "Smith, Adam M and Lewis, Chris and Hullet, Kenneth and Sullivan,
               Anne",
  abstract  = "``Player modeling'' is a loose concept. It can equally apply to
               everything from a predictive model of player actions resulting
               from machine learning to a designer's description of a player's
               expected reactions in response to some piece of game content.
               This lack of a precise terminology prevents practitioners from
               quickly finding introductions to applicable modeling methods or
               determining viable alternatives to their own techniques. We
               introduce a vocabulary that distinguishes between the major
               existing player modeling applications and techniques. Four
               facets together define the kind for a model: the scope of
               application, the purpose of use, the domain of modeled details,
               and the source of a model's derivation or motivation. This
               vocabulary allows the identification of relevant player modeling
               methods for particular problems and clarifies the roles that a
               player model can take.",
  publisher = "ACM",
  pages     = "301--303",
  month     =  jun,
  year      =  2011,
  keywords  = "Important; TALAF; game design; games; player modeling;
               taxonomy;Important;TALAF;GameAI;AFRL\_STTR"
}

@ARTICLE{Paranjape2006-qr,
  title    = "Combat aircraft agility metrics-a review",
  author   = "Paranjape, Aditya A and Ananthkrishnan, N",
  journal  = "J. Aerosp. Sci. Technol.",
  volume   =  58,
  number   =  2,
  pages    = "143--154",
  year     =  2006,
  keywords = "Combat Metrics;TALAF;AFRL\_STTR"
}

@TECHREPORT{McManus1990-pq,
  title       = "Artificial Intelligence ({AI}) Based Tactical Guidance For
                 Fighter Aircraft",
  author      = "McManus, John W and Goodrich, Kenneth H",
  abstract    = "A research program investigating the use of Artificial
                 Intelligence (AI) techniques to aid in the development of a
                 Tactical Decision Generator (TDG) for Within Visual Range
                 (WVR) air combat engagements is discussed. The application of
                 AI programming and problem solving methods in the development
                 and implementation of the Computerized Logic For Air-to-Air
                 Warfare Simulations (CLAWS), a second generation TDG, is
                 presented. The Knowledge-Based Systems used by CLAWS to aid in
                 the tactical decision-making process are outlined in detail,
                 and the results of tests to evaluate the performance of CLAWS
                 versus a baseline TDG developed in FORTRAN to run in real-time
                 in the Langley Differential Maneuvering Simulator (DMS), are
                 presented. To date, these test results have shown significant
                 performance gains with respect to the TDG baseline in
                 one-versus-one air combat engagements, and the AI-based TDG
                 software has proven to be much easier to modify and maintain
                 than the baseline FORTRAN TDG programs. Alternate computing
                 environments and programming approaches, including the use of
                 parallel algorithms and heterogeneous computer networks are
                 discussed, and the design and performance of a prototype
                 concurrent TDG system are presented.",
  institution = "NASA Langley Technical Report Server",
  year        =  1990,
  keywords    = "Combat Metrics;Important;TALAF;AFRL\_STTR"
}

@INPROCEEDINGS{Holmgard2014-zq,
  title     = "Evolving personas for player decision modeling",
  booktitle = "2014 {IEEE} Conference on Computational Intelligence and Games",
  author    = "Holmg{\aa}rd, C and Liapis, A and Togelius, J and Yannakakis, G
               N",
  abstract  = "This paper explores how evolved game playing agents can be used
               to represent a priori defined archetypical ways of playing a
               test-bed game, as procedural personas. The end goal of such
               procedural personas is substituting players when authoring game
               content manually, procedurally, or both (in a mixed-initiative
               setting). Building on previous work, we compare the performance
               of newly evolved agents to agents trained via Q-learning as well
               as a number of baseline agents. Comparisons are performed on the
               grounds of game playing ability, generalizability, and
               conformity among agents. Finally, all agents' decision making
               styles are matched to the decision making styles of human
               players in order to investigate whether the different methods
               can yield agents who mimic or differ from human decision making
               in similar ways. The experiments performed in this paper
               conclude that agents developed from a priori defined objectives
               can express human decision making styles and that they are more
               generalizable and versatile than Q-learning and hand-crafted
               agents.",
  pages     = "1--8",
  month     =  aug,
  year      =  2014,
  keywords  = "Decision making; Important; Navigation; Q-learning; agents
               conformity; agents decision making styles; archetypical ways;
               authoring; baseline agents; computer games; evolved game playing
               agents; evolving personas; game content; game playing ability;
               game playing generalizability; hand-crafted agents; human
               decision making; human players; learning (artificial
               intelligence); multi-agent systems; player decision modeling;
               procedural personas; test-bed game playing;Decision
               making;GameAI;Q-learning;TALAF;computer games;learning
               (artificial intelligence);AFRL\_STTR"
}

@INPROCEEDINGS{Drachen2009-ao,
  title     = "Player modeling using self-organization in Tomb Raider:
               Underworld",
  booktitle = "2009 {IEEE} Symposium on Computational Intelligence and Games",
  author    = "Drachen, A and Canossa, A and Yannakakis, G N",
  abstract  = "We present a study focused on constructing models of players for
               the major commercial title Tomb Raider: Underworld (TRU).
               Emergent self-organizing maps are trained on high-level playing
               behavior data obtained from 1365 players that completed the TRU
               game. The unsupervised learning approach utilized reveals four
               types of players which are analyzed within the context of the
               game. The proposed approach automates, in part, the traditional
               user and play testing procedures followed in the game industry
               since it can inform game developers, in detail, if the players
               play the game as intended by the game design. Subsequently,
               player models can assist the tailoring of game mechanics in
               real-time for the needs of the player type identified.",
  pages     = "1--8",
  year      =  2009,
  keywords  = "Automatic testing; Computer industry; Computerized monitoring;
               Data mining; Emergent self-organizing maps; Gold; Instruments;
               Player modeling; Production; Self organizing feature maps; Tomb
               Raider Underworld; Tomb Raider: Underworld; Toy industry;
               Unsupervised learning; computer games; emergent self-organizing
               maps; game design; game industry; high-level playing behavior
               data obtained; learning (artificial intelligence); player
               modeling; self-organising feature maps; unsupervised learning;
               user modelling;GameAI;TALAF;computer games;learning (artificial
               intelligence);player modeling;AFRL\_STTR"
}

@INPROCEEDINGS{Weber2011-fq,
  title     = "A Particle Model for State Estimation in {Real-Time} Strategy
               Games",
  booktitle = "Seventh Artificial Intelligence and Interactive Digital
               Entertainment Conference",
  author    = "Weber, Ben George and Mateas, Michael and Jhala, Arnav",
  abstract  = "A big challenge for creating human-level game AI is building
               agents capable of operating in imperfect information
               environments. In real-time strategy games the technological
               progress of an opponent and locations of enemy units are
               partially observable. To overcome this limitation, we explore a
               particle-based approach for estimating the location of enemy
               units that have been encountered. We represent state estimation
               as an optimization problem, and automatically learn parameters
               for the particle model by mining a corpus of expert StarCraft
               replays. The particle model tracks opponent units and provides
               conditions for activating tactical behaviors in our StarCraft
               bot. Our results show that incorporating a learned particle
               model improves the performance of EISBot by 10\% over baseline
               approaches.",
  pages     = "103--108",
  month     =  oct,
  year      =  2011,
  keywords  = "Real-Time Strategy Games;GameAI;TALAF;AFRL\_STTR",
  language  = "en"
}

@TECHREPORT{Kelly1979-in,
  title       = "Air Combat Maneuvering Performance Measurement",
  author      = "Kelly, Michael J and Wooldridge, Lee and Hennessy, Robert T
                 and Vreuls, Donald and Barnebey, Steve F and Cotton, John C
                 and Reed, John C",
  abstract    = "Due to the complex, dynamic and fast-moving nature of the air
                 combat task, performance assessment during air-to-air combat
                 provides many unique measurement problems. A combined
                 analytical and empirical technical approach was used to
                 develop a candidate measurement structure and algorithm for
                 the measurement of pilot performance during one-versus-one air
                 combat maneuvering. Nearly all of 28 candidate measures were
                 found to discriminate between high and low skilled pilots
                 during free engagements on the Simulator for Air-to-Air
                 Combat. Discriminant analyses provided a measurement algorithm
                 consisting of 13 measures which accounted for 51\% of the
                 variance in the performance data and which predicted
                 membership in high or low skill groups with 92\% accuracy.",
  volume      =  23,
  pages       = "324--328",
  institution = "CANYON RESEARCH GROUP INC WESTLAKE VILLAGE CA, CANYON RESEARCH
                 GROUP INC WESTLAKE VILLAGE CA",
  month       =  sep,
  year        =  1979,
  keywords    = "Combat Metrics; TALAF;TALAF;AFRL\_STTR",
  language    = "en"
}

@ARTICLE{Yannakakis2009-xf,
  title    = "{Real-Time} Game Adaptation for Optimizing Player Satisfaction",
  author   = "Yannakakis, G N and Hallam, J",
  abstract = "A methodology for optimizing player satisfaction in games on the
              ``playware'' physical interactive platform is demonstrated in
              this paper. Previously constructed artificial neural network user
              models, reported in the literature, map individual playing
              characteristics to reported entertainment preferences for
              augmented-reality game players. An adaptive mechanism then
              adjusts controllable game parameters in real time in order to
              improve the entertainment value of the game for the player. The
              basic approach presented here applies gradient ascent to the user
              model to suggest the direction of parameter adjustment that leads
              toward games of higher entertainment value. A simple rule set
              exploits the derivative information to adjust specific game
              parameters to augment the entertainment value. Those adjustments
              take place frequently during the game with interadjustment
              intervals that maintain the user model's accuracy. Performance of
              the adaptation mechanism is evaluated using a game survey
              experiment. Results indicate the efficacy and robustness of the
              mechanism in adapting the game according to a user's individual
              playing features and enhancing the gameplay experience. The
              limitations and the use of the methodology as an effective
              adaptive mechanism for entertainment capture and augmentation are
              discussed.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  1,
  number   =  2,
  pages    = "121--133",
  month    =  jun,
  year     =  2009,
  keywords = "Augmented-reality games; artificial intelligence; artificial
              neural network user model; augmented reality; augmented-reality
              game player; computer games; controllable game parameter; game
              survey experiment; gradient ascent; neural nets; neuro-evolution;
              player satisfaction; player satisfaction optimisation; playware
              physical interactive platform; real-time adaptation; real-time
              game adaptation; user modeling;GameAI;TALAF;artificial
              intelligence;computer games;AFRL\_STTR"
}

@INPROCEEDINGS{Huynh1987-kl,
  title      = "Numerical optimization of air combat maneuvers",
  booktitle  = "Guidance, Navigation and Control Conference",
  author     = "Huynh, H and Costes, P H and Aumasson, C",
  publisher  = "American Institute of Aeronautics and Astronautics",
  pages      = "647--658",
  month      =  aug,
  year       =  1987,
  address    = "Reston, Virigina",
  keywords   = "Combat Metrics;Important;TALAF;AFRL\_STTR",
  language   = "en",
  conference = "Guidance, Navigation and Control Conference"
}

@ARTICLE{Schreiber2007-ks,
  title    = "Distributed mission operations within-simulator training
              effectiveness baseline study. Volume 2. metric development and
              objectively quantifying the degree of learning",
  author   = "Schreiber, Brian T and Stock, William A and Bennett, Jr, Winston",
  abstract = "The current work reports only the objective data from
              AFRL-HE-AZ-TR-2006-0015, Volume I, Distributed Mission Operations
              Within-Simulator Training Effectiveness: Summary Report, but here
              we expand the reporting of objective data both in depth and
              breadth.We examined F-16 pilots participating in week-long
              Distributed Mission Operation (DMO) training exercises and
              compared extensive computer-collected data between
              beginning-of-week and end-of-week pilot performance on
              mirror-image scenarios. The DMO research environment in Mesa, AZ
              consisted of four high-fidelity F-16 simulators and one
              high-fidelity Airborne Warning and Control System simulator.
              Participating F-16 teams flew over 40 total scenarios according
              to a five-day syllabus, book-ended on Monday and Friday by
              mirror-image point defense air combat benchmark scenarios. Seven
              mission outcome measures were found to be significantly better on
              Friday than Monday: A 58.33\% decrease in enemy strikers reaching
              their target, 38.10\% greater distance from the base the F-16s
              disposed of the strikers, 54.77\% fewer F-16 mortalities, 75.26\%
              more enemy striker kills (before reaching base), 6.82\% higher
              proportion of Viper Advanced Medium Range Air-to-Air Missile
              (AMRAAM) shots resulting in a kill, 51.60\% lower proportion of
              enemy Alamo missile shots resulting in a kill, and a highly
              impressive 314.21\% increase in an overall summary scoring scheme
              developed by subject matter experts. Significant trends were also
              found for a number of other metrics assessing skills. Of all the
              measures investigated in the current work, not a single
              offensive/defensive trade-off was observed, which significantly
              strengthens our conclusion that significant within-simulator
              learning took place.",
  journal  = "Star",
  volume   =  45,
  number   =  5,
  year     =  2007,
  keywords = "10: Aerospace Engineering (General) (MT); 99: General (AH); Aero;
              Air combat; Benchmarking; Control systems; Folder -
              AFRL\{\_\}Wink; Important; Learning; Mechanical \& Transportation
              Engineering (MT); Mechanical \{\&\} Transportation Engineering
              (MT); Mesas; Military aircraft; Military planes; Missiles;
              Missions; Mortality; NotRead; Pilot performance; Pilots; Raw
              materials; Scoring; Shot; Simulators; Tradeoffs; Training;
              Warning;AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@BOOK{Anders_Ericsson2009-cu,
  title     = "Development of Professional Expertise: Toward Measurement of
               Expert Performance and Design of Optimal Learning Environments",
  author    = "Anders Ericsson, K",
  abstract  = "Professionals such as medical doctors, airplane pilots, lawyers,
               and technical specialists find that some of their peers have
               reached high levels of achievement that are difficult to measure
               objectively. In order to understand to what extent it is
               possible to learn from these expert performers for the purpose
               of helping others improve their performance, we first need to
               reproduce and measure this performance. This book is designed to
               provide the first comprehensive overview of research on the
               acquisition and training of professional performance as measured
               by objective methods rather than by subjective ratings by
               supervisors. In this collection of articles, the world's
               foremost experts discuss methods for assessing the experts'
               knowledge and review our knowledge on how we can measure
               professional performance and design training environments that
               permit beginning and experienced professionals to develop and
               maintain their high levels of performance, using examples from a
               wide range of professional domains.",
  publisher = "Cambridge University Press",
  month     =  jun,
  year      =  2009,
  keywords  = "AFRL;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL",
  language  = "en"
}

@ARTICLE{Schreiber2007-fl,
  title    = "Distributed mission operations within-simulator training
              effectiveness baseline study. Volume 1. Summary report",
  author   = "Schreiber, Brian T and Bennett, Jr., Winston and Bennett, Jr,
              Winston and Bennett, Jr., Winston and Bennett, Jr, Winston",
  abstract = "Distributed Mission Operations (DMO) training consists of
              multiplayer networked environments enabling warfighting training
              on higher-order individual and team-oriented skills.
              Surprisingly, only sparse DMO training effectiveness literature
              can be found and very few studies contain objective data. The
              dataset used in this research represents the largest DMO
              effectiveness dataset known to exist today (76 teams/384 pilots
              on over 3,000 engagements), containing 33 months' worth of
              multi-faceted DMO data, including objective data from the
              simulators, multiple participant surveys, subject matter expert
              (SME) ratings of performance, and knowledge structure tests.
              Observed performance differences between the pre- and post-test
              mirror-image point-defense assessment sessions served as the
              primary basis for the evaluation. Results were dramatic: On the
              post-test, 58.33\{\{\}\{\%\}\{\}\} fewer enemy strikers reached
              their target and there were 54.77\{\{\}\{\%\}\{\}\} fewer F-16
              mortalities. Furthermore, there were corroborating significant
              improvements from the numerous measured skill metrics (e.g.,
              weapons employment), SME expert observer ratings, and participant
              self-report opinion ratings. These converging results provide
              substantial evidence that pilots become much more proficient on
              key aspects of combat mission objectives as a function of
              training within the simulator. Finding highly significant
              performance differences across multiple datasets between the pre-
              and post-tests with a combat-ready participant pool in a complex
              task/environment forms a formidable argument that DMO training
              yields considerable within-simulator warfighter competency
              improvement. In this report, we summarize the different dataset
              classes, overview the primary hypotheses and results associated
              with each, and discuss the convergence of the datasets to
              illustrate the 'big picture' DMO training effectiveness. As such,
              more detailed hypotheses, analyses, and discussions are contained
              in separate reports (Vols. II through V).",
  journal  = "Star",
  volume   =  45,
  number   =  5,
  year     =  2007,
  keywords = "10: Aerospace Engineering (General) (MT); 99: General (AH); AFRL;
              Aero; Convergence; DMO; Distributed Mission Operations;
              Employment; Folder - AFRL\{\{\}\{\_\}\{\}\}Wink; Hypotheses; MEC;
              Mechanical \{\&\} Transportation Engineering (MT); Mechanical
              \{\{\}\{\&\}\{\}\} Transportation Engineering (M; Military
              aircraft; Military planes; Mission Essential Competencies;
              Missions; Mortality; Networked environments; NotRead; Observers;
              Pilots; Pools; Ratings; Simulators; Skill acquisition; Surveys;
              TALAF; Training; Training effectiveness; Warfighter training;
              Weapons;AFRL;NotRead;TALAF;AFRL\_STTR;AFRL\_STTR/AFRL"
}

@INCOLLECTION{Goodrich1990-ei,
  title     = "An integrated environment for tactical guidance research and
               evaluation",
  booktitle = "Orbital Debris Conference: Technical Issues andFuture Directions",
  author    = "Goodrich, Kenneth and Mcmanus, John",
  publisher = "American Institute of Aeronautics and Astronautics",
  month     =  apr,
  year      =  1990,
  keywords  = "TALAF;AFRL\_STTR",
  language  = "en"
}

@ARTICLE{Wang2013-fy,
  title     = "Probabilistic movement modeling for intention inference in
               human-robot interaction",
  author    = "Wang, Z and Mulling, K and Deisenroth, M P and Ben Amor, H and
               Vogt, D and Scholkopf, B and Peters, J",
  abstract  = "Inference of human intention may be an essential step towards
               understanding human actions [21] and is
               hence\textbackslashnimportant for realizing efficient
               human-robot interaction. In this paper, we propose the
               Intention-Driven Dynamics Model (IDDM), a latent variable model
               for inferring unknown human intentions. We train the model based
               on observed human behaviors/actions and we introduce an
               approximate inference algorithm to efficiently infer the human's
               intention from an ongoing action.\textbackslashnWe verify the
               feasibility of the IDDM in two scenarios, i.e., target inference
               in robot table tennis and action recognition for interactive
               humanoid robots. In both tasks, the IDDM achieves substantial
               improvements over state-of-the-art regression and
               classification.",
  journal   = "Int. J. Rob. Res.",
  publisher = "MIT Press",
  volume    =  32,
  number    =  7,
  pages     = "841--858",
  series    = "Adaptive computation and machine learning",
  month     =  jun,
  year      =  2013,
  address   = "Cambridge, Mass",
  keywords  = "Approximate inference; Gaussian process; Important; intention
               inference;Folder - Spring2016;TALAF;AFRL\_STTR",
  language  = "en"
}

@INPROCEEDINGS{Cole2004-fh,
  title     = "Using a genetic algorithm to tune first-person shooter bots",
  booktitle = "Proceedings of the 2004 Congress on Evolutionary Computation
               ({IEEE} Cat. {No.04TH8753})",
  author    = "Cole, N and Louis, S J and Miles, C",
  abstract  = "First-person shooter robot controllers (bots) are generally
               rule-based expert systems written in C/C++. As such, many of the
               rules are parameterized with values, which are set by the
               software designer and finalized at compile time. The
               effectiveness of parameter values is dependent on the knowledge
               the programmer has about the game. Furthermore, parameters are
               non-linearly dependent on each other. This paper presents an
               efficient method for using a genetic algorithm to evolve sets of
               parameters for bots which lead to their playing as well as bots
               whose parameters have been tuned by a human with expert
               knowledge about the game's strategy. This indicates genetic
               algorithms as being a potentially useful method for tuning bots.",
  volume    =  1,
  pages     = "139--145 Vol.1",
  month     =  jun,
  year      =  2004,
  keywords  = "CONTROL SYSTEMS; Counter Strike game; Counting circuits; Expert
               systems; Game theory; Logic; NotRead; Programming profession;
               Robot control; Weapons; artificial intelligence; bots tuning;
               computer games; first-person shooter robot controllers; game
               artificial intelligence; genetic algorithm; genetic algorithms;
               software agents; software design;CONTROL SYSTEMS;Game
               theory;GameAI;NotRead;TALAF;artificial intelligence;computer
               games;genetic algorithm;genetic algorithms;software
               agents;AFRL\_STTR"
}

@INPROCEEDINGS{Yannakakis2012-jp,
  title     = "Game {AI} revisited",
  booktitle = "Proceedings of the 9th conference on Computing Frontiers",
  author    = "Yannakakis, Geogios N",
  publisher = "ACM",
  pages     = "285--292",
  month     =  may,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "game AI flagships; game artificial intelligence; game data
               mining; player experience modeling; procedural content
               generation;GameAI;TALAF;AFRL\_STTR"
}

@ARTICLE{Bakkes2009-jh,
  title    = "Rapid and Reliable Adaptation of Video Game {AI}",
  author   = "Bakkes, S and Spronck, P and den Herik, J van",
  abstract = "Current approaches to adaptive game AI typically require numerous
              trials to learn effective behavior (i.e., game adaptation is not
              rapid). In addition, game developers are concerned that applying
              adaptive game AI may result in uncontrollable and unpredictable
              behavior (i.e., game adaptation is not reliable). These
              characteristics hamper the incorporation of adaptive game AI in
              commercially available video games. In this paper, we discuss an
              alternative to these current approaches. Our alternative approach
              to adaptive game AI has as its goal adapting rapidly and reliably
              to game circumstances. Our approach can be classified in the area
              of case-based adaptive game AI. In the approach, domain knowledge
              required to adapt to game circumstances is gathered automatically
              by the game AI, and is exploited immediately (i.e., without
              trials and without resource-intensive learning) to evoke
              effective behavior in a controlled manner in online play. We
              performed experiments that test case-based adaptive game AI on
              three different maps in a commercial real-time strategy (RTS)
              game. From our results, we may conclude that case-based adaptive
              game AI provides a strong basis for effectively adapting game AI
              in video games.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  1,
  number   =  2,
  pages    = "93--104",
  month    =  jun,
  year     =  2009,
  keywords = "artificial intelligence;computer games;AI;adaptive
              game;artificial intelligence;real-time strategy;video
              game;Adaptive behavior;game AI;rapid adaptation;real-time
              strategy (RTS) games;reliable adaptation;GameAI;TALAF;AFRL\_STTR"
}

@ARTICLE{Marshall2013-ef,
  title    = "Games, Gameplay, and {BCI}: The State of the Art",
  author   = "Marshall, D and Coyle, D and Wilson, S and Callaghan, M",
  abstract = "Brain-computer interfaces (BCIs) and basic computer games have
              been interconnected since BCI development began, exploiting
              gameplay elements as a means of enhancing performance in BCI
              training protocols and entertaining and challenging participants
              while training to use a BCI. By providing the BCI user with an
              entertaining environment, researchers hope to assist users in
              becoming more proficient at controlling a BCI system. BCIs have
              been used to enrich the experience of abled-bodied and physically
              impaired users in various computer applications, in particular,
              computer games. BCI games have been reviewed previously, yet a
              critical evaluation of ``gameplay'' within BCI games has not been
              undertaken. Gameplay is a key aspect of any computer game and
              encompasses the challenges presented to the player, the actions
              made available to the player by the game designer to overcome the
              challenges and the interaction mechanism in the game. Here, the
              appropriateness of game genres (a category of games characterized
              by a particular set of gameplay challenges) and the associated
              gameplay challenges for different BCI paradigms is evaluated. The
              gameplay mechanics employed across a range of BCI games are
              reviewed and evaluated in terms of the BCI control strategy's
              suitability, considering the genre and gameplay mechanics
              employed. A number of recommendations for the field relating to
              genre-specific BCI-games development and assessing user
              performance are also provided for BCI game developers.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  5,
  number   =  2,
  pages    = "82--99",
  month    =  jun,
  year     =  2013,
  keywords = "brain-computer interfaces;computer games;BCI development;BCI
              system;BCI training protocols;brain computer interfaces;computer
              applications;computer games;game designer;gameplay;interaction
              mechanism;Computers;Control systems;Electric
              potential;Electroencephalography;Games;Training;Visualization;Brain--computer
              interfaces (BCIs);game design;gameplay;games;review;Games"
}

@INPROCEEDINGS{Chamberlain1990-ez,
  title     = "Analysis in {HUGIN} of Data Conflict",
  booktitle = "{UAI1990}",
  author    = "Chamberlain, Bo and Jensen, Finn Verner and Jensen, Frank and
               Nordahl, Torsten",
  abstract  = "After a brief introduction to causal probabilistic networks and
               the HUGIN approach, the problem of conflicting data is
               discussed. A measure of conflict is defined, and it is used in
               the medical diagnostic system MUNIN. Finally, it is discussed
               how to distinguish between conflicting data and a rare case.",
  month     =  mar,
  year      =  1990,
  keywords  = "Assurances"
}

@ARTICLE{Carrillo-arce2013-ja,
  title    = "Decentralized Multi-robot Cooperative Localization using
              Covariance Intersection",
  author   = "Carrillo-arce, Luis C and Nerurkar, Esha D",
  pages    = "1412--1417",
  year     =  2013,
  keywords = "Folder - Spring2016;Assurances"
}

@ARTICLE{Shi2014-br,
  title     = "An event-triggered approach to state estimation with multiple
               point- and set-valued measurements",
  author    = "Shi, Dawei and Chen, Tongwen and Shi, Ling",
  abstract  = "Abstract In this work, we consider state estimation based on the
               information from multiple sensors that provide their measurement
               updates according to separate event-triggering conditions. An
               optimal sensor fusion problem based on the hybrid measurement
               information (namely, point- and set-valued measurements) is
               formulated and explored. We show that under a commonly-accepted
               Gaussian assumption, the optimal estimator depends on the
               conditional mean and covariance of the measurement innovations,
               which applies to general event-triggering schemes. For the case
               that each channel of the sensors has its own event-triggering
               condition, closed-form representations are derived for the
               optimal estimate and the corresponding error covariance matrix,
               and it is proved that the exploration of the set-valued
               information provided by the event-triggering sets guarantees the
               improvement of estimation performance. The effectiveness of the
               proposed event-based estimator is demonstrated by extensive
               Monte Carlo simulation experiments for different categories of
               systems and comparative simulation with the classical Kalman
               filter.",
  journal   = "Automatica",
  publisher = "Elsevier Ltd",
  volume    =  50,
  number    =  6,
  pages     = "1641--1648",
  year      =  2014,
  keywords  = "Event-based estimation; Kalman filters; Sensor fusion; Wireless
               sensor networks;Folder - Spring2016;Assurances"
}

@BOOK{Stone2015-te,
  title     = "Information Theory: A Tutorial Introduction",
  author    = "Stone, J V",
  abstract  = "Originally developed by Claude Shannon in the 1940s, information
               theory laid the foundations for the digital revolution, and is
               now an essential tool in telecommunications, genetics,
               linguistics, brain sciences, and deep space communication. In
               this richly illustrated book, accessible examples are used to
               introduce information theory in terms of everyday games like `20
               questions' before more advanced topics are explored. Online
               MatLab and Python computer programs provide hands-on experience
               of information theory in action, and PowerPoint slides give
               support for teaching. Written in an informal style, with a
               comprehensive glossary and tutorial appendices, this text is an
               ideal primer for novices who wish to learn the essential
               principles and applications of information theory.",
  publisher = "Sebtel Press",
  pages     = "260",
  edition   =  1,
  year      =  2015,
  keywords  = "Information theory;Textbook;TextBooks;Assurances",
  language  = "en"
}

@BOOK{Cappe2005-uw,
  title    = "Inference in Hidden Markov Models",
  author   = "Capp{\'e}, Olivier and Moulines, Eric and Ryd{\'e}n, Tobias",
  abstract = "This book is a comprehensive treatment of inference for hidden
              Markov models, including both algorithms and statistical theory.
              Topics range from filtering and smoothing of the hidden Markov
              chain to parameter estimation, Bayesian methods and estimation of
              the number of states. In a unified way the book covers both
              models with finite state spaces and models with continuous state
              spaces (also called state-space models) requiring approximate
              simulation-based algorithms that are also described in detail.
              Many examples illustrate the algorithms and theory. This book
              builds on recent developments to present a self-contained view.",
  volume   =  48,
  pages    = "574--575",
  year     =  2005,
  keywords = "Hidden Markov models;Textbook;TextBooks;Assurances"
}

@ARTICLE{Ermachenko1975-lf,
  title    = "[Activation of the fermentative activity of yeasts by yeast cell
              fractions]",
  author   = "Ermachenko, V A and Braginskaia, F I and Krugliakova, K E",
  abstract = "H. Akaike, 'A new look at the statistical model identification',
              IEEE Transactions\textbackslashnon Automatic Control, Vol. 19,
              No. 6, pp. 716-723, 1974",
  journal  = "Izv. Akad. Nauk SSSR Biol.",
  volume   =  19,
  number   =  5,
  pages    = "769--771",
  month    =  sep,
  year     =  1975,
  keywords = "AIC;Information theory;Assurances",
  language = "ru"
}

@ARTICLE{Burnham2004-ae,
  title    = "Multimodel Inference: Understanding {AIC} and {BIC} in Model
              Selection",
  author   = "Burnham, K P",
  abstract = "AICc justification",
  journal  = "Sociol. Methods Res.",
  volume   =  33,
  number   =  2,
  pages    = "261--304",
  month    =  nov,
  year     =  2004,
  keywords = "aic; bic; model averaging; model selection; multimodel
              inference;AIC;Model selection;Assurances"
}

@ARTICLE{Habbema1976-xd,
  title    = "Models for diagnosis and detection of combinations of diseases",
  author   = "Habbema, Jdf",
  journal  = "Decision making and medical care : can information science help?
              : Proceedings of the IFIP Working Conference on Decision Making
              and Medical Care",
  pages    = "399--411",
  year     =  1976,
  keywords = "Supplemental Assurance;Quantify
              Uncertainty;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@BOOK{MacDonald1997-gz,
  title     = "Hidden Markov and Other Models for Discrete- valued Time Series",
  author    = "MacDonald, Iain L and Zucchini, Walter",
  abstract  = "Discrete-valued time series are common in practice, but methods
               for their analysis are not well-known. In recent years, methods
               have been developed which are specifically designed for the
               analysis of discrete-valued time series. Hidden Markov and Other
               Models for Discrete-Valued Time Series introduces a new,
               versatile, and computationally tractable class of models, the
               ``hidden Markov'' models. It presents a detailed account of
               these models, then applies them to data from a wide range of
               diverse subject areas, including medicine, climatology, and
               geophysics. This book will be invaluable to researchers and
               postgraduate and senior undergraduate students in statistics.
               Researchers and applied statisticians who analyze time series
               data in medicine, animal behavior, hydrology, and sociology will
               also find this information useful.",
  publisher = "CRC Press",
  month     =  jan,
  year      =  1997,
  keywords  = "HMM;Assurances;Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@ARTICLE{Leroux1992-di,
  title     = "Consistent Estimation of a Mixing Distribution",
  author    = "Leroux, Brian G",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  20,
  number    =  3,
  pages     = "1350--1360",
  year      =  1992,
  keywords  = "Mixture distribution; maximum likelihood; maximum penalized
               likelihood; model selection;Assurances"
}

@ARTICLE{Leroux1992-du,
  title    = "Maximum-penalized-likelihood estimation for independent and
              Markov-dependent mixture models",
  author   = "Leroux, B G and Puterman, M L",
  abstract = "This paper concerns the use and implementation of
              maximum-penalized-likelihood procedures for choosing the number
              of mixing components and estimating the parameters in independent
              and Markov-dependent mixture models. Computation of the estimates
              is achieved via algorithms for the automatic generation of
              starting values for the EM algorithm. Computation of the
              information matrix is also discussed. Poisson mixture models are
              applied to a sequence of counts of movements by a fetal lamb in
              utero obtained by ultrasound. The resulting estimates are seen to
              provide plausible mechanisms for the physiological process.",
  journal  = "Biometrics",
  volume   =  48,
  number   =  2,
  pages    = "545--558",
  month    =  jun,
  year     =  1992,
  keywords = "Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language = "en"
}

@ARTICLE{Hughes1994-cn,
  title    = "A class of stochastic models for relating synoptic atmospheric
              patterns to regional hydrologic phenomena",
  author   = "Hughes, James P and Guttorp, Peter",
  abstract = "A model for multistation precipitation, conditional on synoptic
              atmospheric patterns, is presented. The model, which we call the
              nonhomogeneous hidden Markov model (NHMM), postulates the
              existence of an unobserved weather state, which serves as a link
              between the large-scale atmospheric measures and the small-scale
              spatially discontinuous precipitation field. The weather state
              effectively acts as an automatic classifier of atmospheric
              patterns. The weather state process is assumed to be
              conditionally Markov, given the atmospheric data. The rainfall
              process is then assumed to be conditionally independent given the
              weather state. Various parameterizations for the weather state
              process and the rainfall process are discussed, and a
              likelihood-based estimation procedure is described. Model-based
              estimates of the storm duration distribution and first and second
              moments of the rainfall process are derived. As an example the
              model is fit to a four-station network of rain gauge stations in
              Washington state. The observed first and second moments are
              reproduced very closely. The fitted duration distributions are
              somewhat lighter tailed than the observed distribution at two of
              the four stations but provide a good fit at the other two. We
              conclude that the NHMM has promise as a method of relating
              synoptic atmospheric data to rainfall and other regional or local
              hydrologic processes.",
  journal  = "Water Resour. Res.",
  volume   =  30,
  number   =  5,
  pages    = "1535--1546",
  month    =  may,
  year     =  1994,
  keywords = "1854 Precipitation; 1869 Stochastic hydrology; 3319 General
              circulation; 3364 Synoptic-scale
              meteorology;HMM;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Satten1996-kt,
  title     = "Markov Chains With Measurement Error: Estimating the `True'
               Course of a Marker of the Progression of Human Immunodeficiency
               Virus Disease",
  author    = "Satten, Glen A and Longini, Ira M",
  abstract  = "A Markov chain is a useful way of describing cohort data.
               Longitudinal observations of a marker of the progression of the
               human immunodeficiency virus (HIV), such as CD4 cell count,
               measured on members of a cohort study, can be analysed as a
               continuous time Markov chain by categorizing the CD4 cell counts
               into stages. Unfortunately, CD4 cell counts are subject to
               substantial measurement error and short timescale variability.
               Thus, fitting a Markov chain to raw CD4 cell count measurements
               does not determine the transition probabilities for the true or
               underlying CD4 cell counts; the measurements error results in a
               process that is too rough. Assuming independent measurement
               errors, we propose a likelihood-based method for estimating the
               'true' or underlying transition probabilities. The Markov
               structure allows efficient calculation of the likelihood by
               using hidden Markov model methodology. As example, we consider
               CD4 cell count data from 430 HIV-infected participants in the
               San Francisco Men's Health Study by categorizing the marker data
               into seven stages; up to 17 observations are available for each
               individual. We find that including measurement error both
               produces a significantly better fit and provides a model for CD4
               progression that is more biologically reasonable.",
  journal   = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  publisher = "[Wiley, Royal Statistical Society]",
  volume    =  45,
  number    =  3,
  pages     = "275--309",
  year      =  1996,
  keywords  = "HMM;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Lee2004-pv,
  title    = "Trust in Automation: Designing for Appropriate Reliance",
  author   = "Lee, J D and See, K A",
  abstract = "Automation is often problematic because people fail to rely upon
              it appropriately. Because people respond to technology socially,
              trust influences reliance on automation. In particular, trust
              guides reliance when complexity and unanticipated situations make
              a complete understanding of the automation impractical. This
              review considers trust from the organizational, sociological,
              interpersonal, psychological, and neurological perspectives. It
              considers how the context, automation characteristics, and
              cognitive processes affect the appropriateness of trust. The
              context in which the automation is used influences automation
              performance and provides a goal-oriented perspective to assess
              automation characteristics along a dimension of attributional
              abstraction. These characteristics can influence trust through
              analytic, analogical, and affective processes. The challenges of
              extrapolating the concept of trust in people to trust in
              automation are discussed. A conceptual model integrates research
              regarding trust in automation and describes the dynamics of
              trust, the role of context, and the influence of display
              characteristics. Actual or potential applications of this
              research include improved designs of systems that require people
              to manage imperfect automation.",
  journal  = "Human Factors: The Journal of the Human Factors and Ergonomics
              Society",
  volume   =  46,
  number   =  1,
  pages    = "50--80",
  year     =  2004,
  keywords = "
              trust\_definition;assurances;very\_similar\_to\_mine;automation;Assurances/Trust
              Background;Assurances"
}

@PHDTHESIS{Lystig2001-sz,
  title    = "Evaluation of Hidden Markov Models",
  author   = "Lystig, Theodore Christian",
  abstract = "Hidden Markov models are a very rich class of models that have
              been used on problems as diverse as speech recognition, sodium
              ion channels, infectious disease processes, and rainfall
              occurrence. Since the late 1960's, however, it has been
              appreciated that even the seemingly mundane task of calculating
              the log-likelihood of hidden Markov models is not a trivial
              matter. This task, known as the evaluation problem, was addressed
              through the development of an early example of the
              Expectation-Maximization algorithm. In this thesis, the problem
              of evaluation is addressed along both quantitative and
              qualitative lines. Quantitatively, and efficient algorithm is
              developed for fast computation of the log-likelihood, the score,
              and the observed information matrix from a single pass through
              the data. This enables one to readily obtain standard errors of
              parameter estimates, something that is rarely achieved in most
              typical EM stettings. It also permits alternatie maximization
              techniques to the EM algorithm. Qualitatively, sound goodnes sof
              fit technniques based on an expansion of the score are developed
              that are consistent against a wide variety of model
              mis-specifications. These techniques are shown to have good power
              in a range of situations, and may be performed with relatively
              little ocmputational effort. A complementary goodness of fit
              method based on conditional residuals is also developed that
              enables one to effectively screen a wide variety of candidate
              predictor variables without requiring additional refitting of the
              model. The methods are evaluated through both simulations and
              real datasets.",
  year     =  2001,
  school   = "University of Washington",
  keywords = "HMM;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@INPROCEEDINGS{Udell2014-uc,
  title     = "Convex Optimization in Julia",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Udell, M and Mohan, K and Zeng, D and Hong, J and Diamond, S and
               Boyd, S",
  abstract  = "This paper describes Convex1, a convex optimization modeling
               framework in Julia. Convex translates problems from a
               user-friendly functional language into an abstract syntax tree
               describing the problem. This concise representation of the
               global structure of the problem allows Convex to infer whether
               the problem complies with the rules of disciplined convex
               programming (DCP), and to pass the problem to a suitable solver.
               These operations are carried out in Julia using multiple
               dispatch, which dramatically reduces the time required to verify
               DCP compliance and to parse a problem into conic form. Convex
               then automatically chooses an appropriate backend solver to
               solve the conic form problem.",
  publisher = "ACM",
  pages     = "18--28",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "convex programming;functional languages;mathematics
               computing;DCP compliance;Julia;abstract syntax tree;conic form
               problem;convex optimization modeling framework;disciplined
               convex programming;user-friendly functional
               language;Abstracts;Convex functions;Frequency modulation;Object
               oriented modeling;Optimization;Programming;Symmetric
               matrices;Convex programming; automatic verification; symbolic
               computation; multiple dispatch;julia;Julia"
}

@INPROCEEDINGS{Olver2014-qe,
  title     = "A Practical Framework for {Infinite-Dimensional} Linear Algebra",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Olver, S and Townsend, A",
  abstract  = "We describe a framework for solving a broad class of
               infinite-dimensional linear equations, consisting of almost
               banded operators, which can be used to representing linear
               ordinary differential equations with general boundary
               conditions. The framework contains a data structure for on which
               row operations can be performed, allowing for the solution of
               infinite-dimensional linear equations by the adaptive QR
               approach. The algorithm achieves O(nopt) complexity, where nopt
               is the number of degrees of freedom required to achieve a
               desired accuracy, which is determined adaptively. In addition,
               special tensor product equations, such as partial differential
               equations on rectangles, can be solved by truncating the
               operator in the y-direction with ny degrees of freedom and using
               a generalized Schur decomposition to upper triangularize, before
               applying the adaptive QR approach to the x-direction, requiring
               O(n3y + n2ynoptx) operations. The framework is implemented in
               the ApproxFun package written in the Julia programming language,
               which achieves highly competitive computational costs by
               exploiting unique features of Julia. Using this framework,
               partial differential equations that require as many as 2.5
               million unknowns can be solved in less than 4 seconds.",
  publisher = "ACM",
  pages     = "57--62",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "high level languages;linear algebra;linear differential
               equations;mathematics computing;ApproxFun package;Julia
               programming language;adaptive QR approach;general boundary
               conditions;infinite-dimensional linear
               algebra;infinite-dimensional linear equations;linear ordinary
               differential equations;special tensor product
               equations;Arrays;Complexity theory;Equations;Mathematical
               model;Vectors;julia;Julia"
}

@ARTICLE{Lubin2015-dh,
  title    = "Computing in Operations Research Using Julia",
  author   = "Lubin, Miles and Dunning, Iain",
  abstract = "The state of numerical computing is currently characterized by a
              divide between highly efficient yet typically cumbersome
              low-level languages such as C, C++, and Fortran and highly
              expressive yet typically slow high-level languages such as Python
              and MATLAB. This paper explores how Julia, a modern programming
              language for numerical computing which claims to bridge this
              divide by incorporating recent advances in language and compiler
              design (such as just-in-time compilation), can be used for
              implementing software and algorithms fundamental to the field of
              operations research, with a focus on mathematical optimization.
              In particular, we demonstrate algebraic modeling for linear and
              nonlinear optimization and a partial implementation of a
              practical simplex code. Extensive cross-language benchmarks
              suggest that Julia is capable of obtaining state-of-the-art
              performance.",
  journal  = "INFORMS J. Comput.",
  volume   =  27,
  number   =  2,
  pages    = "238--248",
  month    =  apr,
  year     =  2015,
  keywords = "julia;Julia"
}

@INPROCEEDINGS{Bezanson2014-na,
  title     = "Array Operators Using Multiple Dispatch: A design methodology
               for array implementations in dynamic languages",
  booktitle = "Proceedings of {ACM} {SIGPLAN} International Workshop on
               Libraries, Languages, and Compilers for Array Programming",
  author    = "Bezanson, Jeff and Chen, Jiahao and Karpinski, Stefan and Shah,
               Viral and Edelman, Alan",
  abstract  = "Arrays are such a rich and fundamental data type that they tend
               to be built into a language, either in the compiler or in a
               large low-level library. Defining this functionality at the user
               level instead provides greater flexibility for application
               domains not envisioned by the language designer. Only a few
               languages, such as C++ and Haskell, provide the necessary power
               to define n-dimensional arrays, but these systems rely on
               compile-time abstraction, sacrificing some flexibility. In
               contrast, dynamic languages make it straightforward for the user
               to define any behavior they might want, but at the possible
               expense of performance. As part of the Julia language project,
               we have developed an approach that yields a novel trade-off
               between flexibility and compile-time analysis. The core
               abstraction we use is multiple dispatch. We have come to believe
               that while multiple dispatch has not been especially popular in
               most kinds of programming, technical computing is its killer
               application. By expressing key functions such as array indexing
               using multi-method signatures, a surprising range of behaviors
               can be obtained, in a way that is both relatively easy to write
               and amenable to compiler analysis. The compact factoring of
               concerns provided by these methods makes it easier for
               user-defined types to behave consistently with types in the
               standard library.",
  publisher = "ACM",
  pages     = "56",
  month     =  jun,
  year      =  2014,
  address   = "New York, \{NY\}, \{USA\}",
  keywords  = "Julia; array indexing; dynamic dispatch; multiple dispatch;
               static analysis; type inference;julia;Julia"
}

@ARTICLE{Stor2015-kf,
  title    = "Forward stable computation of roots of real polynomials with only
              real distinct roots",
  author   = "Stor, N Jakovcevic and Slapnicar, I",
  abstract = "As showed in (Fiedler, 1990), any polynomial can be expressed as
              a characteristic polynomial of a complex symmetric arrowhead
              matrix. This expression is not unique. If the polynomial is real
              with only real distinct roots, the matrix can be chosen real. By
              using accurate forward stable algorithm for computing eigenvalues
              of real symmetric arrowhead matrices from (Jakovcevic Stor,
              Slapnicar, Barlow, 2015), we derive a forward stable algorithm
              for computation of roots of such polynomials in O(n2) operations.
              The algorithm computes each root to almost full accuracy. In some
              cases, the algorithm invokes extended precision routines, but
              only in the non-iterative part. Our examples include numerically
              difficult problems, like the well-known Wilkinson's polynomials.
              Our algorithm compares favourably to other method for polynomial
              root-finding, like MPSolve or Newton's method.",
  pages    = "1--15",
  year     =  2015,
  keywords = "julia;Julia"
}

@ARTICLE{Udell2014-bk,
  title    = "Generalized Low Rank Models",
  author   = "Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd,
              Stephen",
  abstract = "Principal components analysis (PCA) is a well-known technique for
              approximating a data set represented by a matrix by a low rank
              matrix. Here, we extend the idea of PCA to handle arbitrary data
              sets consisting of numerical, Boolean, categorical, ordinal, and
              other data types. This framework encompasses many well known
              techniques in data analysis, such as nonnegative matrix
              factorization, matrix completion, sparse and robust PCA, k-means,
              k-SVD, and maximum margin matrix factorization. The method
              handles heterogeneous data sets, and leads to coherent schemes
              for compressing, denoising, and imputing missing entries across
              all data types simultaneously. It also admits a number of
              interesting interpretations of the low rank factors, which allow
              clustering of examples or of features. We propose several
              parallel algorithms for fitting generalized low rank models, and
              describe implementations and numerical results.",
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Olver2014-az,
  title    = "Sampling unitary invariant ensembles",
  author   = "Olver, Sheehan and Nadakuditi, Raj Rao and Trogdon, Thomas",
  abstract = "We develop an algorithm for sampling from the unitary invariant
              random matrix ensembles. The algorithm is based on the
              representation of their eigenvalues as a determinantal point
              process whose kernel is given in terms of orthogonal polynomials.
              Using this algorithm, statistics beyond those known through
              analysis are calculable through Monte Carlo simulation.
              Unexpected phenomena are observed in the simulations.",
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Townsend2014-vg,
  title    = "Fast computation of \{Gauss\} quadrature nodes and weights on the
              whole real line",
  author   = "Townsend, Alex and Trogdon, Thomas and Olver, Sheehan",
  abstract = "A fast and accurate algorithm for the computation of
              Gauss-Hermite and generalized Gauss-Hermite quadrature nodes and
              weights is presented. The algorithm is based on Newton's method
              with carefully selected initial guesses for the nodes and a fast
              evaluation scheme for the associated orthogonal polynomial. In
              the Gauss-Hermite case the initial guesses and evaluation scheme
              rely on explicit asymptotic formulas. For generalized
              Gauss-Hermite, the initial guesses are furnished by sampling a
              certain equilibrium measure and the associated polynomial
              evaluated via a Riemann-Hilbert reformulation. In both cases the
              n-point quadrature rule is computed in O(n) operations to an
              accuracy that is close to machine precision. For sufficiently
              large n, some of the quadrature weights have a value less than
              the smallest positive normalized floating-point number in double
              precision and we exploit this fact to achieve a complexity as low
              as O(sqrt(n))).",
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Jakovcevic_Stor2015-zz,
  title    = "Forward stable eigenvalue decomposition of rank-one modifications
              of diagonal matrices",
  author   = "Jakov{\v c}evi{\'c} Stor, N and Slapni{\v c}ar, I and Barlow, J L",
  abstract = "We present a new algorithm for solving an eigenvalue problem for
              a real symmetric matrix which is a rank-one modification of a
              diagonal matrix. The algorithm computes each eigenvalue and all
              components of the corresponding eigenvector with high relative
              accuracy in O(n) operations. The algorithm is based on a
              shift-and-invert approach. Only a single element of the inverse
              of the shifted matrix eventually needs to be computed with double
              the working precision. Each eigenvalue and the corresponding
              eigenvector can be computed separately, which makes the algorithm
              adaptable for parallel computing. Our results extend to the
              complex Hermitian case. The algorithm is similar to the algorithm
              for solving the eigenvalue problem for real symmetric arrowhead
              matrices from N. Jakov{\v c}evi{\'c} Stor et al. (2015) [16].",
  journal  = "Linear Algebra Appl.",
  volume   =  487,
  pages    = "301--315",
  month    =  dec,
  year     =  2015,
  keywords = "julia;Julia"
}

@ARTICLE{Townsend2014-gt,
  title    = "The automatic solution of partial differential equations using a
              global spectral method",
  author   = "Townsend, Alex and Olver, Sheehan",
  abstract = "A spectral method for solving linear partial differential
              equations (PDEs) with variable coefficients and general boundary
              conditions defined on rectangular domains is described, based on
              separable representations of partial differential operators and
              the one-dimensional ultraspherical spectral method. If a partial
              differential operator is of splitting rank 2, such as the
              operator associated with Poisson or Helmholtz, the corresponding
              PDE is solved via a generalized Sylvester matrix equation, and a
              bivariate polynomial approximation of the solution of degree
              (n\_x,n\_y) is computed in O(n\_x n\_y)^\{3/2\} operations.
              Partial differential operators of splitting rank >=3 are solved
              via a linear system involving a block-banded matrix in
              O(min(n\_x^\{3\} n\_y,n\_x n\_y^\{3\})) operations. Numerical
              examples demonstrate the applicability of our 2D spectral method
              to a broad class of PDEs, which includes elliptic and dispersive
              time-evolution equations. The resulting PDE solver is written in
              \{\textbackslashsc Matlab\} and is publicly available as part of
              \{\textbackslashsc Chebfun\}. It can resolve solutions requiring
              over a million degrees of freedom in under 60 seconds. An
              experimental implementation in the Julia language can currently
              perform the same solve in 10 seconds.",
  year     =  2014,
  keywords = "julia;Julia"
}

@TECHREPORT{Zhang2015-cl,
  title       = "Matrix Depot: An Extensible Test Matrix Collection for Julia",
  author      = "Zhang, Weijian and Higham, Nicholas J",
  abstract    = "Matrix Depot is a Julia software package that provides easy
                 access to a large and diverse collection of test matrices. Its
                 novelty is threefold. First, it is extensible by the user, and
                 so can be adapted to include the user's own test problems. In
                 doing so it facilitates experimentation and makes it easier to
                 carry out reproducible research. Second, it amalgamates in a
                 single framework three different types of matrix collections,
                 comprising parametrized test matrices, regularization test
                 problems, and real-life sparse matrix data. Third, it fully
                 exploits the Julia language. It uses multiple dispatch to help
                 provide a simple interface and, in particular, to allow
                 matrices to be generated in any of the numeric data types
                 supported by the language.",
  pages       = "25",
  institution = "Manchester Institute for Mathematical Sciences, The University
                 of Manchester",
  month       =  dec,
  year        =  2015,
  keywords    = "julia;Julia"
}

@ARTICLE{Bezanson2012-tj,
  title    = "\{J\}ulia: A Fast Dynamic Language for Technical Computing",
  author   = "Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B and
              Edelman, Alan",
  abstract = "Dynamic languages have become popular for scientific computing.
              They are generally considered highly productive, but lacking in
              performance. This paper presents Julia, a new dynamic language
              for technical computing, designed for performance from the
              beginning by adapting and extending modern programming language
              techniques. A design based on generic functions and a rich type
              system simultaneously enables an expressive programming model and
              successful type inference, leading to good performance for a wide
              range of programs. This makes it possible for much of the Julia
              library to be written in Julia itself, while also incorporating
              best-of-breed C and Fortran libraries.",
  month    =  sep,
  year     =  2012,
  keywords = "julia;Julia"
}

@INPROCEEDINGS{Knopp2014-vp,
  title     = "Experimental Multi-threading Support for the Julia Programming
               Language",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Knopp, T",
  abstract  = "Julia is a young programming language that is designed for
               technical computing. Although Julia is dynamically typed it is
               very fast and usually yields C speed by utilizing a just-in-time
               compiler. Still, Julia has a simple syntax that is similar to
               Matlab, which is widely known as an easy-to-use programming
               environment. While Julia is very versatile and provides
               asynchronous programming facilities in the form of tasks
               (coroutines) as well as distributed multi-process parallelism,
               one missing feature is shared memory multi-threading. In this
               paper we present our experiment on introducing multi-threading
               support in the Julia programming environment. While our
               implementation has some restrictions that have to be taken into
               account when using threads, the results are promising yielding
               almost full speedup for perfectly parallelizable tasks.",
  publisher = "ACM",
  pages     = "1--5",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "computational linguistics;distributed shared memory
               systems;multi-threading;parallel languages;program
               compilers;Julia programming environment;Julia programming
               language;asynchronous programming facilities;distributed
               multiprocess parallelism;just-in-time compiler;shared memory
               multithreading;syntax;technical computing;Arrays;Computer
               languages;Dynamic programming;Instruction
               sets;Libraries;MATLAB;Resource management;julia;Julia"
}

@TECHREPORT{Cyrus_Maher2015-hd,
  title       = "{CauseMap}: Fast inference of causality from complex time
                 series",
  author      = "Cyrus Maher, M and Hernandez, Ryan D",
  abstract    = "Background: Establishing health-related causal relationships
                 is a central pursuit in biomedical research. Yet, the
                 interdependent non-linearity of biological systems renders
                 causal dynamics laborious and at times impractical to
                 disentangle. This pursuit is further impeded by the dearth of
                 time series that are sufficiently long to observe and
                 understand recurrent patterns of flux. However, as data
                 generation costs plummet and technologies like wearable
                 devices democratize data collection, we anticipate a coming
                 surge in the availability of biomedically-relevant time series
                 data. Given the life-saving potential of these burgeoning
                 resources, it is critical to invest in the development of open
                 source software tools that are capable of drawing meaningful
                 insight from vast amounts of time series data.Results: Here we
                 present CauseMap, the first open source implementation of
                 convergent cross mapping (CCM), a method for establishing
                 causality from long time series data (> ~25 observations).
                 Compared to existing time series methods, CCM has the
                 advantage of being model-free and robust to unmeasured
                 confounding that could otherwise induce spurious associations.
                 CCM builds on Takens' Theorem, a well-established result from
                 dynamical systems theory that requires only mild assumptions.
                 This theorem allows us to reconstruct high dimensional system
                 dynamics using a time series of only a single variable. These
                 reconstructions can be thought of as shadows of the true
                 causal system. If the reconstructed shadows can predict points
                 from the opposing time series, we can infer that the
                 corresponding variables are providing views of the same causal
                 system, and so are causally related. Unlike traditional
                 metrics, this test can establish the directionality of
                 causation, even in the presence of feedback loops.
                 Furthermore, since CCM can extract causal relationships from
                 times series of, e.g. a single individual, it may be a
                 valuable tool to personalized medicine. We implement CCM in
                 Julia, a high-performance programming language designed for
                 facile technical computing. Our software package, CauseMap, is
                 platform-independent and freely available as an official Julia
                 package.Conclusions: CauseMap is an efficient implementation
                 of a state-of-the-art algorithm for detecting causality from
                 time series data. We believe this tool will be a valuable
                 resource for biomedical research and personalized medicine.",
  publisher   = "PeerJ Inc.",
  number      = "e1053",
  institution = "PeerJ PrePrints",
  month       =  feb,
  year        =  2015,
  keywords    = "Causality; Open source software; Time series methods;
                 Dynamical systems; Personalized medicine;julia;Julia",
  language    = "en"
}

@INPROCEEDINGS{Chen2014-rc,
  title           = "Parallel Prefix Polymorphism Permits Parallelization,
                     Presentation \& Proof",
  booktitle       = "2014 First Workshop for High Performance Technical
                     Computing in Dynamic Languages",
  author          = "Chen, Jiahao and Edelman, Alan",
  abstract        = "Polymorphism in programming languages enables code reuse.
                     Here, we show that polymorphism has broad applicability
                     far beyond computations for technical computing:
                     parallelism in distributed computing, presentation of
                     visualizations of runtime data flow, and proofs for formal
                     verification of correctness. The ability to reuse a single
                     codebase for all these purposes provides new ways to
                     understand and verify parallel programs.",
  publisher       = "IEEE",
  pages           = "47--56",
  year            =  2014,
  address         = "New York",
  keywords        = "julia;Julia",
  conference      = "2014 First Workshop for High Performance Technical
                     Computing in Dynamic Languages (HPTCDL)"
}

@INPROCEEDINGS{Lin2013-ka,
  title     = "Online Learning of Nonparametric Mixture Models via Sequential
               Variational Approximation",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Lin, Dahua",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  abstract  = "Reliance on computationally expensive algorithms for inference
               has been limiting the use of Bayesian nonparametric models in
               large scale applications. To tackle this problem, we propose a
               Bayesian learning algorithm for DP mixture models. Instead of
               following the conventional paradigm -- random initialization
               plus iterative update, we take an progressive approach. Starting
               with a given prior, our method recursively transforms it into an
               approximate posterior through sequential variational
               approximation. In this process, new components will be
               incorporated on the fly when needed. The algorithm can reliably
               estimate a DP mixture model in one pass, making it particularly
               suited for applications with massive data. Experiments on both
               synthetic data and real datasets demonstrate remarkable
               improvement on efficiency -- orders of magnitude speed-up
               compared to the state-of-the-art.",
  publisher = "Curran Associates, Inc.",
  pages     = "395--403",
  year      =  2013,
  keywords  = "julia;Julia"
}

@ARTICLE{Van_der_Meulen2014-th,
  title    = "Bayesian estimation of discretely observed multi-dimensional
              diffusion processes using guided proposals",
  author   = "van der Meulen, Frank and Schauer, Moritz",
  abstract = "Bayesian estimation of parameters of a diffusion based on
              discrete time observations poses a difficult problem due to the
              lack of a closed form expression for the likelihood.
              Data-augmentation has been proposed for obtaining draws from the
              posterior distribution of the parameters. Within this approach,
              the discrete time observations are augmented with diffusion
              bridges connecting these observations. This poses two challenges:
              (i) efficiently generating diffusion bridges; (ii) if unknown
              parameters appear in the diffusion coefficient, then direct
              implementation of data-augmentation results in an induced Markov
              chain which is reducible. In this paper we show how both
              challenges can be addressed in continuous time (before
              discretisation) by using guided proposals. These are Markov
              processes with dynamics described by the stochastic differential
              equation of the diffusion process with an additional term added
              to the drift coefficient to guide the process to hit the right
              end point of the bridge. The form of these proposals naturally
              provides a mapping that decouples the dependence between the
              diffusion coefficient and diffusion bridge using the driving
              Brownian motion of the proposals. As the guiding term has a
              singularity at the right end point, care is needed when
              discretisation is applied for implementation purposes. We show
              that this problem can be dealt with by appropriately time
              changing and scaling of the guided proposal process. In two
              examples we illustrate the performance of the algorithms we
              propose. The second of these concerns a diffusion approximation
              of a chemical reaction network with a four-dimensional diffusion
              driven by an eight-dimensional Brownian motion.",
  year     =  2014,
  keywords = "julia;Julia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Hisamoto_Sorami2014-ff,
  title     = "技術計算のための新言語Julia (Julia: a new language for technical computing)",
  booktitle = "データサイエンティスト養成読本 R活用編 (Data scientist training reader: practical
               \{R\} edition)",
  author    = "{久本 空海 (Hisamoto, Sorami)} and {西薗 良太 (Nishizono,
               Ry\textbackslash=ota)}",
  publisher = "技術評論社 (Gijutsu-Hyohron)",
  chapter   =  7,
  series    = "Software Design plus",
  month     =  dec,
  year      =  2014,
  address   = "Tokyo",
  keywords  = "julia;Julia"
}

@INPROCEEDINGS{Shah2013-en,
  title     = "Novel algebras for advanced analytics in Julia",
  booktitle = "2013 {IEEE} High Performance Extreme Computing Conference
               ({HPEC})",
  author    = "Shah, V B and Edelman, A and Karpinski, S and Bezanson, J and
               Kepner, J",
  abstract  = "A linear algebraic approach to graph algorithms that exploits
               the sparse adjacency matrix representation of graphs can provide
               a variety of benefits. These benefits include syntactic
               simplicity, easier implementation, and higher performance. One
               way to employ linear algebra techniques for graph algorithms is
               to use a broader definition of matrix and vector multiplication.
               We demonstrate through the use of the Julia language system how
               easy it is to explore semirings using linear algebraic
               methodologies.",
  pages     = "1--4",
  year      =  2013,
  keywords  = "high level languages;linear algebra;mathematics co; mathematics
               computing; Julia; Julia language system; advanced analytics;
               graph algorithms; linear algebra techniques; linear algebraic
               approach; linear algebraic methodologies; matrix multiplication;
               novel algebras; sparse adjacency matrix representation; vector
               multiplication; Electronic mail; Matrices; Sparse matrices;
               Standards; Syntactics;julia;Julia"
}

@ARTICLE{Slevinsky2014-pg,
  title    = "On the use of conformal maps for the acceleration of convergence
              of the trapezoidal rule and Sinc numerical methods",
  author   = "Slevinsky, Richard Mika{\"e}l and Olver, Sheehan",
  abstract = "We investigate the use of conformal maps for the acceleration of
              convergence of the trapezoidal rule and Sinc numerical methods.
              The conformal map is a polynomial adjustment to the sinh map, and
              allows the treatment of a finite number of singularities in the
              complex plane. In the case where locations are unknown, the
              so-called Sinc-Pad{\'e} approximants are used to provide
              approximate results. This adaptive method is shown to have almost
              the same convergence properties. We use the conformal maps to
              generate high accuracy solutions to several challenging
              integrals, nonlinear waves, and multidimensional integrals.",
  year     =  2014,
  keywords = "julia;Julia"
}

@INBOOK{Tate2014-tl,
  title     = "Seven More Languages in Seven Weeks: Languages That Are Shaping
               the Future",
  author    = "Tate, Bruce A and Dees, Ian and Daoud, Frederic and Moffitt,
               Jack",
  publisher = "Pragmatic Bookshelf",
  chapter   =  6,
  edition   =  1,
  year      =  2014,
  keywords  = "julia;Julia"
}

@ARTICLE{Tran2014-ne,
  title    = "Annealed Important Sampling for Models with Latent Variables",
  author   = "Tran, M-N and Strickland, C and Pitt, M K and Kohn, R",
  abstract = "This paper is concerned with Bayesian inference when the
              likelihood is analytically intractable but can be unbiasedly
              estimated. We propose an annealed importance sampling procedure
              for estimating expectations with respect to the posterior. The
              proposed algorithm is useful in cases where finding a good
              proposal density is challenging, and when estimates of the
              marginal likelihood are required. The effect of likelihood
              estimation is investigated, and the results provide guidelines on
              how to set up the precision of the likelihood estimation in order
              to optimally implement the procedure. The methodological results
              are empirically demonstrated in several simulated and real data
              examples.",
  year     =  2014,
  keywords = "julia;Julia"
}

@INCOLLECTION{Caulfield2014-te,
  title     = "Compositional Security Modelling",
  booktitle = "Human Aspects of Information Security, Privacy, and Trust",
  author    = "Caulfield, Tristan and Pym, David and Williams, Julian",
  editor    = "Tryfonas, Theo and Askoxylakis, Ioannis",
  abstract  = "Security managers face the challenge of formulating and
               implementing policies that deliver their desired system security
               postures --- for example, their preferred balance of
               confidentiality, integrity, and availability --- within budget
               (monetary and otherwise). In this paper, we describe a security
               modelling methodology, grounded in rigorous mathematical systems
               modelling and economics, that captures the managers' policies
               and the behavioural choices of agents operating within the
               system. Models are executable, so allowing systematic
               experimental exploration of the system-policy co-design space,
               and compositional, so managing the complexity of large-scale
               systems.",
  publisher = "Springer International Publishing",
  volume    =  8533,
  pages     = "233--245",
  series    = "Lecture Notes in Computer Science",
  year      =  2014,
  address   = "Cham",
  keywords  = "julia;Julia"
}

@ARTICLE{Mariana_M_Odashima_Beatriz_G_Prado2016-im,
  title    = "Introduction to the equilibrium Green's functions: condensed
              matter examples with numerical implementations",
  author   = "{Mariana M. Odashima Beatriz G. Prado} and Vernek, E",
  abstract = "The Green's function method has applications in several fields in
              Physics, from classical differential equations to quantum
              many-body problems. In the quantum context, Green's functions are
              correlation functions, from which it is possible to extract
              information from the system under study, such as the density of
              states, relaxation times and response functions. Despite its
              power and versatility, it is known as a laborious and sometimes
              cumbersome method. Here we introduce the equilibrium Green's
              functions and the equation-of-motion technique, exemplifying the
              method in discrete lattices of non-interacting electrons. We
              start with simple models, such as the two-site molecule, the
              infinite and semi-infinite one-dimensional chains, and the
              two-dimensional ladder. Numerical implementations are developed
              via the recursive Green's function, implemented in Julia, an
              open-source, efficient and easy-to-learn scientific language. We
              also present a new variation of the surface recursive Green's
              function method, which can be of interest when simulating
              simultaneously the properties of surface and bulk.",
  pages    = "1--20",
  year     =  2016,
  keywords = "julia;Julia"
}

@ARTICLE{Jakovcevic_Stor2015-oy,
  title    = "Accurate eigenvalue decomposition of real symmetric arrowhead
              matrices and applications",
  author   = "Jakov{\v c}evi{\'c} Stor, Nevena and Slapni{\v c}ar, Ivan and
              Barlow, Jesse L",
  abstract = "We present a new algorithm for solving the eigenvalue problem for
              an n $\times$ n real symmetric arrowhead matrix. The algorithm
              computes all eigenvalues and all components of the corresponding
              eigenvectors with high relative accuracy in O(n^2) operations
              under certain circumstances. The algorithm is based on a
              shift-and-invert approach. Only a single element of the inverse
              of the shifted matrix eventually needs to be computed with double
              the working precision. Each eigenvalue and the corresponding
              eigenvector can be computed separately, which makes the algorithm
              adaptable for parallel computing. Our results extend to Hermitian
              arrowhead matrices, real symmetric diagonal-plus-rank-one
              matrices and singular value decomposition of real triangular
              arrowhead matrices.",
  journal  = "Linear Algebra Appl.",
  volume   =  464,
  pages    = "62--89",
  month    =  jan,
  year     =  2015,
  keywords = "julia;Julia"
}

@ARTICLE{Baldassi2014-ae,
  title    = "Fast and accurate multivariate Gaussian modeling of protein
              families: predicting residue contacts and protein-interaction
              partners",
  author   = "Baldassi, Carlo and Zamparo, Marco and Feinauer, Christoph and
              Procaccini, Andrea and Zecchina, Riccardo and Weigt, Martin and
              Pagnani, Andrea",
  abstract = "In the course of evolution, proteins show a remarkable
              conservation of their three-dimensional structure and their
              biological function, leading to strong evolutionary constraints
              on the sequence variability between homologous proteins. Our
              method aims at extracting such constraints from rapidly
              accumulating sequence data, and thereby at inferring protein
              structure and function from sequence information alone. Recently,
              global statistical inference methods (e.g. direct-coupling
              analysis, sparse inverse covariance estimation) have achieved a
              breakthrough towards this aim, and their predictions have been
              successfully implemented into tertiary and quaternary protein
              structure prediction methods. However, due to the discrete nature
              of the underlying variable (amino-acids), exact inference
              requires exponential time in the protein length, and efficient
              approximations are needed for practical applicability. Here we
              propose a very efficient multivariate Gaussian modeling approach
              as a variant of direct-coupling analysis: the discrete amino-acid
              variables are replaced by continuous Gaussian random variables.
              The resulting statistical inference problem is efficiently and
              exactly solvable. We show that the quality of inference is
              comparable or superior to the one achieved by mean-field
              approximations to inference with discrete variables, as done by
              direct-coupling analysis. This is true for (i) the prediction of
              residue-residue contacts in proteins, and (ii) the identification
              of protein-protein interaction partner in bacterial signal
              transduction. An implementation of our multivariate Gaussian
              approach is available at the website
              http://areeweb.polito.it/ricerca/cmp/code.",
  journal  = "PLoS One",
  volume   =  9,
  number   =  3,
  pages    = "e92721",
  month    =  mar,
  year     =  2014,
  keywords = "biocomp;julia;Julia",
  language = "en"
}

@INPROCEEDINGS{Huchette2014-or,
  title     = "Parallel Algebraic Modeling for Stochastic Optimization",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Huchette, J and Lubin, M and Petra, C",
  abstract  = "We present scalable algebraic modeling software, StochJuMP, for
               stochastic optimization as applied to power grid economic
               dispatch. It enables the user to express the problem in a
               high-level algebraic format with minimal boiler-plate. StochJuMP
               allows efficient parallel model instantiation across nodes and
               efficient data localization. Computational results are presented
               showing that the model construction is efficient, requiring
               roughly one percent of solve time. StochJuMP is configured with
               the parallel interior-point solver PIPS-IPM but is sufficiently
               generic to allow straight forward adaptation to other solvers.",
  publisher = "ACM",
  pages     = "29--35",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "parallel programming;power generation dispatch;power
               grids;stochastic programming;PIPS-IPM;StochJuMP;data
               localization;high-level algebraic format;minimal
               boiler-plate;parallel algebraic modeling;parallel interior-point
               solver;parallel model instantiation;power grid economic
               dispatch;scalable algebraic modeling software;stochastic
               optimization;Computational modeling;Data models;Mathematical
               model;Optimization;Sparse matrices;Stochastic
               processes;Symmetric matrices;optimization; parallel programming;
               high performance computing; mathematical model; Power system
               modeling; scalability;julia;Julia"
}

@INPROCEEDINGS{Heitzinger2014-ud,
  title     = "Julia and the Numerical Homogenization of {PDEs}",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Heitzinger, C and Tulzer, G",
  abstract  = "We discuss the advantages of using Julia for solving multiscale
               problems involving partial differential equations (PDEs).
               Multiscale problems are problems where the coefficients of a PDE
               oscillate rapidly on a microscopic length scale, but solutions
               are sought on a much larger, macroscopic domain. Solving
               multiscale problems requires both a theoretic result, i.e., a
               homogenization result yielding effective coefficients, as well
               as numerical solutions of the PDE at the microscopic and the
               macroscopic length scales. Numerical homogenization of PDEs with
               stochastic coefficients is especially computationally expensive.
               Under certain assumptions, effective coefficients can be found,
               but their calculation involves subtle numerical problems. The
               computational cost is huge due to the generally large number of
               stochastic dimensions. Multiscale problems arise in many
               applications, e.g., in uncertainty quantification, in the
               rational design of nanoscale sensors, and in the rational design
               of materials. Our code for the numerical stochastic
               homogenization of elliptic problems is implemented in Julia.
               Since multiscale problems pose new numerical problems, it is in
               any case necessary to develop new numerical codes. Julia is a
               dynamic language inspired by the Lisp family of languages, it is
               open-source, and it provides native-code compilation, access to
               highly optimized linear-algebra routines, support for parallel
               computing, and a powerful macro system. We describe our
               experience in using Julia and discuss the advantages of Julia's
               features in this problem domain.",
  publisher = "ACM",
  pages     = "36--40",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "linear algebra;numerical analysis;parallel processing;partial
               differential equations;Julia;Lisp family;PDE;dynamic
               language;elliptic problems;homogenization result yielding
               effective coefficients;macroscopic domain;macroscopic length
               scales;microscopic length scales;multiscale problems;nanoscale
               sensors;native code compilation;numerical codes;numerical
               problems;numerical solutions;numerical stochastic
               homogenization;optimized linear algebra routines;parallel
               computing;partial differential equations;powerful macro
               system;stochastic coefficients;stochastic
               dimensions;Mathematical model;Microscopy;Nanoscale
               devices;Object oriented modeling;Sensors;Sparse
               matrices;Stochastic processes;Julia; high-performance computing;
               PDEs; numerical homogenization;julia;Julia"
}

@INPROCEEDINGS{Foulds2013-qa,
  title     = "Stochastic collapsed variational Bayesian inference for latent
               Dirichlet allocation",
  booktitle = "Proceedings of the 19th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Foulds, James and Boyles, Levi and DuBois, Christopher and
               Smyth, Padhraic and Welling, Max",
  abstract  = "There has been an explosion in the amount of digital text
               information available in recent years, leading to challenges of
               scale for traditional inference algorithms for topic models.
               Recent advances in stochastic variational inference algorithms
               for latent Dirichlet allocation (LDA) have made it feasible to
               learn topic models on very large-scale corpora, but these
               methods do not currently take full advantage of the collapsed
               representation of the model. We propose a stochastic algorithm
               for collapsed variational Bayesian inference for LDA, which is
               simpler and more efficient than the state of the art method. In
               experiments on large-scale text corpora, the algorithm was found
               to converge faster and often to a better solution than previous
               methods. Human-subject experiments also demonstrated that the
               method can learn coherent topics in seconds on small corpora,
               facilitating the use of topic models in interactive document
               analysis software.",
  publisher = "ACM",
  pages     = "446--454",
  series    = "KDD '13",
  month     =  aug,
  year      =  2013,
  address   = "New York, NY, USA",
  keywords  = "stochastic learning; topic models; variational
               inference;julia;Julia"
}

@MASTERSTHESIS{Verstraete2014-ag,
  title    = "Parallelle abstracties voor het programmeren van {\{GPU's\}} in
              \{Julia\} (Parallel abstractions for programming {\{GPUs\}} in
              \{Julia\})",
  author   = "Verstraete, Pieter",
  editor   = "De Sutter, Bjorn and Besard, Tim",
  abstract = "This master's thesis explores the possibility to provide access
              to the computing power of a GPU from the high-level programming
              language Julia. An important requirement here is to keep the
              programmer's productivity at the same high level as if he would
              use Julia without a GPU. Indeed, very specialized and detailed
              technical knowledge is needed in order to program a GPU, making
              it complex and time-consuming. In many modern scientific domains
              quite a lot of brute computing power is required, but often these
              domains lack the technical expertise to use GPUs in an efficient
              manner. The purpose of this thesis is to provide access to a GPU
              from Julia in a way that shields the GPU details from the
              programmer. In a first step we define and implement in Julia
              abstractions that can be executed in parallel on the GPU. Next we
              adapt the Julia compiler such that it can translate these
              abstractions to GPU code. The resulting compiler infrastructure
              manages the GPU in a way that is transparent to the programmer.
              Finally we evaluate the abstractions and compiler infrastructure
              in the context of a concrete application, namely the trace
              transform.",
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Bezanson2014-xl,
  title    = "\{J\}ulia: A Fresh Approach to Numerical Computing",
  author   = "Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah,
              Viral B",
  abstract = "The Julia programming language is gaining enormous popularity.
              Julia was designed to be easy and fast. Most importantly, Julia
              shatters deeply established notions widely held in the applied
              community: 1. High-level, dynamic code has to be slow by some
              sort of law of nature. 2. It is sensible to prototype in one
              language and then recode in another language. 3. There are parts
              of a system for the programmer, and other parts best left
              untouched as they are built by the experts. Julia began with a
              deep understanding of the needs of the scientific programmer and
              the needs of the computer in mind. Bridging cultures that have
              often been distant, Julia combines expertise from computer
              science and computational science creating a new approach to
              scientific computing. This note introduces the programmer to the
              language and the underlying design theory. It invites the reader
              to rethink the fundamental foundations of numerical computing
              systems. In particular, there is the fascinating dance between
              specialization and abstraction. Specialization allows for custom
              treatment. We can pick just the right algorithm for the right
              circumstance and this can happen at runtime based on argument
              types (code selection via multiple dispatch). Abstraction
              recognizes what remains the same after differences are stripped
              away and ignored as irrelevant. The recognition of abstraction
              allows for code reuse (generic programming). A simple idea that
              yields incredible power. The Julia design facilitates this
              interplay in many explicit and subtle ways for machine
              performance and, most importantly, human convenience.",
  month    =  nov,
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Gaudreau2014-cu,
  title    = "Computing Energy Eigenvalues of Anharmonic Oscillators using the
              Double Exponential Sinc collocation Method",
  author   = "Gaudreau, Philippe and Slevinsky, Richard and Safouhi, Hassan",
  abstract = "A quantum anharmonic oscillator is defined by the Hamiltonian H,
              where the potential is given by V. Using the Sinc collocation
              method combined with the double exponential transformation, we
              develop a method to efficiently compute highly accurate
              approximations of energy eigenvalues for anharmonic oscillators.
              Convergence properties of the proposed method are presented.
              Using the principle of minimal sensitivity, we introduce an
              alternate expression for the mesh size for the Sinc collocation
              method which improves considerably the accuracy in computing
              eigenvalues for potentials with multiple wells. We apply our
              method to a number of potentials including potentials with
              multiple wells. The numerical results section clearly illustrates
              the high efficiency and accuracy of the proposed method. All our
              codes are written using the programming language Julia and are
              available upon request.",
  year     =  2014,
  keywords = "julia;Julia"
}

@ARTICLE{Deisenroth2015-zq,
  title    = "Gaussian Processes for {Data-Efficient} Learning in Robotics and
              Control",
  author   = "Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward",
  abstract = "Autonomous learning has been a promising direction in control and
              robotics for more than a decade since data-driven learning allows
              to reduce the amount of engineering knowledge, which is otherwise
              required. However, autonomous reinforcement learning (RL)
              approaches typically require many interactions with the system to
              learn controllers, which is a practical limitation in real
              systems, such as robots, where many interactions can be
              impractical and time consuming. To address this problem, current
              learning approaches typically require task-specific knowledge in
              form of expert demonstrations, realistic simulators, pre-shaped
              policies, or specific knowledge about the underlying dynamics. In
              this paper, we follow a different approach and speed up learning
              by extracting more information from data. In particular, we learn
              a probabilistic, non-parametric Gaussian process transition model
              of the system. By explicitly incorporating model uncertainty into
              long-term planning and controller learning our approach reduces
              the effects of model errors, a key problem in model-based
              learning. Compared to state-of-the art RL our model-based policy
              search method achieves an unprecedented speed of learning. We
              demonstrate its applicability to autonomous learning in real
              robot and control tasks.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  37,
  number   =  2,
  pages    = "408--423",
  month    =  feb,
  year     =  2015,
  keywords = "Bayesian inference; Gaussian processes; Index Terms---Policy
              search; control; reinforcement learning !; robotics;Robotics",
  language = "en"
}

@INPROCEEDINGS{Forte2010-sa,
  title     = "Robot learning by Gaussian process regression",
  booktitle = "19th International Workshop on Robotics in {Alpe-Adria-Danube}
               Region ({RAAD} 2010)",
  author    = "Forte, D and Ude, A and Kos, A",
  abstract  = "Intelligent robots cannot be programmed in advance for all
               possible situations, but they should be able to generalize based
               on the acquired knowledge. In robot learning based on imitation
               of human activity we often use statistical methods that
               generalize observed (learned) movements. The acquired data is
               used to generate useful robots responses in situations for which
               the robot has not been specifically instructed how to respond.
               The paper describes the robot learning with Gaussian process
               regression that creates the model and estimates the parameters
               for generalization of the acquired motor knowledge, which is
               accumulated as a database of example movements. New actions are
               synthesized by applying Gaussian process regression, where the
               goal and other characteristics of an action are utilized as
               queries to create an optimal control policy with respect to the
               previously acquired knowledge. The paper demonstrates that the
               proposed methodology can be integrated with an active vision
               system of a humanoid robot. 3D vision data is used to provide
               query points for statistical generalization.",
  publisher = "IEEE",
  pages     = "303--308",
  month     =  jun,
  year      =  2010,
  keywords  = "3D vision data; Control system synthesis; Databases; Gaussian
               process regression; Gaussian processes; Humanoid robots; Humans;
               Intelligent robots; Machine vision; Optimal control; Parameter
               estimation; Statistical analysis; active vision system; humanoid
               robot; humanoid robots; intelligent robots; learning systems;
               optimal control; optimal control policy; regression analysis;
               robot learning; robot vision; statistical methods;Robotics"
}

@ARTICLE{Kaelbling1996-tw,
  title    = "Reinforcement Learning: A Survey",
  author   = "Kaelbling, L P and Littman, M L and Moore, A W",
  journal  = "J. Artif. Intell. Res.",
  volume   =  4,
  pages    = "237--285",
  year     =  1996,
  keywords = "reinforcement learning;ReinforcementLearning;Robotics"
}

@INPROCEEDINGS{Jordan1997-nb,
  title     = "Hidden Markov decision trees",
  booktitle = "{NIPS}",
  author    = "Jordan, Michael I and Ghahramani, Zoubin and Saul, Lawrence K",
  abstract  = "We study a time series model that can be viewed as a decision
               tree with Markov temporal structure. The model is intractable
               for exact calculations, thus we utilize variational
               approximations. We consider three diierent distributions for the
               approximation: one in which the Markov calculations are
               performed exactly and the layers of the decision tree are
               decoupled, one in which the decision tree calculations are
               performed exactly and the time steps of the Markov chain are
               decoupled, and one in which a Viterbi-like assumption is made to
               pick out a single most likely state sequence. We present
               simulation results for artiicial data and the Bach chorales.",
  year      =  1997,
  keywords  = "HMM;Assurances"
}

@ARTICLE{Tesch2012-mg,
  title    = "Expensive Multiobjective Optimization and Validation with a
              Robotics Application",
  author   = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract = "Many practical optimization problems, especially in robotics,
              involve multiple competing objectives, e.g. performance metrics
              such as speed and energy effi-ciency. Proper treatment of these
              objective functions is often overlooked. Addi-tionally,
              optimization of the performance of robotic systems can be
              restricted due to the expensive nature of testing control
              parameters on a physical system. This paper presents a
              multi-objective optimization (MOO) algorithm for
              expensive-to-evaluate functions which generates a Pareto set of
              solutions. This algorithm is compared against another leading MOO
              algorithm, and then used to optimize the speed and head stability
              of the sidewinding gait for a snake robot.",
  journal  = "NIPS",
  year     =  2012,
  keywords = "optimisation;robotics;Robotics"
}

@INPROCEEDINGS{Tesch2013-kg,
  title     = "Expensive multiobjective optimization for robotics",
  booktitle = "2013 {IEEE} International Conference on Robotics and Automation",
  author    = "Tesch, M and Schneider, J and Choset, H",
  abstract  = "Many practical optimization problems in robotics involve
               multiple competing objectives - from design trade-offs to
               performance metrics of the physical system such as speed and
               energy efficiency. Proper treatment of these objective
               functions, while commonplace in fields such as economics, is
               often overlooked in robotics. Additionally, optimization of the
               performance of robotic systems can be restricted due to the
               expensive nature of testing control parameters on a physical
               system. This paper presents a multi-objective optimization (MOO)
               algorithm for expensive-to-evaluate functions that generates a
               Pareto set of solutions. This algorithm is compared against
               another leading MOO algorithm, and then used to optimize the
               speed and head stability of the sidewinding gait for a snake
               robot.",
  publisher = "IEEE",
  pages     = "973--980",
  month     =  may,
  year      =  2013,
  keywords  = "Estimation; MOO algorithm; Pareto optimisation; Pareto solution
               set; control parameters; design trade-offs; energy efficiency;
               expensive multiobjective optimization; expensive-to-evaluate
               functions; head stability; mobile robots; multiple competing
               objectives; physical system; robotic systems; sidewinding gait;
               snake robot; speed stability;
               stability;optimisation;robotics;Robotics"
}

@INPROCEEDINGS{Tesch2013-xq,
  title     = "Learning Stochastic Binary Tasks using Bayesian Optimization
               with Shared Task Knowledge",
  booktitle = "International Conference on Machine Learning: Workshop on Robot
               Learning (Atlanta, June 16-21 2013)",
  author    = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract  = "Robotic systems often have tunable parame-ters which can affect
               performance; Bayesian optimization methods provide for efficient
               pa-rameter optimization, reducing required tests on the robot.
               This paper addresses Bayesian optimization in the setting where
               perfor-mance is only observed through a stochastic binary
               outcome -- success or failure. We de-fine the stochastic binary
               optimization prob-lem, present a Bayesian framework using
               Gaussian processes for classification, adapt the existing
               expected improvement metric for the binary case, and benchmark
               its perfor-mance. We also exploit problem structure and task
               similarity to generate principled task priors allowing efficient
               search for diffi-cult tasks. This method is used to create an
               adaptive policy for climbing over obstacles of varying heights.",
  year      =  2013,
  keywords  = "optimisation;robotics;Robotics"
}

@ARTICLE{Ayvali2015-bl,
  title         = "Using Bayesian Optimization to Guide Probing of a Flexible
                   Environment for Simultaneous Registration and Stiffness
                   Mapping",
  author        = "Ayvali, Elif and Srivatsan, Rangaprasad Arun and Wang, Long
                   and Roy, Rajarshi and Simaan, Nabil and Choset, Howie",
  abstract      = "One of the goals of computer-aided surgery is to match
                   intraoperative data to preoperative images of the anatomy
                   and add complementary information that can facilitate the
                   task of surgical navigation. In this context, mechanical
                   palpation can reveal critical anatomical features such as
                   arteries and cancerous lumps which are stiffer that the
                   surrounding tissue. This work uses position and force
                   measurements obtained during mechanical palpation for
                   registration and stiffness mapping. Prior approaches,
                   including our own, exhaustively palpated the entire organ to
                   achieve this goal. To overcome the costly palpation of the
                   entire organ, a Bayesian optimization framework is
                   introduced to guide the end effector to palpate stiff
                   regions while simultaneously updating the registration of
                   the end effector to an a priori geometric model of the
                   organ, hence enabling the fusion of ntraoperative data into
                   the a priori model obtained through imaging. This new
                   framework uses Gaussian processes to model the stiffness
                   distribution and Bayesian optimization to direct where to
                   sample next for maximum information gain. The proposed
                   method was evaluated with experimental data obtained using a
                   Cartesian robot interacting with a silicone organ model and
                   an ex vivo porcine liver.",
  month         =  sep,
  year          =  2015,
  keywords      = "optimisation;robotics;Robotics",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1509.05830"
}

@ARTICLE{Kandasamy_undated-rf,
  title    = "High Dimensional Bayesian Optimisation and Bandits via Additive
              Models",
  author   = "Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos,
              Barnab{\'a}s",
  abstract = "Bayesian Optimisation (BO) is a technique used in optimising a
              D-dimensional function which is typically expensive to evaluate.
              While there have been many successes for BO in low dimen-sions,
              scaling it to high dimensions has been no-toriously difficult.
              Existing literature on the topic are under very restrictive
              settings. In this paper, we identify two key challenges in this
              endeavour. We tackle these challenges by assuming an addi-tive
              structure for the function. This setting is sub-stantially more
              expressive and contains a richer class of functions than previous
              work. We prove that, for additive functions the regret has only
              lin-ear dependence on D even though the function depends on all D
              dimensions. We also demon-strate several other statistical and
              computational benefits in our framework. Via synthetic exam-ples,
              a scientific simulation and a face detection problem we
              demonstrate that our method outper-forms naive BO on additive
              functions and on sev-eral examples where the function is not
              additive.",
  keywords = "optimisation;robotics;Robotics"
}

@ARTICLE{Tesch_undated-vw,
  title    = "Adapting Control Policies for Expensive Systems to Changing
              Environments",
  author   = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract = "--- Many controlled systems must operate over a range of external
              conditions. In this paper, we focus on the problem of learning a
              policy to adapt a system's controller based on the value of these
              external conditions in order to always perform well (i.e.,
              maximize system output). In addition, we are concerned with
              systems for which it is expensive to run experiments, and
              therefore restrict the number that can be run during training. We
              formally define the problem setup and the notion of an optimal
              control policy. We propose two algorithms which aim to find such
              a policy while minimizing the number of system output
              evaluations. We present results comparing these algorithms and
              various other approaches and discuss the inherent tradeoffs in
              the proposed algorithms. Finally, we use these methods to train
              both simulated and physical snake robots to automatically adapt
              to changing terrain, and demonstrate improved performance on test
              courses with changing environ-ments.",
  keywords = "optimisation;robotics;Robotics"
}

@INPROCEEDINGS{Tesch2011-ep,
  title     = "Using response surfaces and expected improvement to optimize
               snake robot gait parameters",
  booktitle = "2011 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems",
  author    = "Tesch, M and Schneider, J and Choset, H",
  abstract  = "Several categories of optimization problems suffer from
               expensive objective function evaluation, driving the need for
               smart selection of subsequent experiments. One such category of
               problems involves physical robotic systems, which often require
               significant time, effort, and monetary expenditure in order to
               run tests. To assist in the selection of the next experiment,
               there has been a focus on the idea of response surfaces in
               recent years. These surfaces interpolate the existing data and
               provide a measure of confidence in their error, serving as a
               low-fidelity surrogate function that can be used to more
               intelligently choose the next experiment. In this paper, we
               robustly implement a previous algorithm based on the response
               surface methodology with an expected improvement criteria. We
               apply this technique to optimize open-loop gait parameters for
               snake robots, and demonstrate improved locomotive capabilities.",
  publisher = "IEEE",
  pages     = "1069--1074",
  year      =  2011,
  keywords  = "Noise; Noise measurement; Optimization; Response surface
               methodology; Robot kinematics; Surface
               treatment;optimisation;robotics;Robotics"
}

@ARTICLE{Berenson2008-bb,
  title    = "An Optimization Approach to Planning for Mobile Manipulation",
  author   = "Berenson, Dmitry and Kuffner, James and Choset, Howie",
  abstract = "--- We present an optimization-based approach to grasping and
              path planning for mobile manipulators. We focus on pick-and-place
              operations, where a given object must be moved from its start
              configuration to its goal configuration by the robot. Given only
              the start and goal configurations of the object and a model of
              the robot and scene, our algorithm finds a grasp and a trajectory
              for the robot that will bring the object to its goal
              configuration. The algorithm consists of two phases: optimization
              and planning. In the optimization phase, the optimal robot
              configurations and grasp are found for the object in its start
              and goal configurations using a co-evolutionary algorithm. In the
              planning phase, a path is found connecting the two robot
              configurations found by the optimization phase using
              Rapidly-Exploring Random Trees (RRTs). We benchmark our algorithm
              and demonstrate it on a 10 DOF mobile manipulator performing
              complex pick-and-place tasks in simulation.",
  year     =  2008,
  keywords = "Evolutionary Robotics; Manipulation Planning; Motion and Path
              Planning;optimisation;robotics;Robotics;Robotics/Planning"
}

@ARTICLE{Leger1999-pr,
  title    = "Automated Synthesis and Optimization of Robot Configurations : An
              Evolutionary Approach",
  author   = "Leger, Chris",
  abstract = "Robot configuration design is hampered by the lack of
              established, well-known design rules, and designers cannot easily
              grasp the space of possible designs and the im- pact of all
              design variables on a robots performance. Realistically, a human
              can only de- sign and evaluate several candidate configurations,
              though there may be thousands of competitive designs that should
              be investigated. In contrast, an automated approach to
              configuration synthesis can create tens of thousands of designs
              and measure the perfor- mance of each one without relying on
              previous experience or design rules. This thesis creates
              Darwin2K, an extensible, automated system for robot configu-
              ration synthesis. This research focuses on the development of
              synthesis capabilities re- quired for many robot design problems:
              a flexible and effective synthesis algorithm, useful simulation
              capabilities, appropriate representation of robots and their
              properties, and the ability to accomodate application-specific
              synthesis needs. Darwin2K can synthe- size and optimize
              kinematics, dynamics, structural geometry, actuator selection,
              and task and control parameters for a wide range of robots.
              Darwin2K uses an evolutionary algorithm to synthesize robots, and
              utilizes two new multi-objective selection procedures that are
              applicable to other evolutionary design domains. The evolutionary
              algorithm can effectively optimize multiple performance ob-
              jectives while satisfying multiple performance constraints, and
              can generate a range of so- lutions representing different
              trade-offs between objectives. Darwin2K uses a novel
              representation for robot configurations called the parameterized
              module configuration graph, enabling efficient and extensible
              synthesis of mobile robots, of single, multiple and bifurcating
              manipulators, and of robots with either modular or monolithic
              construction. Task-specific simulation is used to provide the
              synthesis algorithm with perfor- mance measurements for each
              robot. Darwin2K can automatically derive dynamic equa- tions for
              each robot it simulates, enabling dynamic simulation to be used
              during synthesis for the first time. Darwin2K also includes a
              variety of simulation components, including Jacobian and PID
              controllers, algorithms for estimating link deflection and for
              detecting collisions; modules for robot links, joints (including
              actuator models), tools, and bases (fixed and mobile); and
              metrics such as task coverage, task completion time, end effector
              error, actuator saturation, and link deflection. A significant
              component of the system is its extensible object-oriented
              software architecture, which allows new simulation capabili- ties
              and robot modules to be added without impacting the synthesis
              algorithm. The ar- chitecture also encourages re-use of existing
              toolkit components, allowing task-specific simulators to be
              quickly constructed. Darwin2Ks synthesis algorithm, simulation
              capabilities, and extensible architec- ture combine to allow
              synthesis of robots for a wide range of tasks. Results are
              presented for nearly 150 synthesis experiments for six different
              applications, including synthesis of a free-flying 22-DOF robot
              with multiple manipulators and a walking machine for zero-
              gravity truss walking. The synthesis system and results represent
              a significant advance in the state-of-the-art in automated
              synthesis for robotics.",
  journal  = "Des. Eng.",
  pages    = "1--234",
  year     =  1999,
  keywords = "robotics;Robotics"
}

@BOOK{Shannon2015-cs,
  title    = "The mathematical theory of communication",
  author   = "Shannon, C E and Weaver, W",
  year     =  2015,
  keywords = "Information theory;NotRead;Reading List/Historical"
}

@MISC{DeepDive_undated-du,
  title    = "Factor Graph",
  author   = "{DeepDive}",
  abstract = "A simple tutorial about factor graphs",
  keywords = "GraphicalModels"
}

@ARTICLE{Sutton_undated-hy,
  title    = "An Introduction to Conditional Random Fields for Relational
              Learning",
  author   = "Sutton, Charles and Mccallum, Andrew",
  keywords = "GraphicalModels"
}

@BOOK{Lavalle2006-gt,
  title     = "Planning Algorithms",
  author    = "Lavalle, S M",
  abstract  = "This book presents a unified treatment of many different kinds
               of planning algorithms. The subject lies at the crossroads
               between robotics, control theory, artificial intelligence,
               algorithms, and computer graphics. The particular subjects
               covered include motion planning, discrete planning, planning
               under uncertainty, sensor-based planning, visibility,
               decision-theoretic planning, game theory, information spaces,
               reinforcement learning, nonlinear systems, trajectory planning,
               nonholonomic planning, and kinodynamic planning.",
  publisher = "Cambridge University Press, Cambridge, London and New York",
  pages     = "842",
  year      =  2006,
  keywords  = "Textbook;TextBooks;Robotics/Planning"
}

@INCOLLECTION{Mateus2016-du,
  title     = "{Human-Aware} Navigation Using External Omnidirectional Cameras",
  booktitle = "Robot 2015: Second Iberian Robotics Conference",
  author    = "Mateus, Andr{\'e} and Miraldo, Pedro and Lima, Pedro U and
               Sequeira, Jo{\~a}o",
  editor    = "Reis, Lu{\'\i}s Paulo and Moreira, Ant{\'o}nio Paulo and Lima,
               Pedro U and Montano, Luis and Mu{\~n}oz-Martinez, Victor",
  abstract  = "If robots are to invade our homes and offices, they will have to
               interact more naturally with humans. Natural interaction will
               certainly include the ability of robots to plan their motion,
               accounting for the social norms enforced. In this paper we
               propose a novel solution for Human-Aware Navigation resorting to
               external omnidirectional static cameras, used to implement a
               vision-based person tracking system. The proposed solution was
               tested in a typical domestic indoor scenario in four different
               experiments. The results show that the robot is able to cope
               with human-aware constraints, defined after common proxemics
               rules.",
  publisher = "Springer International Publishing",
  pages     = "283--295",
  series    = "Advances in Intelligent Systems and Computing",
  year      =  2016,
  address   = "Lisbon",
  keywords  = "Robotics/Planning",
  language  = "en"
}

@INCOLLECTION{Ventura2014-gn,
  title     = "Towards Optimal Robot Navigation in Domestic Spaces",
  booktitle = "{RoboCup} 2014: Robot World Cup {XVIII}",
  author    = "Ventura, Rodrigo and Ahmad, Aamir",
  abstract  = "The work presented in this paper is motivated by the goal of
               depend-able autonomous navigation of mobile robots. This goal is
               a fundamental require-ment for having autonomous robots in
               spaces such as domestic spaces and public establishments, left
               unattended by technical staff. In this paper we tackle this
               problem by taking an optimization approach: on one hand, we use
               a Fast March-ing Approach for path planning, resulting in
               optimal paths in the absence of un-mapped obstacles, and on the
               other hand we use a Dynamic Window Approach for guidance. To the
               best of our knowledge, the combination of these two meth-ods is
               novel. We evaluate the approach on a real mobile robot, capable
               of moving at high speed. The evaluation makes use of an external
               ground truth system. We report controlled experiments that we
               performed, including the presence of peo-ple moving randomly
               nearby the robot. In our long term experiments we report a total
               distance of 18 km traveled during 11 hours of movement time.",
  pages     = "318--331",
  year      =  2014,
  keywords  = "Robotics/Planning"
}

@ARTICLE{Kruse2013-qw,
  title    = "Human-aware robot navigation: A survey",
  author   = "Kruse, Thibault and Pandey, Amit Kumar and Alami, Rachid and
              Kirsch, Alexandra",
  abstract = "Abstract Navigation is a basic skill for autonomous robots. In
              the last years human--robot interaction has become an important
              research field that spans all of the robot capabilities including
              perception, reasoning, learning, manipulation and navigation. For
              navigation, the presence of humans requires novel approaches that
              take into account the constraints of human comfort as well as
              social rules. Besides these constraints, putting robots among
              humans opens new interaction possibilities for robots, also for
              navigation tasks, such as robot guides. This paper provides a
              survey of existing approaches to human-aware navigation and
              offers a general classification scheme for the presented methods.",
  journal  = "Rob. Auton. Syst.",
  volume   =  61,
  number   =  12,
  pages    = "1726--1743",
  month    =  dec,
  year     =  2013,
  keywords = "Autonomous robot; Navigation; Human-aware; Human-centered
              environment; Survey;Robotics/Planning"
}

@INPROCEEDINGS{Lillard2015-yg,
  title     = "Assurances for Enhanced Trust in Autonomous Systems",
  booktitle = "Proceedings: 2015 {AAAI} Fall Symposium",
  author    = "Lillard, Austin and Frew, Eric W and Argrow, Brian and Lawrence,
               Dale and Ahmed, Nisar",
  abstract  = "This paper investigates a model-based approach to understanding
               how user trust evolves in systems consisting of a supervising
               user and an autonomous agent. This model consists of a
               multivariate model for user trust, and a feedback connection
               between user and agent. Feedback information is termed
               assurance, which is also shown to consist of multiple aspects
               concerning the state of the autonomous agent. It is argued that
               the closed loop interactions between user and agent can and
               should be designed to foster user trust. In order to develop
               design principles, it is first necessary to define the terms and
               salient components of these models and provide a logical
               framework for their interconnection. This is the object of this
               paper. Although elements such as trust and assurance are
               essential in a usable autonomous system, they are also nebulous
               concepts with multiple meanings. Here we provide definitions and
               stucture that enables a systematic study of the problem.",
  month     =  nov,
  year      =  2015,
  keywords  = "self-confidence;assurances;in\_presentation;Assurances"
}

@INPROCEEDINGS{Zucchini1999-yi,
  title     = "Illustrations of the use of pseudo-residuals in assessing the
               fit of a model",
  booktitle = "proceedings of the 14th international workshop on statistical
               modelling",
  author    = "Zucchini, Walter and Macdonald, Iain L",
  pages     = "37--40",
  year      =  1999,
  address   = "Graz, Austria",
  keywords  = "model checking; model selection;
               pseudo-residuals;Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Zhang2015-ql,
  title         = "Plan Explicability and Predictability for Robot Task
                   Planning",
  author        = "Zhang, Yu and Sreedharan, Sarath and Kulkarni, Anagha and
                   Chakraborti, Tathagata and Zhuo, Hankz Hankui and
                   Kambhampati, Subbarao",
  abstract      = "Intelligent robots and machines are becoming pervasive in
                   human populated environments. A desirable capability of
                   these agents is to respond to goal-oriented commands by
                   autonomously constructing task plans. However, such autonomy
                   can add significant cognitive load and potentially introduce
                   safety risks to humans when agents behave unexpectedly.
                   Hence, for such agents to be helpful, one important
                   requirement is for them to synthesize plans that can be
                   easily understood by humans. While there exists previous
                   work that studied socially acceptable robots that interact
                   with humans in ``natural ways'', and work that investigated
                   legible motion planning, there lacks a general solution for
                   high level task planning. To address this issue, we
                   introduce the notions of plan \{\textbackslashit
                   explicability\} and \{\textbackslashit predictability\}. To
                   compute these measures, first, we postulate that humans
                   understand agent plans by associating abstract tasks with
                   agent actions, which can be considered as a labeling
                   process. We learn the labeling scheme of humans for agent
                   plans from training examples using conditional random fields
                   (CRFs). Then, we use the learned model to label a new plan
                   to compute its explicability and predictability. These
                   measures can be used by agents to proactively choose or
                   directly synthesize plans that are more explicable and
                   predictable to humans. We provide evaluations on a synthetic
                   domain and with human subjects using physical robots to show
                   the effectiveness of our approach",
  month         =  nov,
  year          =  2015,
  keywords      = "Assurances/Quantifying/Consistency",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1511.08158"
}

@ARTICLE{Burrell2016-uy,
  title     = "How the machine `thinks': Understanding opacity in machine
               learning algorithms",
  author    = "Burrell, Jenna",
  abstract  = "This article considers the issue of opacity as a problem for
               socially consequential mechanisms of classification and ranking,
               such as spam filters, credit card fraud detection, search
               engines, news trends, market segmentation and advertising,
               insurance or loan qualification, and credit scoring. These
               mechanisms of classification all frequently rely on
               computational algorithms, and in many cases on machine learning
               algorithms to do this work. In this article, I draw a
               distinction between three forms of opacity: (1) opacity as
               intentional corporate or state secrecy, (2) opacity as technical
               illiteracy, and (3) an opacity that arises from the
               characteristics of machine learning algorithms and the scale
               required to apply them usefully. The analysis in this article
               gets inside the algorithms themselves. I cite existing
               literatures in computer science, known industry practices (as
               they are publicly presented), and do some testing and
               manipulation of code as a form of lightweight code audit. I
               argue that recogni...",
  journal   = "Big Data \& Society",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  3,
  number    =  1,
  pages     = "78",
  month     =  jan,
  year      =  2016,
  keywords  = "classification;transparency;Assurances",
  language  = "en"
}

@INPROCEEDINGS{Kadous1999-rx,
  title     = "Learning Comprehensible Descriptions of Multivariate Time Series",
  booktitle = "In Proceedings of the 16 th International Conference of Machine
               Learning ({ICML-99}",
  author    = "Kadous, Mohammed Waleed",
  editor    = "Ivan Bratko, Saso Dzeroski",
  abstract  = "Supervised classification is one of the most active areas of
               machine learning research. Most work has focused on
               classification in static domains, where an instantaneous
               snapshot of attributes is meaningful. In many domains,
               attributes are not static; in fact, it is the way they vary
               temporally that can make classification possible. Examples of
               such domains include speech recognition, gesture recognition and
               electrocardiograph classification. While it is possible to use
               ad hoc, domain-specific techniques for ``flattening '' the time
               series to a learner-friendly representation, this fails to take
               into account both the special problems and special heuristics
               applicable to temporal data and often results in unreadable
               concept descriptions. Though traditional time series techniques
               can sometimes produce accurate classifiers, few can provide
               comprehensible descriptions. We propose a general architecture
               for classification and description of multivariate time series.
               It employs event primitive...",
  pages     = "454--463",
  year      =  1999,
  keywords  = "assurance\_explicit;trust\_informal\_treatment;interp\_models;Assurances"
}

@ARTICLE{Dundas2011-mn,
  title         = "{IBSEAD}: - A {Self-Evolving} {Self-Obsessed} Learning
                   Algorithm for Machine Learning",
  author        = "Dundas, Jitesh and Chik, David",
  abstract      = "We present IBSEAD or distributed autonomous entity systems
                   based Interaction - a learning algorithm for the computer to
                   self-evolve in a self-obsessed manner. This learning
                   algorithm will present the computer to look at the internal
                   and external environment in series of independent entities,
                   which will interact with each other, with and/or without
                   knowledge of the computer's brain. When a learning algorithm
                   interacts, it does so by detecting and understanding the
                   entities in the human algorithm. However, the problem with
                   this approach is that the algorithm does not consider the
                   interaction of the third party or unknown entities, which
                   may be interacting with each other. These unknown entities
                   in their interaction with the non-computer entities make an
                   effect in the environment that influences the information
                   and the behaviour of the computer brain. Such details and
                   the ability to process the dynamic and unsettling nature of
                   these interactions are absent in the current learning
                   algorithm such as the decision tree learning algorithm.
                   IBSEAD is able to evaluate and consider such algorithms and
                   thus give us a better accuracy in simulation of the highly
                   evolved nature of the human brain. Processes such as dreams,
                   imagination and novelty, that exist in humans are not fully
                   simulated by the existing learning algorithms. Also, Hidden
                   Markov models (HMM) are useful in finding ``hidden''
                   entities, which may be known or unknown. However, this model
                   fails to consider the case of unknown entities which maybe
                   unclear or unknown. IBSEAD is better because it considers
                   three types of entities- known, unknown and invisible. We
                   present our case with a comparison of existing algorithms in
                   known environments and cases and present the results of the
                   experiments using dry run of the simulated runs of the
                   existing machine learning algorithms versus IBSEAD.",
  month         =  jun,
  year          =  2011,
  keywords      = "Assurances/Self-Aware",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1106.6186"
}

@ARTICLE{Khajah2016-xt,
  title         = "How deep is knowledge tracing?",
  author        = "Khajah, Mohammad and Lindsey, Robert V and Mozer, Michael C",
  abstract      = "In theoretical cognitive science, there is a tension between
                   highly structured models whose parameters have a direct
                   psychological interpretation and highly complex,
                   general-purpose models whose parameters and representations
                   are difficult to interpret. The former typically provide
                   more insight into cognition but the latter often perform
                   better. This tension has recently surfaced in the realm of
                   educational data mining, where a deep learning approach to
                   predicting students' performance as they work through a
                   series of exercises---termed deep knowledge tracing or
                   DKT---has demonstrated a stunning performance advantage over
                   the mainstay of the field, Bayesian knowledge tracing or
                   BKT. In this article, we attempt to understand the basis for
                   DKT's advantage by considering the sources of statistical
                   regularity in the data that DKT can leverage but which BKT
                   cannot. We hypothesize four forms of regularity that BKT
                   fails to exploit: recency effects, the contextualized trial
                   sequence, inter-skill similarity, and individual variation
                   in ability. We demonstrate that when BKT is extended to
                   allow it more flexibility in modeling statistical
                   regularities---using extensions previously proposed in the
                   literature---BKT achieves a level of performance
                   indistinguishable from that of DKT. We argue that while DKT
                   is a powerful, useful, general-purpose framework for
                   modeling student learning, its gains do not come from the
                   discovery of novel representations---the fundamental
                   advantage of deep learning. To answer the question posed in
                   our title, knowledge tracing may be a domain that does not
                   require `depth'; shallow models like BKT can perform just as
                   well and offer us greater interpretability and explanatory
                   power.",
  month         =  mar,
  year          =  2016,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1604.02416"
}

@ARTICLE{Bottou_undated-lv,
  title    = "Two big challenges in machine learning",
  author   = "Bottou, L{\'e}on",
  keywords = "Assurances"
}

@ARTICLE{Lacave2002-cu,
  title     = "A review of explanation methods for Bayesian networks",
  author    = "Lacave, Carmen and D{\'\i}ez, Francisco J",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge University Press",
  volume    =  17,
  number    =  02,
  pages     = "107--127",
  month     =  jun,
  year      =  2002,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;review;Supplemental
               Assurance;Reduce
               Complexity;Assurances/Quantifying/Consistency;Assurances"
}

@ARTICLE{Gomez2015-je,
  title         = "{Real-Time} Stochastic Optimal Control for Multi-agent
                   Quadrotor Systems",
  author        = "G{\'o}mez, Vicen{\c c} and Thijssen, Sep and Symington,
                   Andrew and Hailes, Stephen and Kappen, Hilbert J",
  abstract      = "This paper presents a novel method for controlling teams of
                   unmanned aerial vehicles using Stochastic Optimal Control
                   (SOC) theory. The approach consists of a centralized
                   high-level planner that computes optimal state trajectories
                   as velocity sequences, and a platform-specific low-level
                   controller which ensures that these velocity sequences are
                   met. The planning task is expressed as a centralized
                   path-integral control problem, for which optimal control
                   computation corresponds to a probabilistic inference problem
                   that can be solved by efficient sampling methods. Through
                   simulation we show that our SOC approach (a) has significant
                   benefits compared to deterministic control and other SOC
                   methods in multimodal problems with noise-dependent optimal
                   solutions, (b) is capable of controlling a large number of
                   platforms in real-time, and (c) yields collective emergent
                   behaviour in the form of flight formations. Finally, we show
                   that our approach works for real platforms, by controlling a
                   team of three quadrotors in outdoor conditions.",
  pages         = "1--17",
  month         =  feb,
  year          =  2015,
  keywords      = "Robotics/r4sim",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SY",
  eprint        = "1502.04548"
}

@ARTICLE{Allamraju_undated-tz,
  title    = "{Multi-Agent} Game Emulator ( {MAGE} ) for",
  author   = "Allamraju, Rakshit and Chowdhary, Girish",
  keywords = "Robotics/r4sim"
}

@ARTICLE{Keivan_undated-mo,
  title    = "Generative scene models with analytical path-tracing",
  author   = "Keivan, Nima and Sibley, Gabe",
  pages    = "1--4",
  keywords = "Robotics/r4sim"
}

@ARTICLE{Heckman_undated-zw,
  title    = "Simulation-in-the-loop for Planning and {Model-Predictive}
              Control",
  author   = "Heckman, Christoffer and Keivan, Nima and Sibley, Gabe",
  keywords = "Robotics/r4sim"
}

@ARTICLE{Bennetts2015-gi,
  title    = "Integrated simulation of gas dispersion and mobile sensing
              systems",
  author   = "Bennetts, Victor Hernandez and Lilienthal, Achim J and
              Schaffernicht, Erik and Ferrari, Silvia and Albertson, John",
  pages    = "1--4",
  year     =  2015,
  keywords = "Robotics/r4sim"
}

@ARTICLE{Hauser_undated-xy,
  title    = "Rigid Body Simulation with Point Cloud Models in Klamp ' t",
  author   = "Hauser, Kris",
  keywords = "Robotics/r4sim"
}

@INCOLLECTION{Furrer2016-bz,
  title     = "{RotorS---A} Modular Gazebo {MAV} Simulator Framework",
  booktitle = "Robot Operating System ({ROS})",
  author    = "Furrer, Fadri and Burri, Michael and Achtelik, Markus and
               Siegwart, Roland",
  editor    = "Koubaa, Anis",
  abstract  = "In this chapter we present a modular Micro Aerial Vehicle (MAV)
               simulation framework, which enables a quick start to perform
               research on MAVs. After reading this chapter, the reader will
               have a ready to use MAV simulator, including control and state
               estimation. The simulator was designed in a modular way, such
               that different controllers and state estimators can be used
               interchangeably, while incorporating new MAVs is reduced to a
               few steps. The provided controllers can be adapted to a custom
               vehicle by only changing a parameter file. Different controllers
               and state estimators can be compared with the provided
               evaluation framework. The simulation framework is a good
               starting point to tackle higher level tasks, such as collision
               avoidance, path planning, and vision based problems, like
               Simultaneous Localization and Mapping (SLAM), on MAVs. All
               components were designed to be analogous to its real world
               counterparts. This allows the usage of the same controllers and
               state estimators, including their parameters, in the simulation
               as on the real MAV.",
  publisher = "Springer International Publishing",
  pages     = "595--625",
  series    = "Studies in Computational Intelligence",
  year      =  2016,
  keywords  = "Robotics/r4sim",
  language  = "en"
}

@ARTICLE{Koenig2015-es,
  title    = "Gazebo in the {DRC} : development from the last three years",
  author   = "Koenig, Nathan and Foote, Tully",
  year     =  2015,
  keywords = "Robotics/r4sim"
}

@ARTICLE{Degroote_undated-bw,
  title    = "Integrating Realistic Simulation Engines within the {MORSE}
              Framework",
  author   = "Degroote, Arnaud and Koch, Pierrick and Lacroix, Simon",
  keywords = "Robotics/r4sim"
}

@ARTICLE{Wick1989-jx,
  title    = "The 1988 {AAAI} Workshop on Explanation",
  author   = "Wick, Michael R",
  abstract = "This article is a summary of the Workshop on Explanation held
              during the 1988 National Conference on Artificial Intelligence in
              St. Paul, Minnesota. The purpose of the workshop was to identify
              key research issues in the rapidly emerging area of expert system
              explanation.",
  journal  = "AI Magazine",
  volume   =  10,
  number   =  3,
  pages    = "22",
  month    =  sep,
  year     =  1989,
  keywords = "Assurances"
}

@ARTICLE{Smolensky1988-sq,
  title     = "On the proper treatment of connectionism",
  author    = "Smolensky, Paul",
  journal   = "Behav. Brain Sci.",
  publisher = "Cambridge Univ Press",
  volume    =  11,
  number    =  01,
  pages     = "1--23",
  year      =  1988,
  keywords  = "Assurances;CurrentStudy"
}

@TECHREPORT{Smolensky1987-tj,
  title       = "On the proper treatment of connectionism",
  author      = "Smolensky, Paul",
  number      = " CU-CS-359-87",
  institution = "University of Colorado, Boulder",
  year        =  1987,
  keywords    = "Assurances"
}

@INPROCEEDINGS{Santambrogio2010-ak,
  title     = "Enabling technologies for self-aware adaptive systems",
  booktitle = "2010 {NASA/ESA} Conference on Adaptive Hardware and Systems",
  author    = "Santambrogio, M D and Hoffmann, H and Eastep, J and Agarwal, A",
  abstract  = "Self-aware computer systems will be capable of adapting their
               behavior and resources thousands of times a second to
               automatically find the best way to accomplish a given goal
               despite changing environmental conditions and demands. Such a
               capability benefits a broad spectrum of computer systems from
               embedded systems to supercomputers and is particularly useful
               for meeting power, performance, and resource-metering challenges
               in mobile computing, cloud computing, multicore computing,
               adaptive and dynamic compilation environments, and parallel
               operating systems. Some of the challenges in implementing
               self-aware systems are a) knowing within the system what the
               goals of applications are and if they are meeting them, b)
               deciding what actions to take to help applications meet their
               goals, and c) developing standard techniques that generalize and
               can be applied to a broad range of self-aware systems. This work
               presents our vision for self-aware adaptive systems and proposes
               enabling technologies to address these three challenges. We
               describe a framework called Application Heartbeats that provides
               a general, standardized way for applications to monitor their
               performance and make that information available to external
               observers. Then, through a study of a self-optimizing
               synchronization library called Smartlocks, we demonstrate a
               powerful technique that systems can use to determine which
               optimization actions to take. We show that Heartbeats can be
               applied naturally in the context of reinforcement learning
               optimization strategies as a reward signal and that, using such
               a strategy, Smartlocks are able to significantly improve
               performance of applications on an important emerging class of
               multicore systems called asymmetric multicores.",
  publisher = "IEEE",
  pages     = "149--156",
  month     =  jun,
  year      =  2010,
  keywords  = "embedded systems;learning (artificial
               intelligence);multiprocessing systems;software
               libraries;software performance evaluation;system
               monitoring;Application Heartbeats;Smartlocks;adaptive
               compilation environment;asymmetric multicore;cloud
               computing;dynamic compilation environment;embedded
               system;environmental condition;mobile computing;multicore
               computing;multicore system;parallel operating
               system;reinforcement learning optimization strategy;self-aware
               adaptive system;self-aware computer system;self-optimizing
               synchronization library;Adaptive systems;Benchmark
               testing;Biomedical monitoring;Computers;Monitoring;Multicore
               processing;Assurances/Self-Aware"
}

@ARTICLE{Gorbenko2012-pu,
  title    = "Robot {Self-Awareness}: Occam's Razor for Fluents",
  author   = "Gorbenko, Anna and Popov, Vladimir",
  abstract = "In this paper we consider robot self-awareness from the point of
              view of temporal relation based data mining. In particular, we
              use rational function approximations and consider Occam's razor
              for fluents. A robot with self-aware system has the possibility
              of dealing with complex novel situations more effectively than a
              robot without self-awareness (see e.g. [1, 2]). In [1], the
              authors considered robot self-awareness from the point of view of
              temporal relation based data mining. Temporal patterns can be
              expressed using fluents (see e.g. [1, 3]). A fluent is a
              proposition with temporal extent. For example, `` drinking-coffee
              '' can be defined as a fluent that is true whenever I am drinking
              coffee. This fluent can be represented as a binary time series x,
              where x[t] is 1 if and only if I am drinking coffee at time t.
              Respectively, temporal relations needed only to express relations
              of fluents. To build a system of self-awareness the robot needs a
              system of automatic selection of fluents and prediction of values
              of selected fluents. Using some kind of system of self-learning
              and accumulation of knowledge (see e.g. [1, 3]) the",
  journal  = "Int. J. Math. Anal.",
  volume   =  6,
  number   =  30,
  pages    = "1453--1455",
  year     =  2012,
  keywords = "41A20 Keywords; Mathematics Subject Classification; Occam's
              razor; fluents; robot self-awareness;MLTheory"
}

@ARTICLE{Cox2007-tk,
  title    = "Perpetual {Self-Aware} Cognitive Agents",
  author   = "Cox, Michael T",
  abstract = "To construct a perpetual self-aware cognitive agent that can
              continuously operate with independence, an introspective machine
              must be produced. To assemble such an agent, it is necessary to
              perform a full integration of cognition (planning, understanding,
              and learning) and metacognition (control and monitoring of
              cognition) with intelligent behaviors. The failure to do this
              completely is why similar, more limited efforts have not
              succeeded in the past. I outline some key computational
              requirements of metacognition by describing a multi- strategy
              learning system called Meta-AQUA and then discuss an integration
              of Meta-AQUA with a nonlinear state-space planning agent. I show
              how the resultant system, INTRO, can independently generate its
              own goals, and I relate this work to the general issue of
              self-awareness by machine.",
  journal  = "AI Magazine",
  volume   =  28,
  number   =  1,
  pages    = "32",
  month    =  mar,
  year     =  2007,
  keywords = "introspection;Assurances/Self-Aware"
}

@ARTICLE{Brachman2002-oz,
  title     = "Systems that know what they're doing",
  author    = "Brachman, R J",
  abstract  = "The author discusses cognitive computers with the ability to
               reason, learn and respond intelligently to things that they have
               never encountered before. A truly cognitive system would be able
               to learn from its experience, as well as by being instructed,
               and perform better on day two than it did on day one. It would
               be able to explain what it was doing and why it was doing it.
               The author considers application foundations.",
  journal   = "IEEE Intell. Syst.",
  publisher = "IEEE",
  volume    =  17,
  number    =  6,
  pages     = "67--71",
  month     =  nov,
  year      =  2002,
  keywords  = "Artificial intelligence; Cognition; Humans; Information
               processing; Machine learning; Machine learning algorithms;
               Robustness; Speech processing; Speech recognition; Symbiosis;
               application foundations; cognitive computers; cognitive system;
               cognitive systems; explanation; inference mechanisms; knowledge
               based systems; learning; learning (artificial intelligence);
               reasoning; systems architecture;Assurances"
}

@INPROCEEDINGS{Dwork2012-fq,
  title     = "Fairness through awareness",
  booktitle = "Proceedings of the 3rd Innovations in Theoretical Computer
               Science Conference",
  author    = "Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and
               Reingold, Omer and Zemel, Richard",
  abstract  = "We study fairness in classification, where individuals are
               classified, e.g., admitted to a university, and the goal is to
               prevent discrimination against individuals based on their
               membership in some group, while maintaining utility for the
               classifier (the university). The main conceptual contribution of
               this paper is a framework for fair classification comprising (1)
               a (hypothetical) task-specific metric for determining the degree
               to which individuals are similar with respect to the
               classification task at hand; (2) an algorithm for maximizing
               utility subject to the fairness constraint, that similar
               individuals are treated similarly. We also present an adaptation
               of our approach to achieve the complementary goal of ``fair
               affirmative action,'' which guarantees statistical parity (i.e.,
               the demographics of the set of individuals receiving any
               classification are the same as the demographics of the
               underlying population), while treating similar individuals as
               similarly as possible. Finally, we discuss the relationship of
               fairness to privacy: when fairness implies privacy, and how
               tools developed in the context of differential privacy may be
               applied to fairness.",
  publisher = "ACM",
  pages     = "214--226",
  month     =  jan,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "classification;transparency;trust\_informal\_treatment;assurance\_explicit;des\_behavior;Assurances"
}

@ARTICLE{Li2010-nx,
  title     = "Knows what it knows: a framework for self-aware learning",
  author    = "Li, Lihong and Littman, Michael L and Walsh, Thomas J and
               Strehl, Alexander L",
  abstract  = "We introduce a learning framework that combines elements of the
               well-known PAC and mistake-bound models. The KWIK (knows what it
               knows) framework was designed particularly for its utility in
               learning settings where active exploration can impact the
               training examples the learner is exposed to, as is true in
               reinforcement-learning and active-learning problems. We catalog
               several KWIK-learnable classes as well as open problems, and
               demonstrate their applications in experience-efficient
               reinforcement learning.",
  journal   = "Mach. Learn.",
  publisher = "Springer US",
  volume    =  82,
  number    =  3,
  pages     = "399--443",
  month     =  nov,
  year      =  2010,
  keywords  = "MLTheory/UnsupervisedLearning",
  language  = "en"
}

@ARTICLE{Zahalka2011-mq,
  title    = "An experimental test of Occam's razor in classification",
  author   = "Zah{\'a}lka, Jan and {\v Z}elezn{\'y}, Filip",
  abstract = "A widely persisting interpretation of Occam's razor is that given
              two classifiers with the same training error, the simpler
              classifier is more likely to generalize better. Within a
              long-lasting debate in the machine learning community over
              Occam's razor, Domingos (Data Min. Knowl. Discov. 3:409--425,
              1999) rejects this interpretation and proposes that model
              complexity is only a confounding factor usually correlated with
              the number of mod-els from which the learner selects. It is thus
              hypothesized that the risk of overfitting (poor generalization)
              follows only from the number of model tests rather than the
              complexity of the selected model. We test this hypothesis on 30
              UCI data sets using polynomial classi-fication models. The
              results confirm Domingos' hypothesis on the 0.05 significance
              level and thus refutes the above interpretation of Occam's razor.
              Our experiments however also illustrate that decoupling the two
              factors (model complexity and number of model tests) is
              problematic.",
  journal  = "Mach. Learn.",
  volume   =  82,
  number   =  3,
  pages    = "475--481",
  month    =  mar,
  year     =  2011,
  keywords = "MLTheory"
}

@INPROCEEDINGS{Tse2015-tz,
  title     = "Human-robot information sharing with structured language
               generation from probabilistic beliefs",
  booktitle = "2015 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems ({IROS})",
  author    = "Tse, R and Campbell, M",
  abstract  = "This paper presents a framework for information sharing and
               fusion in cooperative tasks involving humans and robots. In this
               context, all information regarding the state of interest is
               recursively fused and maintained by each agent in a form of
               belief. For a robot agent, its belief is commonly and
               practically represented as a probability density function (pdf),
               formed by traditional sensor fusion and state estimation
               algorithms. In cooperative tasks with non-expert humans, a robot
               needs to effectively communicate its belief so that the gathered
               information can be easily processed and interpreted by the
               humans. The goal of this research is to provide two-way
               information exchange and fusion between robots and humans, the
               former operating on pdfs, while the latter on English sentences.
               This is achieved by considering two goodness measures: semantic
               correctness and information preservation. Based on the goodness
               measures studied, results show that the proposed framework is
               able to generate optimal statements describing the given belief
               pdfs and successfully recover the initial inputs used to
               generate them. Additionally, in order to describe complex belief
               pdfs, a Mixture of Statements (MoS) model is proposed such that
               the optimal expression can be generated through a composition of
               more than one statements. With a nonparametric Dirichlet Process
               MoS generation, it is found that the robot can determine
               correctly the number of statements as well as the corresponding
               reference parameters needed to describe all hypotheses
               underlying its belief.",
  volume    = "2015-Decem",
  pages     = "1242--1248",
  year      =  2015,
  keywords  = "Information management; Logistics; Probability density function;
               Robot sensing systems; Semantics; Time measurement; belief
               networks; human-robot interaction; language translation;
               probability; sensor fusion; state estimation; English sentences;
               human-robot information sharing; information preservation;
               mixture of statements model; nonparametric Dirichlet process MoS
               generation; probabilistic beliefs; robot agent; semantic
               correctness; state estimation algorithms; structured language
               generation; two-way information exchange;Integral Assurance;User
               Interaction;Human-RobotCollaboration"
}

@INPROCEEDINGS{Dragan2013-wd,
  title     = "Generating Legible Motion",
  booktitle = "Proceedings of Robotics: Science and Systems",
  author    = "Dragan, Anca and Srinivasa, Siddhartha",
  abstract  = "Legible motion --- motion that communicates its intent to a
               human observer --- is crucial for enabling seamless human-robot
               collaboration. In this paper, we propose a functional gradient
               optimization technique for autonomously generating legible
               motion. Our algorithm optimizes a legibility metric inspired by
               the psychology of action interpretation in humans, resulting in
               motion trajectories that purposefully deviate from what an
               observer would expect in order to better convey intent. A trust
               region constraint on the optimization ensures that the motion
               does not become too surprising or unpredictable to the observer.
               Our studies with novice users that evaluate the resulting
               trajectories support the applicability of our method and of such
               a trust region. They show that within the region, legibility as
               measured in practice does significantly increase. Outside of it,
               however, the trajectory becomes confusing and the users'
               confidence in knowing the robot's intent significantly
               decreases.",
  year      =  2013,
  keywords  = "human\_study;assurance\_predictability;reinforcement
               learning;ai\_planning;assurance\_explicit;trust\_formal\_treatment;in\_paper;Integral
               Assurance;Human-like Behavior;Assurances"
}

@ARTICLE{Kadous2005-of,
  title     = "Classification of Multivariate Time Series and Structured Data
               Using Constructive Induction",
  author    = "Kadous, Mohammed Waleed and Sammut, Claude",
  abstract  = "We present a method of constructive induction aimed at learning
               tasks involving multivariate time series data. Using
               metafeatures, the scope of attribute-value learning is expanded
               to domains with instances that have some kind of recurring
               substructure, such as strokes in handwriting recognition, or
               local maxima in time series data. The types of substructures are
               defined by the user, but are extracted automatically and are
               used to construct attributes.Metafeatures are applied to two
               real domains: sign language recognition and ECG classification.
               Using metafeatures we are able to generate classifiers that are
               either comprehensible or accurate, producing results that are
               comparable to hand-crafted preprocessing and comparable to human
               experts.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  58,
  number    = "2-3",
  pages     = "179--216",
  month     =  feb,
  year      =  2005,
  keywords  = "time series;Assurances/Quantifying/Consistency",
  language  = "en"
}

@INPROCEEDINGS{Williams1996-xr,
  title     = "Computing With Infinite Networks",
  booktitle = "Advances in Neural Information Processing Systems 9",
  author    = "Williams, Christopher",
  abstract  = "For neural networks with a wide class of weight-priors, it can
               be shown that in the limit of an infinite number of hidden units
               the prior over functions tends to a Gaussian process. In this
               paper analytic forms are derived for the covariance function of
               the Gaussian processes corresponding to networks with sigmoidal
               and Gaussian hidden units. This allows predictions to be made
               efficiently using networks with an infinite number of hidden
               units, and shows that, somewhat paradoxically, it may be easier
               to compute with infinite networks than finite ones.",
  publisher = "MIT Press",
  pages     = "295--301",
  year      =  1996,
  keywords  = "GPs"
}

@PHDTHESIS{Neal1995-xb,
  title    = "{BAYESIAN} {LEARNING} {FOR} {NEURAL} {NETWORKS}",
  author   = "Neal, Radford M",
  abstract = "Two features distinguish the Bayesian approach to learning models
              from data. First, beliefs derived from background knowledge are
              used to select a prior probability distribution for the model
              parameters. Second, predictions of future observations are made
              by integrating the model's predictions with respect to the
              posterior parameter distribution obtained by updating this prior
              to take account of the data. For neural network models, both
              these aspects present diiculties | the prior over network
              parameters has no obvious relation to our prior knowledge, and
              integration over the posterior is computationally very demanding.
              I address the problem by deening classes of prior distributions
              for network param-eters that reach sensible limits as the size of
              the network goes to innnity. In this limit, the properties of
              these priors can be elucidated. Some priors converge to Gaussian
              processes, in which functions computed by the network may be
              smooth, Brownian, or fractionally Brownian. Other priors converge
              to non-Gaussian stable processes. Interesting eeects are obtained
              by combining priors of both sorts in networks with more than one
              hidden layer.",
  year     =  1995,
  school   = "University of Toronto",
  keywords = "MLTheory/DeepLearning"
}

@INPROCEEDINGS{Gal2015-wt,
  title     = "Dropout as a Bayesian Approximation: Insights and Applications",
  booktitle = "Deep Learning Workshop, {ICML}",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  abstract  = "Deep learning techniques are used more and more often, but they
               lack the ability to reason about uncertainty over the features.
               Features ex-tracted from a dataset are given as point
               esti-mates, and do not capture how much the model is confident
               in its estimation. This is in contrast to probabilistic Bayesian
               models, which allow rea-soning about model confidence, but often
               with the price of diminished performance. We show that a
               multilayer perceptron (MLP) with arbitrary depth and
               non-linearities, with dropout applied after every weight layer,
               is math-ematically equivalent to an approximation to a well
               known Bayesian model. This interpretation offers an explanation
               to some of dropout's key properties, such as its robustness to
               over-fitting. Our interpretation allows us to reason about
               un-certainty in deep learning, and allows the intro-duction of
               the Bayesian machinery into existing deep learning frameworks in
               a principled way. Our analysis suggests straightforward
               generalisa-tions of dropout for future research which should
               improve on current techniques.",
  year      =  2015,
  keywords  = "BayesOpt;MLTheory/DeepLearning;Assurances/Quantifying/Consistency"
}

@INPROCEEDINGS{Snoek2015-lr,
  title     = "Scalable Bayesian Optimization Using Deep Neural Networks",
  booktitle = "Proceedings of The 32nd International Conference on Machine
               Learning",
  author    = "Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros,
               Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary,
               Md Mostofa Ali and {Prabhat} and Adams, Ryan P",
  abstract  = "Bayesian optimization is an effective methodology for the global
               optimization of functions with expensive evaluations. It relies
               on querying a distribution over functions defined by a
               relatively cheap surrogate model. An accurate model for this
               distribution over functions is critical to the effectiveness of
               the approach, and is typically fit using Gaussian processes
               (GPs). However, since GPs scale cubically with the number of
               observations, it has been challenging to handle objectives whose
               optimization requires many evaluations, and as such, massively
               parallelizing the optimization. In this work, we explore the use
               of neural networks as an alternative to GPs to model
               distributions over functions. We show that performing adaptive
               basis function regression with a neural network as the
               parametric form performs competitively with state-of-the-art
               GP-based approaches, but scales linearly with the number of data
               rather than cubically. This allows us to achieve a previously
               intractable degree of parallelism, which we apply to large scale
               hyperparameter optimization, rapidly finding competitive models
               on benchmark object recognition tasks using convolutional
               networks, and image caption generation using neural language
               models.",
  pages     = "2171--2180",
  month     =  feb,
  year      =  2015,
  keywords  = "BayesOpt"
}

@ARTICLE{Tan2013-ua,
  title    = "More than Accuracy: Interpretability",
  author   = "Tan, Chenhao",
  year     =  2013,
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@INCOLLECTION{Otte2013-oo,
  title     = "Safe and Interpretable Machine Learning: A Methodological Review",
  booktitle = "Computational Intelligence in Intelligent Data Analysis",
  author    = "Otte, Clemens",
  editor    = "Moewes, Christian and N{\"u}rnberger, Andreas",
  abstract  = "When learning models from data, the interpretability of the
               resulting model is often mandatory. For example, safety-related
               applications for automation and control require that the
               correctness of the model must be ensured not only for the
               available data but for all possible input combinations. Thus,
               understanding what the model has learned and in particular how
               it will extrapolate to unseen data is a crucial concern. The
               paper discusses suitable learning methods for classification and
               regression. For classification problems, we review an approach
               based on an ensemble of nonlinear low-dimensional submodels,
               where each submodel is simple enough to be completely verified
               by domain experts. For regression problems, we review related
               approaches that try to achieve interpretability by using
               low-dimensional submodels (for instance, MARS and tree-growing
               methods). We compare them with symbolic regression, which is a
               different approach based on genetic algorithms. Finally, a novel
               approach is proposed for combining a symbolic regression model,
               which is shown to be easily interpretable, with a Gaussian
               Process. The combined model has an improved accuracy and
               provides error bounds in the sense that the deviation from the
               verified symbolic model is always kept below a defined limit.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "111--122",
  series    = "Studies in Computational Intelligence",
  year      =  2013,
  keywords  = "Safety\_AI;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Assurances",
  language  = "en"
}

@PHDTHESIS{Ruping2006-xj,
  title    = "Learning Interpretable Models",
  author   = "Ruping, Stephan",
  year     =  2006,
  school   = "University of Darmstadt",
  keywords = "
              trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;in\_presentation;Integral
              Assurance;Interpretable Models;CurrentStudy;Assurances"
}

@ARTICLE{Rudin2013-no,
  title    = "Learning Theory Analysis for Association Rules and Sequential
              Event Prediction",
  author   = "Rudin, Cynthia and Letham, Benjamin and Madigan, David",
  abstract = "We present a theoretical analysis for prediction algorithms based
              on association rules. As part of this analysis, we introduce a
              problem for which rules are particularly natural, called ``
              sequential event prediction. '' In sequential event prediction,
              events in a sequence are revealed one by one, and the goal is to
              determine which event will next be revealed. The training set is
              a collection of past sequences of events. An example application
              is to predict which item will next be placed into a customer's
              online shopping cart, given his/her past purchases. In the
              context of this problem, algorithms based on association rules
              have distinct advantages over classical statistical and machine
              learning methods: they look at correlations based on subsets of
              co-occurring past events (items a and b imply item c), they can
              be applied to the sequential event prediction problem in a
              natural way, they can potentially handle the `` cold start ''
              problem where the training set is small, and they yield
              interpretable predictions. In this work, we present two
              algorithms that incorporate association rules. These algorithms
              can be used both for sequential event prediction and for
              supervised classification, and they are simple enough that they
              can possibly be understood by users, customers, patients,
              managers, etc. We provide generalization guarantees on these
              algorithms based on algorithmic stability analysis from
              statistical learning theory. We include a discussion of the
              strict minimum support threshold often used in association rule
              mining, and introduce an `` adjusted confidence '' measure that
              provides a weaker minimum support condition that has advantages
              over the strict minimum support. The paper brings together ideas
              from statistical learning theory, association rule mining and
              Bayesian analysis.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  14,
  pages    = "3385--3436",
  year     =  2013,
  keywords = "algorithmic stability; association rules; associative
              classification; sequence prediction; statistical learning
              theory;Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Letham2015-fx,
  title     = "Interpretable classifiers using rules and Bayesian analysis:
               Building a better stroke prediction model",
  author    = "Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H and
               Madigan, David",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Appl. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  9,
  number    =  3,
  pages     = "1350--1371",
  month     =  sep,
  year      =  2015,
  keywords  = "Bayesian analysis; classification; interpretability;Reading
               List;COHRINT Reading List -- Brett",
  language  = "en"
}

@ARTICLE{Zeng2017-us,
  title    = "Interpretable classification models for recidivism prediction",
  author   = "Zeng, Jiaming and Ustun, Berk and Rudin, Cynthia",
  abstract = "We investigate a long-debated question, which is how to create
              predictive models of recidivism that are sufficiently accurate,
              transparent and interpretable to use for decision making. This
              question is complicated as these models are used to support
              different decisions, from sentencing, to determining release on
              probation to allocating preventative social services. Each case
              might have an objective other than classification accuracy, such
              as a desired true positive rate TPR or false positive rate FPR.
              Each (TPR, FPR) pair is a point on the receiver operator
              characteristic (ROC) curve. We use popular machine learning
              methods to create models along the full ROC curve on a wide range
              of recidivism prediction problems. We show that many methods
              (support vector machines, stochastic gradient boosting and ridge
              regression) produce equally accurate models along the full ROC
              curve. However, methods that are designed for interpretability
              (classification and regression trees and C5.0) cannot be tuned to
              produce models that are accurate and/or interpretable. To handle
              this shortcoming, we use a recent method called supersparse
              linear integer models to produce accurate, transparent and
              interpretable scoring systems along the full ROC curve. These
              scoring systems can be used for decision making for many
              different use cases, since they are just as accurate as the most
              powerful black box machine learning models for many applications,
              but completely transparent, and highly interpretable.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  180,
  number   =  3,
  pages    = "689--722",
  month    =  jun,
  year     =  2017,
  keywords = "Binary classification; Interpretability; Machine learning;
              Recidivism; Scoring systems;Reading List;COHRINT Reading List --
              Brett"
}

@INCOLLECTION{Gama2004-so,
  title     = "Learning with Drift Detection",
  booktitle = "Advances in Artificial Intelligence -- {SBIA} 2004",
  author    = "Gama, Jo{\~a}o and Medas, Pedro and Castillo, Gladys and
               Rodrigues, Pedro",
  editor    = "Bazzan, Ana L C and Labidi, Sofiane",
  abstract  = "Most of the work in machine learning assume that examples are
               generated at random according to some stationary probability
               distribution. In this work we study the problem of learning when
               the distribution that generate the examples changes over time.
               We present a method for detection of changes in the probability
               distribution of examples. The idea behind the drift detection
               method is to control the online error-rate of the algorithm. The
               training examples are presented in sequence. When a new training
               example is available, it is classified using the actual model.
               Statistical theory guarantees that while the distribution is
               stationary, the error will decrease. When the distribution
               changes, the error will increase. The method controls the trace
               of the online error of the algorithm. For the actual context we
               define a warning level, and a drift level. A new context is
               declared, if in a sequence of examples, the error increases
               reaching the warning level at example k w , and the drift level
               at example k d . This is an indication of a change in the
               distribution of the examples. The algorithm learns a new model
               using only the examples since k w . The method was tested with a
               set of eight artificial datasets and a real world dataset. We
               used three learning algorithms: a perceptron, a neural network
               and a decision tree. The experimental results show a good
               performance detecting drift and with learning the new concept.
               We also observe that the method is independent of the learning
               algorithm.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  3171,
  pages     = "286--295",
  series    = "Lecture Notes in Computer Science",
  year      =  2004,
  address   = "Berlin, Heidelberg",
  keywords  = "Assurances/Quantifying/Consistency"
}

@BOOK{Quinonero-Candela2009-fj,
  title     = "Dataset Shift in Machine Learning",
  author    = "Quinonero-Candela, Joaquin and Sugiyama, Masashi and
               Schwaighofer, Anton and Lawrence, Neil D",
  abstract  = "Dataset shift is a common problem in predictive modeling that
               occurs when the joint distribution of inputs and outputs differs
               between training and test stages. Covariate shift, a particular
               case of dataset shift, occurs when only the input distribution
               changes. Dataset shift is present in most practical
               applications, for reasons ranging from the bias introduced by
               experimental design to the irreproducibility of the testing
               conditions at training time. (An example is -email spam
               filtering, which may fail to recognize spam that differs in form
               from the spam the automatic filter has been built on.) Despite
               this, and despite the attention given to the apparently similar
               problems of semi-supervised learning and active learning,
               dataset shift has received relatively little attention in the
               machine learning community until recently. This volume offers an
               overview of current efforts to deal with dataset and covariate
               shift. The chapters offer a mathematical and philosophical
               introduction to the problem, place dataset shift in relationship
               to transfer learning, transduction, local learning, active
               learning, and semi-supervised learning, provide theoretical
               views of dataset and covariate shift (including decision
               theoretic and Bayesian perspectives), and present algorithms for
               covariate shift. Contributors [cut for catalog if necessary]Shai
               Ben-David, Steffen Bickel, Karsten Borgwardt, Michael
               Br{\"u}ckner, David Corfield, Amir Globerson, Arthur Gretton,
               Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo,
               Takafumi Kanamori, Klaus-Robert M{\"u}ller, Sam Roweis, Neil
               Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard
               Sch{\"o}lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey,
               Masashi Sugiyama",
  publisher = "MIT Press",
  year      =  2009,
  keywords  = "
               Safety\_AI;Uncertainty;NotRead;trust\_informal\_treatment;assurance\_implicit;risk\_safety\_nonstationary;in\_paper;Integral
               Assurance;Value Alignment;Assurances",
  language  = "en"
}

@BOOK{Minsky1988-jh,
  title     = "Perceptrons: An Introduction to Computational Geometry",
  author    = "Minsky, Marvin Lee and Papert, Seymour",
  abstract  = "Perceptrons - the first systematic study of parallelism in
               computation - has remained a classical work on threshold
               automata networks for nearly two decades. It marked a historical
               turn in artificial intelligence, and it is required reading for
               anyone who wants to understand the connectionist
               counterrevolution that is going on today.Artificial-intelligence
               research, which for a time concentrated on the programming of
               ton Neumann computers, is swinging back to the idea that
               intelligence might emerge from the activity of networks of
               neuronlike entities. Minsky and Papert's book was the first
               example of a mathematical analysis carried far enough to show
               the exact limitations of a class of computing machines that
               could seriously be considered as models of the brain. Now the
               new developments in mathematical tools, the recent interest of
               physicists in the theory of disordered matter, the new insights
               into and psychological models of how the brain works, and the
               evolution of fast computers that can simulate networks of
               automata have given Perceptrons new importance.Witnessing the
               swing of the intellectual pendulum, Minsky and Papert have added
               a new chapter in which they discuss the current state of
               parallel computers, review developments since the appearance of
               the 1972 edition, and identify new research directions related
               to connectionism. They note a central theoretical challenge
               facing connectionism: the challenge to reach a deeper
               understanding of how ``objects'' or ``agents'' with
               individuality can emerge in a network. Progress in this area
               would link connectionism with what the authors have called
               ``society theories of mind.''Marvin L. Minsky is Donner
               Professor of Science in MIT's Electrical Engineering and
               Computer Science Department. Seymour A. Papert is Professor of
               Media Technology at MIT.",
  publisher = "M.I.T.Pr.",
  pages     = "295",
  edition   = "Expanded",
  year      =  1988,
  keywords  = "Machine learning;Textbook;TextBooks;MLTheory",
  language  = "en"
}

@INPROCEEDINGS{Doyle2014-gy,
  title     = "Rapid Adaptive Realistic Behavior Modeling is Viable for Use in
               Training",
  booktitle = "Proceedings of the 23rd Conference on Behavior Representation in
               Modeling and Simulation ({BRIMS})",
  author    = "Doyle, Margery J and Portrey, Antoinette M",
  abstract  = "For many years it has been recognized that the design,
               development, and execution of adaptive threat generation systems
               and the use of rapid modeling techniques within applied research
               and training environments poses many methodological and
               integration challenges. With support from the Air Force Research
               Laboratory, 711th Human Performance Wing (711/HPW) Warfighter
               Readiness Research Division (WRRD) at Wright-Patterson Air Force
               Base, Ohio, through collaboration with Tier1, Aptima, Charles
               River Analytics, CHI Systems, SoarTech, Alion, and
               Stottler-Henke, and AFRL's Performance and Learning Models
               (PALM) branch, Phase I of the `` Not-So-Grand Challenge ''
               (NSGC) was launched to assess the critical issues facing current
               and future threat generation systems, the models used in them,
               and the extent to which current behavior and systems modeling
               architectures could provide military training with flexible,
               adaptable accurate/credible models of human behavior and
               realistic threats.",
  year      =  2014,
  keywords  = "AFRL\_STTR/AFRL"
}

@ARTICLE{Bottou2016-az,
  title         = "Optimization Methods for {Large-Scale} Machine Learning",
  author        = "Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge",
  abstract      = "This paper provides a review and commentary on the past,
                   present, and future of numerical optimization algorithms in
                   the context of machine learning applications. Through case
                   studies on text classification and the training of deep
                   neural networks, we discuss how optimization problems arise
                   in machine learning and what makes them challenging. A major
                   theme of our study is that large-scale machine learning
                   represents a distinctive setting in which the stochastic
                   gradient (SG) method has traditionally played a central role
                   while conventional gradient-based nonlinear optimization
                   techniques typically falter. Based on this viewpoint, we
                   present a comprehensive theory of a straightforward, yet
                   versatile SG algorithm, discuss its practical behavior, and
                   highlight opportunities for designing algorithms with
                   improved performance. This leads to a discussion about the
                   next generation of optimization methods for large-scale
                   machine learning, including an investigation of two main
                   streams of research on techniques that diminish noise in the
                   stochastic directions and methods that make use of
                   second-order derivative approximations.",
  month         =  jun,
  year          =  2016,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.04838"
}

@ARTICLE{Mnih_undated-zh,
  title    = "Asynchronous Methods for Deep Reinforcement Learning",
  author   = "Mnih, Volodymyr and Puigdom{\`e}nech Badia, Adri{\`a} and Mirza,
              Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P
              and Silver, David and Kavukcuoglu, Koray and Com, Korayk@google
              and Deepmind, Google",
  abstract = "We propose a conceptually simple and lightweight framework for
              deep reinforce-ment learning that uses asynchronous gradient
              descent for optimization of deep neural network controllers. We
              present asynchronous variants of four standard reinforcement
              learning algorithms and show that parallel actor-learners have a
              stabilizing effect on training allowing all four methods to
              successfully train neural network controllers. The best
              performing method, an asynchronous variant of actor-critic,
              surpasses the current state-of-the-art on the Atari domain while
              training for half the time on a single multi-core CPU instead of
              a GPU. Furthermore, we show that asynchronous actor-critic
              succeeds on a wide variety of continuous motor control problems
              as well as on a new task of navigating random 3D mazes using a
              visual input.",
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Amodei2016-xi,
  title         = "Concrete Problems in {AI} Safety",
  author        = "Amodei, Dario and Olah, Chris and Steinhardt, Jacob and
                   Christiano, Paul and Schulman, John and Man{\'e}, Dan",
  abstract      = "Rapid progress in machine learning and artificial
                   intelligence (AI) has brought increasing attention to the
                   potential impacts of AI technologies on society. In this
                   paper we discuss one such potential impact: the problem of
                   accidents in machine learning systems, defined as unintended
                   and harmful behavior that may emerge from poor design of
                   real-world AI systems. We present a list of five practical
                   research problems related to accident risk, categorized
                   according to whether the problem originates from having the
                   wrong objective function (``avoiding side effects'' and
                   ``avoiding reward hacking''), an objective function that is
                   too expensive to evaluate frequently (``scalable
                   supervision''), or undesirable behavior during the learning
                   process (``safe exploration'' and ``distributional shift'').
                   We review previous work in these areas as well as suggesting
                   research directions with a focus on relevance to
                   cutting-edge AI systems. Finally, we consider the high-level
                   question of how to think most productively about the safety
                   of forward-looking applications of AI.",
  month         =  jun,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_explicit;perf\_prediction;des\_behavior;Integral
                   Assurance;Assurances/Self-Aware;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1606.06565"
}

@ARTICLE{Liang2015-jv,
  title    = "Bayesian Sensitivity Analysis and Uncertainty Integration for
              Robust Optimization",
  author   = "Liang, Chen and Mahadevan, Sankaran",
  abstract = "This paper presents a comprehensive methodology that combines
              uncertainty quantification, uncertainty propagation, and design
              optimization using a Bayesian framework. The epistemic
              uncertainty due to input data uncertainty is considered. Two
              types of uncertainty models for input variables and/or their
              distribution parameters are addressed: 1) uncertainty modeled as
              family of distributions, and 2) uncertainty modeled as interval
              data. A Bayesian approach is adopted to update the uncertainty
              models, where the likelihood functions are constructed using
              limited experimental data. Global sensitivity analysis, which
              previously only considered aleatory inputs in the context of
              probabilistic representation, is extended in this paper to
              quantify the contributions of both aleatory and epistemic
              uncertainty sources for multioutput problems using an auxiliary
              variable approach. Gaussian process surrogate modeling is
              employed to replace the expensive physics models and improve the
              computational efficiency. A previously developed
              bias-minimization technique, which only dealt with single-output
              functions, is extended to reduce the surrogate model error for a
              multioutput function. A decoupled robustness-based design
              optimization framework is developed to include both aleatory and
              epistemic uncertainties. The proposed methodology is illustrated
              using the NASA Langley Research Center's multidisciplinary
              uncertainty quantification challenge problem.",
  journal  = "Journal of Aerospace Information Systems",
  volume   =  12,
  number   =  1,
  pages    = "189--203",
  year     =  2015,
  keywords = "AFRL\_STTR/Related\_JAIS"
}

@ARTICLE{Grande2014-pz,
  title    = "Experimental Validation of Bayesian Nonparametric Adaptive
              Control Using Gaussian Processes",
  author   = "Grande, Robert C and Chowdhary, Girish and How, Jonathan P",
  abstract = "Many current model reference adaptive control methods employ
              parametric adaptive elements in which the number of parameters
              are fixed a priori and the hyperparameters, such as the
              bandwidth, are predefined, often through expert judgment. As an
              alternative to these methods, a nonparametric model using
              Gaussian processes was recently proposed. Using Gaussian
              processes, it is possible to maintain constant coverage over the
              operating domain by adaptively selecting new kernel locations as
              well as adapt hyperparameters in an online setting to improve
              model prediction. In this work, the first extensive experimental
              flight results are presented using Gaussian process/model
              reference adaptive control. Experimental results show that
              Gaussian process/model reference adaptive control outperforms
              traditional model reference adaptive control methods that use
              radial basis function neural networks in terms of tracking error
              as well as transient behavior on trajectory following using a
              quadrotor. Results show an improvement of a factor of two to
              three over preexisting state-of-the-art methods. Additionally,
              many model reference adaptive control frameworks treat the
              adaptive element as being known exactly, and they do not
              incorporate certainty of the prior model into the control policy.
              In this paper, the notion of a Bayesian scaling factor is
              introduced that scales the adaptive element in order to
              incorporate the uncertainty of the prior model and current model
              confidence. The stability and convergence of using the Bayesian
              scaling factor in a closed loop is proven.",
  journal  = "Journal of Aerospace Information Systems",
  volume   =  11,
  number   =  9,
  pages    = "565--578",
  year     =  2014,
  keywords = "AFRL\_STTR/Related\_JAIS"
}

@INPROCEEDINGS{Adebayo2015-fa,
  title     = "The Hidden Cost of Efficiency: Fairness and Discrimination in
               Predictive Modeling",
  booktitle = "Bloomberg Data for Good Exchaange Conference",
  author    = "Adebayo, Julius and Kagal, Lalana",
  abstract  = "We present a data transformation procedure that completely
               eliminates all linear information regarding a sensitive
               at-tribute in a large scale individual level data with several
               correlated attributes. The algorithm presented here forms a
               component of a larger fairness rating system being devel-oped.
               The goal of the rating system is to elucidate black-box models
               and bring interpretability to any predictive model no matter how
               complex. As part of the system, we learn lower dimensional
               interpretable versions of potentially com-plex models, attempt
               to learn a causal structure of the un-derlying data, and propose
               an augumentation to the un-derlying black-box algorithm as a way
               of reducing bias in predictive modeling. This work is still
               ongoing, but here we highlight one component of the system: the
               orthogonal projection algorithm. The orthogonal projection
               algorithm combines principal components analysis of the data set
               with orthogonalization with respect to sensitive attribute(s).
               The orthogonalization algorithm presented is motivated by
               appli-cations where there is a need to drastically 'sanitize' a
               data set of all information relating to sensitive(s)
               attribute(s) in the data, or that perhaps could be inferred from
               the data, before analysis of the data using a data mining
               algorithm. Our proposed methodology outperforms other privacy
               pre-serving methodologies by more than 20 percent in lowering
               the ability to reconstruct sensitive attribute from a sample
               large scale individual level data. In high stakes contexts such
               as determination of access to credit, employment, and insur-ance
               where discrimination based on sensitive attributes such as race,
               gender, and sexual orientation is prohibited by law, our
               proposed algorithm provides a way to help reduce the information
               content of such sensitive attributes in available data, hence
               limiting bias.",
  year      =  2015,
  address   = "NYC, NY USA",
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Seshia2016-ck,
  title         = "Towards Verified Artificial Intelligence",
  author        = "Seshia, Sanjit A and Sadigh, Dorsa and Shankar Sastry, S",
  abstract      = "Verified artificial intelligence (AI) is the goal of
                   designing AI-based systems that are provably correct with
                   respect to mathematically-specified requirements. This paper
                   considers Verified AI from a formal methods perspective. We
                   describe five challenges for achieving Verified AI, and five
                   corresponding principles for addressing these challenges.",
  month         =  jun,
  year          =  2016,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1606.08514"
}

@ARTICLE{Goodfellow2014-cz,
  title         = "Explaining and Harnessing Adversarial Examples",
  author        = "Goodfellow, Ian J and Shlens, Jonathon and Szegedy,
                   Christian",
  abstract      = "Several machine learning models, including neural networks,
                   consistently misclassify adversarial examples---inputs
                   formed by applying small but intentionally worst-case
                   perturbations to examples from the dataset, such that the
                   perturbed input results in the model outputting an incorrect
                   answer with high confidence. Early attempts at explaining
                   this phenomenon focused on nonlinearity and overfitting. We
                   argue instead that the primary cause of neural networks'
                   vulnerability to adversarial perturbation is their linear
                   nature. This explanation is supported by new quantitative
                   results while giving the first explanation of the most
                   intriguing fact about them: their generalization across
                   architectures and training sets. Moreover, this view yields
                   a simple and fast method of generating adversarial examples.
                   Using this approach to provide examples for adversarial
                   training, we reduce the test set error of a maxout network
                   on the MNIST dataset.",
  month         =  dec,
  year          =  2014,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1412.6572"
}

@ARTICLE{Azimi2012-gk,
  title         = "Hybrid Batch Bayesian Optimization",
  author        = "Azimi, Javad and Jalali, Ali and Fern, Xiaoli",
  abstract      = "Bayesian Optimization aims at optimizing an unknown
                   non-convex/concave function that is costly to evaluate. We
                   are interested in application scenarios where concurrent
                   function evaluations are possible. Under such a setting, BO
                   could choose to either sequentially evaluate the function,
                   one input at a time and wait for the output of the function
                   before making the next selection, or evaluate the function
                   at a batch of multiple inputs at once. These two different
                   settings are commonly referred to as the sequential and
                   batch settings of Bayesian Optimization. In general, the
                   sequential setting leads to better optimization performance
                   as each function evaluation is selected with more
                   information, whereas the batch setting has an advantage in
                   terms of the total experimental time (the number of
                   iterations). In this work, our goal is to combine the
                   strength of both settings. Specifically, we systematically
                   analyze Bayesian optimization using Gaussian process as the
                   posterior estimator and provide a hybrid algorithm that,
                   based on the current state, dynamically switches between a
                   sequential policy and a batch policy with variable batch
                   sizes. We provide theoretical justification for our
                   algorithm and present experimental results on eight
                   benchmark BO problems. The results show that our method
                   achieves substantial speedup (up to \%78) compared to a pure
                   sequential policy, without suffering any significant
                   performance loss.",
  month         =  feb,
  year          =  2012,
  keywords      = "BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1202.5597"
}

@ARTICLE{Werling2015-wg,
  title         = "{On-the-Job} Learning with Bayesian Decision Theory",
  author        = "Werling, Keenon and Chaganty, Arun and Liang, Percy and
                   Manning, Chris",
  abstract      = "Our goal is to deploy a high-accuracy system starting with
                   zero training examples. We consider an ``on-the-job''
                   setting, where as inputs arrive, we use real-time
                   crowdsourcing to resolve uncertainty where needed and output
                   our prediction when confident. As the model improves over
                   time, the reliance on crowdsourcing queries decreases. We
                   cast our setting as a stochastic game based on Bayesian
                   decision theory, which allows us to balance latency, cost,
                   and accuracy objectives in a principled way. Computing the
                   optimal policy is intractable, so we develop an
                   approximation based on Monte Carlo Tree Search. We tested
                   our approach on three datasets---named-entity recognition,
                   sentiment classification, and image classification. On the
                   NER task we obtained more than an order of magnitude
                   reduction in cost compared to full human annotation, while
                   boosting performance relative to the expert provided labels.
                   We also achieve a 8\% F1 improvement over having a single
                   human label the whole set, and a 28\% F1 improvement over
                   online learning.",
  month         =  jun,
  year          =  2015,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1506.03140"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiesemann2013-ct,
  title    = "Robust Markov Decision Processes",
  author   = "Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c c}",
  abstract = "Markov decision processes (MDPs) are powerful tools for decision
              making in uncertain dynamic environments. However, the solutions
              of MDPs are of limited practical use because of their sensitivity
              to distributional model parameters, which are typically unknown
              and have to be estimated by the decision maker. To counter the
              detrimental effects of estimation errors, we consider robust MDPs
              that offer probabilistic guarantees in view of the unknown
              parameters. To this end, we assume that an observation history of
              the MDP is available. Based on this history, we derive a
              confidence region that contains the unknown parameters with a
              prespecified probability 1 − Afterward, we determine a policy
              that attains the highest worst-case performance over this
              confidence region. By construction, this policy achieves or
              exceeds its worst-case performance with a confidence of at least
              1 − Our method involves the solution of tractable conic programs
              of moderate size.",
  journal  = "Mathematics of OR",
  volume   =  38,
  number   =  1,
  pages    = "153--183",
  month    =  feb,
  year     =  2013,
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Ghosh2016-qu,
  title     = "Trusted Machine Learning for Probabilistic Models",
  booktitle = "Reliable Machine Learning in the Wild at {ICML} 2016",
  author    = "Ghosh, Shalini and Lincoln, Patrick and Tiwari, Ashish and Zhu,
               Xiaojin",
  abstract  = "In several mission-critical domains (e.g., self-driving cars,
               cybersecurity, robotics) where ma-chine learning algorithms are
               being used heav-ily, it is becoming increasingly important to
               en-sure that the learned models satisfy some domain properties
               (e.g., temporal constraints). Towards this goal, we propose
               Trusted Machine Learning (TML), wherein we combine the strengths
               of ma-chine learning and model checking. If the desired logical
               properties are not satisfied by a trained model, we modify
               either the model ('model re-pair') or the data from which the
               model is learned ('data repair'). We outline a concrete case
               study based on the Markov Chain model of a car con-troller for
               'lane changing' --- we demonstrate how we can ensure that such a
               model, learned from data, satisfies properties specified in
               Probabilistic Computation Tree Logic (PCTL).",
  year      =  2016,
  keywords  = "Assurances/Self-Aware"
}

@ARTICLE{Bottou2013-sx,
  title    = "Counterfactual Reasoning and Learning Systems: The Example of
              Computational Advertising",
  author   = "Bottou, L{\'e}on and Peters, Jonas and Ch, Peters@stat and
              Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and
              Chickering, D Max and Portugaly, Elon and Ray, Dipankar and
              Simard, Patrice and Snelson, Ed",
  abstract = "This work shows how to leverage causal inference to understand
              the behavior of complex learning systems interacting with their
              environment and predict the consequences of changes to the
              sys-tem. Such predictions allow both humans and algorithms to
              select the changes that would have improved the system
              performance. This work is illustrated by experiments on the ad
              placement system associated with the Bing search engine.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  14,
  pages    = "3207--3260",
  year     =  2013,
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Bottou2011-gp,
  title         = "From Machine Learning to Machine Reasoning",
  author        = "Bottou, Leon",
  abstract      = "A plausible definition of ``reasoning'' could be
                   ``algebraically manipulating previously acquired knowledge
                   in order to answer a new question''. This definition covers
                   first-order logical inference or probabilistic inference. It
                   also includes much simpler manipulations commonly used to
                   build large learning systems. For instance, we can build an
                   optical character recognition system by first training a
                   character segmenter, an isolated character recognizer, and a
                   language model, using appropriate labeled training sets.
                   Adequately concatenating these modules and fine tuning the
                   resulting system can be viewed as an algebraic operation in
                   a space of models. The resulting model answers a new
                   question, that is, converting the image of a text page into
                   a computer readable text. This observation suggests a
                   conceptual continuity between algebraically rich inference
                   systems, such as logical or probabilistic inference, and
                   simple manipulations, such as the mere concatenation of
                   trainable learning systems. Therefore, instead of trying to
                   bridge the gap between machine learning systems and
                   sophisticated ``all-purpose'' inference mechanisms, we can
                   instead algebraically enrich the set of manipulations
                   applicable to training systems, and build reasoning
                   capabilities from the ground up.",
  month         =  feb,
  year          =  2011,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1102.1808"
}

@ARTICLE{Srinivas2009-vw,
  title         = "Gaussian Process Optimization in the Bandit Setting: No
                   Regret and Experimental Design",
  author        = "Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M
                   and Seeger, Matthias",
  abstract      = "Many applications require optimizing an unknown, noisy
                   function that is expensive to evaluate. We formalize this
                   task as a multi-armed bandit problem, where the payoff
                   function is either sampled from a Gaussian process (GP) or
                   has low RKHS norm. We resolve the important open problem of
                   deriving regret bounds for this setting, which imply novel
                   convergence rates for GP optimization. We analyze GP-UCB, an
                   intuitive upper-confidence based algorithm, and bound its
                   cumulative regret in terms of maximal information gain,
                   establishing a novel connection between GP optimization and
                   experimental design. Moreover, by bounding the latter in
                   terms of operator spectra, we obtain explicit sublinear
                   regret bounds for many commonly used covariance functions.
                   In some important cases, our bounds have surprisingly weak
                   dependence on the dimensionality. In our experiments on real
                   sensor data, GP-UCB compares favorably with other
                   heuristical GP optimization approaches.",
  month         =  dec,
  year          =  2009,
  keywords      = "BayesOpt;BayesOpt/Acquisition/InfillFxns",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "0912.3995"
}

@PHDTHESIS{Schonlau1997-wq,
  title    = "Computer experiments and global optimization",
  author   = "Schonlau, M",
  year     =  1997,
  school   = "University of Waterloo",
  keywords = "BayesOpt;BayesOpt/Acquisition/InfillFxns"
}

@MISC{Gretton2015-ei,
  title       = "Introduction to {RKHS}, and some simple kernel algorithms",
  author      = "Gretton, Arthur",
  pages       = "30",
  institution = "University College London",
  year        =  2015,
  keywords    = "CurrentStudy"
}

@BOOK{Paiva2000-tj,
  title     = "Affective Interactions: Towards a New Generation of Computer
               Interfaces",
  author    = "Paiva, Ana",
  abstract  = "Affective computing is a fascinating new area of research
               emerging in computer science. It dwells on problems where
               ``computing is related to, arises from or deliberately
               influences emotions'' (Picard 1997). Following this new research
               direction and considering the human element as crucial in
               designing and implementing interactive intelligent interfaces,
               affective computing is now influencing the way we shape, design,
               construct, and evaluate human-computer interaction and
               computer-mediated communcation. This book originates from a
               workshop devoted to affective interactions. It presents revised
               full versions of several papers accepted in preliminary version
               for the workshop and various selectively solicited papers by key
               people as well as an introductory survey by the volume editor
               and interview with Rosaling Picard, a pioneer researcher in the
               field. The book competently assesses the state of the art in
               this fascinating new field.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "270",
  month     =  dec,
  year      =  2000,
  keywords  = "Reading List;COHRINT Reading List -- Brett",
  language  = "en"
}

@INPROCEEDINGS{Ribeiro2016-uc,
  title     = "``Why Should {I} Trust You?'': Explaining the Predictions of Any
               Classifier",
  booktitle = "Proceedings of the 22nd {ACM} {SIGKDD} international conference
               on knowledge discovery and data mining",
  author    = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
  abstract  = "Despite widespread adoption, machine learning models remain
               mostly black boxes. Understanding the reasons behind predictions
               is, however, quite important in assessing trust, which is
               fundamental if one plans to take action based on a prediction,
               or when choosing whether to deploy a new model. Such
               understanding also provides insights into the model, which can
               be used to transform an untrustworthy model or prediction into a
               trustworthy one. In this work, we propose LIME, a novel
               explanation technique that explains the predictions of any
               classifier in an interpretable and faithful manner, by learning
               an interpretable model locally around the prediction. We also
               propose a method to explain models by presenting representative
               individual predictions and their explanations in a non-redundant
               way, framing the task as a submodular optimization problem. We
               demonstrate the flexibility of these methods by explaining
               different models for text (e.g. random forests) and image
               classification (e.g. neural networks). We show the utility of
               explanations via novel experiments, both simulated and with
               human subjects, on various scenarios that require trust:
               deciding if one should trust a prediction, choosing between
               models, improving an untrustworthy classifier, and identifying
               why a classifier should not be trusted.",
  publisher = "ACM",
  pages     = "1135--1144",
  month     =  feb,
  year      =  2016,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Reduce
               Complexity;Supplemental Assurance;Assurances"
}

@ARTICLE{Ribeiro2016-hv,
  title         = "{Model-Agnostic} Interpretability of Machine Learning",
  author        = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
  abstract      = "Understanding why machine learning models behave the way
                   they do empowers both system designers and end-users in many
                   ways: in model selection, feature engineering, in order to
                   trust and act upon the predictions, and in more intuitive
                   user interfaces. Thus, interpretability has become a vital
                   concern in machine learning, and work in the area of
                   interpretable models has found renewed interest. In some
                   applications, such models are as accurate as
                   non-interpretable ones, and thus are preferred for their
                   transparency. Even when they are not accurate, they may
                   still be preferred when interpretability is of paramount
                   importance. However, restricting machine learning to
                   interpretable models is often a severe limitation. In this
                   paper we argue for explaining machine learning predictions
                   using model-agnostic approaches. By treating the machine
                   learning models as black-box functions, these approaches
                   provide crucial flexibility in the choice of models,
                   explanations, and representations, improving debugging,
                   comparison, and interfaces for a variety of users and
                   models. We also outline the main challenges for such
                   methods, and review a recently-introduced model-agnostic
                   explanation approach (LIME) that addresses these challenges.",
  month         =  jun,
  year          =  2016,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.05386"
}

@ARTICLE{Lipton2016-ug,
  title         = "The Mythos of Model Interpretability",
  author        = "Lipton, Zachary C",
  abstract      = "Supervised machine learning models boast remarkable
                   predictive capabilities. But can you trust your model? Will
                   it work in deployment? What else can it tell you about the
                   world? We want models to be not only good, but
                   interpretable. And yet the task of interpretation appears
                   underspecified. Papers provide diverse and sometimes
                   non-overlapping motivations for interpretability, and offer
                   myriad notions of what attributes render models
                   interpretable. Despite this ambiguity, many papers proclaim
                   interpretability axiomatically, absent further explanation.
                   In this paper, we seek to refine the discourse on
                   interpretability. First, we examine the motivations
                   underlying interest in interpretability, finding them to be
                   diverse and occasionally discordant. Then, we address model
                   properties and techniques thought to confer
                   interpretability, identifying transparency to humans and
                   post-hoc explanations as competing notions. Throughout, we
                   discuss the feasibility and desirability of different
                   notions, and question the oft-made assertions that linear
                   models are interpretable and that deep neural networks are
                   not.",
  month         =  jun,
  year          =  2016,
  keywords      = "
                   assurance\_predictability;supervised\_learning;assurance\_competence;assurance\_normality;trust\_informal\_treatment;assurance\_explicit;Integral
                   Assurance;interp\_models;Interpretable Models;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.03490"
}

@ARTICLE{Montavon2015-mv,
  title         = "Explaining {NonLinear} Classification Decisions with Deep
                   Taylor Decomposition",
  author        = "Montavon, Gr{\'e}goire and Bach, Sebastian and Binder,
                   Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert",
  abstract      = "Nonlinear methods such as Deep Neural Networks (DNNs) are
                   the gold standard for various challenging machine learning
                   problems, e.g., image classification, natural language
                   processing or human action recognition. Although these
                   methods perform impressively well, they have a significant
                   disadvantage, the lack of transparency, limiting the
                   interpretability of the solution and thus the scope of
                   application in practice. Especially DNNs act as black boxes
                   due to their multilayer nonlinear structure. In this paper
                   we introduce a novel methodology for interpreting generic
                   multilayer neural networks by decomposing the network
                   classification decision into contributions of its input
                   elements. Although our focus is on image classification, the
                   method is applicable to a broad set of input data, learning
                   tasks and network architectures. Our method is based on deep
                   Taylor decomposition and efficiently utilizes the structure
                   of the network by backpropagating the explanations from the
                   output to the input layer. We evaluate the proposed method
                   empirically on the MNIST and ILSVRC data sets.",
  month         =  dec,
  year          =  2015,
  keywords      = "trust\_informal\_treatment;assurance\_explicit;classification;explain;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1512.02479"
}

@ARTICLE{Abdollahi2016-vn,
  title         = "Explainable Restricted Boltzmann Machines for Collaborative
                   Filtering",
  author        = "Abdollahi, Behnoush and Nasraoui, Olfa",
  abstract      = "Most accurate recommender systems are black-box models,
                   hiding the reasoning behind their recommendations. Yet
                   explanations have been shown to increase the user's trust in
                   the system in addition to providing other benefits such as
                   scrutability, meaning the ability to verify the validity of
                   recommendations. This gap between accuracy and transparency
                   or explainability has generated an interest in automated
                   explanation generation methods. Restricted Boltzmann
                   Machines (RBM) are accurate models for CF that also lack
                   interpretability. In this paper, we focus on RBM based
                   collaborative filtering recommendations, and further assume
                   the absence of any additional data source, such as item
                   content or user attributes. We thus propose a new
                   Explainable RBM technique that computes the top-n
                   recommendation list from items that are explainable.
                   Experimental results show that our method is effective in
                   generating accurate and explainable recommendations.",
  month         =  jun,
  year          =  2016,
  keywords      = "trust\_informal\_treatment;assurance\_explicit;interp\_models;Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.07129"
}

@ARTICLE{Kamruzzaman2010-vz,
  title         = "Extraction of Symbolic Rules from Artificial Neural Networks",
  author        = "Kamruzzaman, S M and Islam, Md Monirul",
  abstract      = "Although backpropagation ANNs generally predict better than
                   decision trees do for pattern classification problems, they
                   are often regarded as black boxes, i.e., their predictions
                   cannot be explained as those of decision trees. In many
                   applications, it is desirable to extract knowledge from
                   trained ANNs for the users to gain a better understanding of
                   how the networks solve the problems. A new rule extraction
                   algorithm, called rule extraction from artificial neural
                   networks (REANN) is proposed and implemented to extract
                   symbolic rules from ANNs. A standard three-layer feedforward
                   ANN is the basis of the algorithm. A four-phase training
                   algorithm is proposed for backpropagation learning.
                   Explicitness of the extracted rules is supported by
                   comparing them to the symbolic rules generated by other
                   methods. Extracted rules are comparable with other methods
                   in terms of number of rules, average number of conditions
                   for a rule, and predictive accuracy. Extensive experimental
                   studies on several benchmarks classification problems, such
                   as breast cancer, iris, diabetes, and season classification
                   problems, demonstrate the effectiveness of the proposed
                   approach with good generalization ability.",
  month         =  sep,
  year          =  2010,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1009.4570"
}

@ARTICLE{Adler2016-yt,
  title         = "Auditing Black-box Models for Indirect Influence",
  author        = "Adler, Philip and Falk, Casey and Friedler, Sorelle A and
                   Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon
                   and Venkatasubramanian, Suresh",
  abstract      = "Data-trained predictive models see widespread use, but for
                   the most part they are used as black boxes which output a
                   prediction or score. It is therefore hard to acquire a
                   deeper understanding of model behavior, and in particular
                   how different features influence the model prediction. This
                   is important when interpreting the behavior of complex
                   models, or asserting that certain problematic attributes
                   (like race or gender) are not unduly influencing decisions.
                   In this paper, we present a technique for auditing black-box
                   models, which lets us study the extent to which existing
                   models take advantage of particular features in the dataset,
                   without knowing how the models work. Our work focuses on the
                   problem of indirect influence: how some features might
                   indirectly influence outcomes via other, related features.
                   As a result, we can find attribute influences even in cases
                   where, upon further direct examination of the model, the
                   attribute is not referred to by the model at all. Our
                   approach does not require the black-box model to be
                   retrained. This is important if (for example) the model is
                   only accessible via an API, and contrasts our work with
                   other methods that investigate feature influence like
                   feature selection. We present experimental evidence for the
                   effectiveness of our procedure using a variety of publicly
                   available datasets and models. We also validate our
                   procedure using techniques from interpretable learning and
                   feature selection, as well as against other black-box
                   auditing procedures.",
  month         =  feb,
  year          =  2016,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1602.07043"
}

@ARTICLE{Dundas2011-us,
  title         = "Implementing Human-like Intuition Mechanism in Artificial
                   Intelligence",
  author        = "Dundas, Jitesh and Chik, David",
  abstract      = "Human intuition has been simulated by several research
                   projects using artificial intelligence techniques. Most of
                   these algorithms or models lack the ability to handle
                   complications or diversions. Moreover, they also do not
                   explain the factors influencing intuition and the accuracy
                   of the results from this process. In this paper, we present
                   a simple series based model for implementation of human-like
                   intuition using the principles of connectivity and unknown
                   entities. By using Poker hand datasets and Car evaluation
                   datasets, we compare the performance of some well-known
                   models with our intuition model. The aim of the experiment
                   was to predict the maximum accurate answers using intuition
                   based models. We found that the presence of unknown
                   entities, diversion from the current problem scenario, and
                   identifying weakness without the normal logic based
                   execution, greatly affects the reliability of the answers.
                   Generally, the intuition based models cannot be a substitute
                   for the logic based mechanisms in handling such problems.
                   The intuition can only act as a support for an ongoing logic
                   based model that processes all the steps in a sequential
                   manner. However, when time and computational cost are very
                   strict constraints, this intuition based model becomes
                   extremely important and useful, because it can give a
                   reasonably good performance. Factors affecting intuition are
                   analyzed and interpreted through our model.",
  month         =  jun,
  year          =  2011,
  keywords      = "Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1106.5917"
}

@ARTICLE{Lake2016-wb,
  title    = "Building Machines That Learn and Think Like People",
  author   = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and
              Gershman, Samuel J",
  abstract = "Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people.
              Many advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats
              humans in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we
              argue that these machines should (a) build causal models of the
              world that support explanation and understanding, rather than
              merely solving pattern recognition problems; (b) ground learning
              in intuitive theories of physics and psychology, to support and
              enrich the knowledge that is learned; and (c) harness
              compositionality and learning-to-learn to rapidly acquire and
              generalize knowledge to new tasks and situations. We suggest
              concrete challenges and promising routes towards these goals that
              can combine the strengths of recent neural network advances with
              more structured cognitive models.",
  journal  = "Behav. Brain Sci.",
  pages    = "1--101",
  month    =  nov,
  year     =  2016,
  keywords = "Reading List;COHRINT Reading List -- Brett",
  language = "en"
}

@ARTICLE{Tang2016-yp,
  title         = "Visualizing Large-scale and High-dimensional Data",
  author        = "Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei,
                   Qiaozhu",
  abstract      = "We study the problem of visualizing large-scale and
                   high-dimensional data in a low-dimensional (typically 2D or
                   3D) space. Much success has been reported recently by
                   techniques that first compute a similarity structure of the
                   data points and then project them into a low-dimensional
                   space with the structure preserved. These two steps suffer
                   from considerable computational costs, preventing the
                   state-of-the-art methods such as the t-SNE from scaling to
                   large-scale and high-dimensional data (e.g., millions of
                   data points and hundreds of dimensions). We propose the
                   LargeVis, a technique that first constructs an accurately
                   approximated K-nearest neighbor graph from the data and then
                   layouts the graph in the low-dimensional space. Comparing to
                   t-SNE, LargeVis significantly reduces the computational cost
                   of the graph construction step and employs a principled
                   probabilistic model for the visualization step, the
                   objective of which can be effectively optimized through
                   asynchronous stochastic gradient descent with a linear time
                   complexity. The whole procedure thus easily scales to
                   millions of high-dimensional data points. Experimental
                   results on real-world data sets demonstrate that the
                   LargeVis outperforms the state-of-the-art methods in both
                   efficiency and effectiveness. The hyper-parameters of
                   LargeVis are also much more stable over different data sets.",
  month         =  feb,
  year          =  2016,
  keywords      = "Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.00370"
}

@ARTICLE{Dzindolet2003-ts,
  title    = "The role of trust in automation reliance",
  author   = "Dzindolet, Mary T and Peterson, Scott A and Pomranky, Regina A
              and Pierce, Linda G and Beck, Hall P",
  abstract = "A recent and dramatic increase in the use of automation has not
              yielded comparable improvements in performance. Researchers have
              found human operators often underutilize (disuse) and overly rely
              on (misuse) automated aids (Parasuraman and Riley, 1997). Three
              studies were performed with Cameron University students to
              explore the relationship among automation reliability, trust, and
              reliance. With the assistance of an automated decision aid,
              participants viewed slides of Fort Sill terrain and indicated the
              presence or absence of a camouflaged soldier. Results from the
              three studies indicate that trust is an important factor in
              understanding automation reliance decisions. Participants
              initially considered the automated decision aid trustworthy and
              reliable. After observing the automated aid make errors,
              participants distrusted even reliable aids, unless an explanation
              was provided regarding why the aid might err. Knowing why the aid
              might err increased trust in the decision aid and increased
              automation reliance, even when the trust was unwarranted. Our
              studies suggest a need for future research focused on
              understanding automation use, examining individual differences in
              automation reliance, and developing valid and reliable
              self-report measures of trust in automation.",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  58,
  number   =  6,
  pages    = "697--718",
  year     =  2003,
  keywords = "Automation trust; Automation reliance; Misuse;
              Disuse;automation;human\_study;trust\_formal\_treatment;Assurances"
}

@ARTICLE{Baehrens2010-bs,
  title    = "How to Explain Individual Classification Decisions",
  author   = "Baehrens, David and Schroeter, Timon and Harmeling, Stefan and
              Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  number   = "Jun",
  pages    = "1803--1831",
  year     =  2010,
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Trumbelj2010-sr,
  title    = "An Efficient Explanation of Individual Classifications using Game
              Theory",
  author   = "Trumbelj, Erik {\AA} and Kononenko, Igor",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  number   = "Jan",
  pages    = "1--18",
  year     =  2010,
  keywords = "Reading List;COHRINT Reading List -- Brett"
}

@ARTICLE{Freitas2014-xf,
  title     = "Comprehensible classification models: a position paper",
  author    = "Freitas, Alex A",
  journal   = "ACM SIGKDD Explorations Newsletter",
  publisher = "ACM",
  volume    =  15,
  number    =  1,
  pages     = "1--10",
  month     =  mar,
  year      =  2014,
  keywords  = "Bayesian network classifiers; decision table; decision tree;
               monotonicity constraint; nearest neighbors; rule
               induction;Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Thomaz2005-qj,
  title     = "{Real-Time} Interactive Reinforcement Learning for Robots",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Thomaz, Andrea Lockerd and Hoffman, Guy and Breazeal, Cynthia",
  abstract  = "It is our goal to understand the role real-time human
               in-teraction can play in machine learning algorithms for robots.
               In this paper we present Interactive Reinforce-ment Learning
               (IRL) as a plausible approach for train-ing human-centric
               assistive robots by natural interac-tion. We describe an
               experimental platform to study IRL, pose questions arising from
               IRL, and discuss ini-tial observations obtained during the
               development of our system.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Stumpf2005-tx,
  title     = "Predicting User Tasks: {I} Know What You're Doing!",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Stumpf, Simone and Bao, Xinlong and Dragunov, Anton and
               Dietterich, Thomas G and Herlocker, Jon and Johnsrude, Kevin and
               Li, Lida and Shen, Jianqiang",
  abstract  = "Knowledge workers spend the majority of their working hours
               processing and manipulating information. These users face
               continual costs as they switch between tasks to retrieve and
               create information. The TaskTracer project at Oregon State
               University is investigating the possibilities of a desktop
               software system that will record in detail how knowledge workers
               complete tasks, and intelligently leverage that information to
               increase efficiency and productivity. Our approach combines
               human-computer interaction and machine learning to assign each
               observed action (opening a file, saving a file, sending an
               email, cutting and pasting information, etc.) to a task for
               which it is likely being performed. In this paper we report on
               ways we have applied machine learning in this environment and
               lessons learned so far.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Vasile2005-ie,
  title     = "{TRIPPER}: Rule learning using taxonomies",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Vasile, Flavian and Silvescu, Adrian and Kang, Dae-Ki and
               Honavar, Vasant",
  abstract  = "In many application domains, there is a need for learning
               algorithms that generate accurate as well as comprehensible
               classifiers. In this paper, we present TRIPPER -a rule induction
               algorithm that extends RIPPER, a widely used rule-learning
               algorithm. TRIPPER exploits background knowledge in the form of
               taxonomies over values of features used to describe data. We
               compare the performance of TRIPPER with that of RIPPER on a text
               classification problem (using the Reuters 21578 dataset).
               Experiments were performed using WordNet (a human-generated
               taxonomy), as well as a taxonomy generated by WTL (Word Taxonomy
               Learning) algorithm. Our experiments show that the rules
               generated by TRIPPER are generally more accurate and more
               concise (and hence more comprehensible) than those generated by
               RIPPER.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Burge2005-uz,
  title     = "Comprehensibility of Generative vs. Class Discriminative Dynamic
               Bayesian Multinets",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Burge, John and Lane, Terran",
  abstract  = "We investigate the comprehensibility of dynamic Bayesian
               multinets (DBMs) and the dynamic Bayesian networks (DBNs) that
               compose them. Specifically, we compare the DBM structures
               resulting from searches employing generative and class
               discriminative scoring functions. The DBMs are used to model the
               temporal relationships among RVs and show how the relationships
               change between different classes of data. We apply our technique
               to the identification of dynamic relationships among
               neuro-anatomical regions of interest in both healthy and
               demented elderly patients based on functional magnetic resonance
               imaging (fMRI) data. The structures resulting from both
               generative and class discriminative scores were found to be
               useful by our collaborating neuroscientist, but for differing
               reasons. For example, generative scores result in structures
               that illuminate highly likely relationships and are more easily
               interpreted. Conversely, structures resulting from class
               discriminating scores are capable of representing more subtle
               changes and can illuminate important behavioral differences not
               apparent from structures learned from generative scores.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Datta2016-we,
  title     = "Algorithmic Transparency via Quantitative Input Influence",
  booktitle = "Proceedings of the 37th {IEEE} Symposium on Security and Privacy",
  author    = "Datta, A and Sen, S and Zick, Y",
  year      =  2016,
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{El-Arini2012-by,
  title     = "Transparent user models for personalization",
  booktitle = "Proceedings of the 18th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "El-Arini, Khalid and Paquet, Ulrich and Herbrich, Ralf and Van
               Gael, Jurgen and Ag{\"u}era y Arcas, Blaise",
  publisher = "ACM",
  pages     = "678--686",
  month     =  aug,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "Twitter; graphical models; personalization; transparency;Reading
               List;COHRINT Reading List -- Brett"
}

@INPROCEEDINGS{Sinha2002-px,
  title     = "The Role of Transparency in Recommender Systems",
  booktitle = "{CHI'02} extended abstracts on Human factors in computing
               systems. {ACM}",
  author    = "Sinha, R and Swearingen, K",
  year      =  2002,
  keywords  = "Reading List;COHRINT Reading List -- Brett"
}

@UNPUBLISHED{Dewey_undated-kg,
  title    = "Learning What to Value",
  author   = "Dewey, Daniel",
  abstract = "We examine ultraintelligent reinforcement learning agents.
              Reinforcement learning can only be used in the real world to
              define agents whose goal is to maximize expected rewards, and
              since this goal does not match with human goals, AGIs based on
              reinforcement learning will often work at crosspurposes to us. We
              define value learners, agents that can be designed to learn and
              maximize any initially unknown utility function so long as we
              provide them with an idea of what constitutes evidence about that
              utility function. 1 Agents and Implementations Traditional agents
              [2, 3] interact with their environments cyclically: in cycle k,
              an agent acts with action y k , then perceives observation x k .
              The interaction history of an agent with lifespan m is a string y
              1 x 1 y 2 x 2 ...y m x m , also written yx 1:m or yx $\leq$m .
              Beyond these interactions, a traditional agent is isolated from
              its environment, so an agent can be formalized as an agent
              function from an interaction history yx <k to an action y k .
              Since we are concerned not with agents in the abstract, but with
              very powerful agents in the real world, we introduce the concept
              of an agent implementation. An agent implementation is a physical
              structure that, in the absence of interference from its
              environment, implements an agent function. In cycle k, an
              unaltered agent implementation executes its agent function on its
              recalled interaction history yx <k , sends the resulting y k into
              the environment as output, then receives and records an
              observation x k . An agent implementation's behavior is only
              guaranteed to match its implemented function so long as effects
              from the environment do not destroy the agent or alter its
              functionality. In keeping with this realism, an agent
              implementation's environment is considered to be the real world
              in which we live. We may engineer some parts of the world to meet
              our specifications, but (breaking with some traditional agent
              formulations) we do not consider the environment to be completely
              under our control, to be defined as we wish. Why would one want
              to study agent implementations? For narrowlyintelligent agents,
              the distinction between traditional agents and agent
              implementations may not be worth making. For ultraintelligent
              agents, the distinction is quite important: agent implementations
              offer us better predictions about how powerful agents will affect
              their environments and their own machinery, and are the basis for
              understanding realworld agents that model, defend, maintain, and
              improve themselves.",
  journal  = "MIRI",
  keywords = "MIRI;Assurances/Self-Aware"
}

@ARTICLE{Bostrom2012-uf,
  title    = "{THE} {SUPERINTELLIGENT} {WILL}: {MOTIVATION} {AND}
              {INSTRUMENTAL} {RATIONALITY} {IN} {ADVANCED} {ARTIFICIAL}
              {AGENTS}",
  author   = "Bostrom, Nick",
  abstract = "www.nickbostrom.com [Forthcoming in Minds and Machines, 2012]
              ABSTRACT This paper discusses the relation between intelligence
              and motivation in artificial agents, developing and briefly
              arguing for two theses. The first, the orthogonality thesis,
              holds (with some caveats) that intelligence and final goals
              (purposes) are orthogonal axes along which possible artificial
              intellects can freely vary---more or less any level of
              intelligence could be combined with more or less any final goal.
              The second, the instrumental convergence thesis, holds that as
              long as they possess a sufficient level of intelligence, agents
              having any of a wide range of final goals will pursue similar
              intermediary goals because they have instrumental reasons to do
              so. In combination, the two theses help us understand the
              possible range of behavior of superintelligent agents, and they
              point to some potential dangers in building such an agent.",
  journal  = "Minds Mach.",
  volume   =  22,
  number   =  2,
  pages    = "7185",
  year     =  2012,
  keywords = "MIRI;Assurances/Self-Aware"
}

@BOOK{Yudkowsky_undated-ni,
  title    = "Rationality: From {AI} to Zombies",
  author   = "Yudkowsky, Eliezer",
  keywords = "Assurances/Self-Aware"
}

@INPROCEEDINGS{Thornton2013-ll,
  title     = "{Auto-WEKA}: combined selection and hyperparameter optimization
               of classification algorithms",
  booktitle = "Proceedings of the 19th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Thornton, Chris and Hutter, Frank and Hoos, Holger H and
               Leyton-Brown, Kevin",
  abstract  = "Many different machine learning algorithms exist; taking into
               account each algorithm's hyperparameters, there is a
               staggeringly large number of possible alternatives overall. We
               consider the problem of simultaneously selecting a learning
               algorithm and setting its hyperparameters, going beyond previous
               work that attacks these issues separately. We show that this
               problem can be addressed by a fully automated approach,
               leveraging recent innovations in Bayesian optimization.
               Specifically, we consider a wide range of feature selection
               techniques (combining 3 search and 8 evaluator methods) and all
               classification approaches implemented in WEKA's standard
               distribution, spanning 2 ensemble methods, 10 meta-methods, 27
               base classifiers, and hyperparameter settings for each
               classifier. On each of 21 popular datasets from the UCI
               repository, the KDD Cup 09, variants of the MNIST dataset and
               CIFAR-10, we show classification performance often much better
               than using standard selection and hyperparameter optimization
               methods. We hope that our approach will help non-expert users to
               more effectively identify machine learning algorithms and
               hyperparameter settings appropriate to their applications, and
               hence to achieve improved performance.",
  publisher = "ACM",
  pages     = "847--855",
  month     =  aug,
  year      =  2013,
  address   = "New York, New York, USA",
  keywords  = "hyperparameter optimization; model selection; weka;Reading
               List;COHRINT Reading List -- Brett"
}

@ARTICLE{Wang2016-ln,
  title         = "Parallel Bayesian Global Optimization of Expensive Functions",
  author        = "Wang, Jialei and Clark, Scott C and Liu, Eric and Frazier,
                   Peter I",
  abstract      = "We consider parallel global optimization of derivative-free
                   expensive-to-evaluate functions, and propose an efficient
                   method based on stochastic approximation for implementing a
                   conceptual Bayesian optimization algorithm proposed by
                   Ginsbourger et al. (2007). To accomplish this, we use
                   infinitessimal perturbation analysis (IPA) to construct a
                   stochastic gradient estimator and show that this estimator
                   is unbiased. We also show that the stochastic gradient
                   ascent algorithm using the constructed gradient estimator
                   converges to a stationary point of the q-EI surface, and
                   therefore, as the number of multiple starts of the gradient
                   ascent algorithm and the number of steps for each start grow
                   large, the one-step Bayes optimal set of points is
                   recovered. We show in numerical experiments that our method
                   for maximizing the q-EI is faster than methods based on
                   closed-form evaluation using high-dimensional integration,
                   when considering many parallel function evaluations, and is
                   comparable in speed when considering few. We also show that
                   the resulting one-step Bayes optimal algorithm for parallel
                   global optimization finds high quality solutions with fewer
                   evaluations that a heuristic based on approximately
                   maximizing the q-EI. A high quality open source
                   implementation of this algorithm is available in the open
                   source Metrics Optimization Engine (MOE).",
  month         =  feb,
  year          =  2016,
  keywords      = "Acquisition/InfillFxns;BayesOpt;TALAF;batch
                   selection;qEI;BayesOpt/Acquisition/InfillFxns",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1602.05149"
}

@MISC{Johnson2014-xr,
  title    = "The {NLopt} nonlinear-optimization package",
  author   = "Johnson, Steven G",
  year     =  2014,
  keywords = "AFRL\_STTR"
}

@ARTICLE{Wang2016-ph,
  title  = "{Trust-Based} Symbolic Robot Motion Planning with
            {Human-in-the-Loop}",
  author = "Wang, Yue",
  pages  = "2015--2016",
  year   =  2016
}

@ARTICLE{Junges_undated-lv,
  title  = "Probabilistic Verification for Cognitive Models",
  author = "Junges, Sebastian and Jansen, Nils and Katoen, Joost-Pieter and
            Topcu, Ufuk",
  pages  = "1--5"
}

@MISC{Swersky2014-sq,
  title    = "{FreezeThaw} Bayesian Optimization",
  author   = "Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott",
  abstract = "In this paper we develop a dynamic form of Bayesian optimization
              for machine learning models with the goal of rapidly finding good
              hyperparameter settings. Our method uses the partial information
              gained during the training of a machine learning model in order
              to decide whether to pause training and start a new model, or
              resume the training of a previouslyconsidered model. We
              specifically tailor our method to machine learning problems by
              developing a novel positivedefinite covariance kernel to capture
              a variety of training curves. Furthermore, we develop a Gaussian
              process prior that scales gracefully with additional temporal
              observations. Finally, we provide an informationtheoretic
              framework to automate the decision process. Experiments on
              several common machine learning models show that our approach is
              extremely effective in practice.",
  year     =  2014,
  keywords = "BayesOpt;BayesOpt"
}

@ARTICLE{Jaderberg_undated-fr,
  title    = "Decoupled Neural Interfaces using Synthetic Gradients",
  author   = "Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon
              and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray",
  abstract = "Training directed neural networks typically requires
              forwardpropagating data through a computation graph, followed by
              backpropagating error signal, to produce weight updates. All
              layers, or more generally, modules, of the network are therefore
              locked, in the sense that they must wait for the remainder of the
              network to execute forwards and propagate error backwards before
              they can be updated. In this work we break this constraint by
              decoupling modules by introducing a model of the future
              computation of the network graph. These models predict what the
              result of the modelled subgraph will produce using only local
              information. In particular we focus on modelling error gradients:
              by using the modelled synthetic gradient in place of true
              backpropagated error gradients we decouple subgraphs, and can
              update them independently and asynchronously i.e. we realise
              decoupled neural interfaces. We show results for feedforward
              models, where every layer is trained asynchronously, recurrent
              neural networks (RNNs) where predicting one's future gradient
              extends the time over which the RNN can effectively model, and
              also a hierarchical RNN system with ticking at different
              timescales. Finally, we demonstrate that in addition to
              predicting gradients, the same framework can be used to predict
              inputs, resulting in models which are decoupled in both the
              forward and backwards pass -- amounting to independent networks
              which colearn such that they can be composed into a single
              functioning corporation.",
  keywords = "neural\_networks;MLTheory/DeepLearning"
}

@ARTICLE{Picheny2013-mp,
  title     = "A Nonstationary {Space-Time} Gaussian Process Model for
               Partially Converged Simulations",
  author    = "Picheny, Victor and Ginsbourger, David",
  abstract  = "In the context of expensive numerical experiments, a promising
               solution for alleviating the computational costs consists of
               using partially converged simulations instead of exact
               solutions. The gain in computational time is at the price of
               precision in the response. This work addresses the issue of
               fitting a Gaussian process model to partially converged
               simulation data for further use in prediction. The main
               challenge consists of the adequate approximation of the error
               due to partial convergence, which is correlated in both design
               variables and time directions. Here, we propose fitting a
               Gaussian process in the joint space of design parameters and
               computational time. The model is constructed by building a
               nonstationary covariance kernel that reflects accurately the
               actual structure of the error. Practical solutions are proposed
               for solving parameter estimation issues associated with the
               proposed model. The method is applied to a computational fluid
               dynamics test case and shows significant improvement in
               prediction compared to a classical kriging model.",
  journal   = "SIAM/ASA Journal on Uncertainty Quantification",
  publisher = "American Statistical Association",
  volume    =  1,
  number    =  1,
  pages     = "57--78",
  year      =  2013,
  keywords  = "GPs"
}

@ARTICLE{Dietterich2015-qz,
  title     = "Rise of concerns about {AI}: reflections and directions",
  author    = "Dietterich, Thomas G and Horvitz, Eric J",
  abstract  = "Research, leadership, and communication about AI futures. lives,
               including those lost to accidents on our roadways and to errors
               made in medicine. Over the longer-term, advances in machine
               intelligence will have deeply beneficial influences on
               healthcare, education, transportation, commerce, and the overall
               march of science. Beyond the creation of new applications and
               services, the pursuit of insights about the computational D
               ISCUSSIONS ABOUT ARTIFI-CIAL intelligence (AI) have jumped into
               the public eye over the past year, with sev-eral luminaries
               speaking about the threat of AI to the future of humanity. Over
               the last several de-cades, AI---automated perception, learning,
               reasoning, and decision making---has become commonplace in our
               lives. We plan trips using GPS systems that rely on the A*
               algorithm to optimize the route. Our smartphones understand our
               speech, and Siri, Cor-tana, and Google Now are getting bet-ter
               at understanding our intentions. Machine vision detects faces as
               we take pictures with our phones and recogniz-es the faces of
               individual people when we post those pictures to Facebook.
               Internet search engines rely on a fabric of AI subsystems. On
               any day, AI pro-vides hundreds of millions of people with search
               results, traffic predictions, and recommendations about books
               and movies. AI translates among lan-guages in real time and
               speeds up the operation of our laptops by guessing what we will
               do next. Several compa-nies are working on cars that can drive
               themselves---either with partial hu-man oversight or entirely
               autonomous-ly. Beyond the influences in our daily lives, AI
               techniques are playing roles in science and medicine. AI is
               already at work in some hospitals helping physi-cians understand
               which patients are at highest risk for complications, and AI
               algorithms are finding important nee-dles in massive data
               haystacks, such as identifying rare but devastating side
               ef-fects of medications. The AI in our lives today provides a
               small glimpse of more profound con-tributions to come. For
               example, the fielding of currently available technol-ogies could
               save many thousands of",
  journal   = "Commun. ACM",
  publisher = "ACM",
  volume    =  58,
  number    =  10,
  pages     = "38--40",
  month     =  sep,
  year      =  2015,
  keywords  = "Assurances"
}

@ARTICLE{Daftry2016-hi,
  title         = "Introspective Perception: Learning to Predict Failures in
                   Vision Systems",
  author        = "Daftry, Shreyansh and Zeng, Sam and Andrew Bagnell, J and
                   Hebert, Martial",
  abstract      = "As robots aspire for long-term autonomous operations in
                   complex dynamic environments, the ability to reliably take
                   mission-critical decisions in ambiguous situations becomes
                   critical. This motivates the need to build systems that have
                   situational awareness to assess how qualified they are at
                   that moment to make a decision. We call this self-evaluating
                   capability as introspection. In this paper, we take a small
                   step in this direction and propose a generic framework for
                   introspective behavior in perception systems. Our goal is to
                   learn a model to reliably predict failures in a given
                   system, with respect to a task, directly from input sensor
                   data. We present this in the context of vision-based
                   autonomous MAV flight in outdoor natural environments, and
                   show that it effectively handles uncertain situations.",
  month         =  jul,
  year          =  2016,
  keywords      = "introspection;Assurances/Self-Aware",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1607.08665"
}

@ARTICLE{Ghavamzadeh2016-xh,
  title         = "Bayesian Reinforcement Learning: A Survey",
  author        = "Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle
                   and Tamar, Aviv",
  abstract      = "Bayesian methods for machine learning have been widely
                   investigated, yielding principled methods for incorporating
                   prior information into inference algorithms. In this survey,
                   we provide an in-depth review of the role of Bayesian
                   methods for the reinforcement learning (RL) paradigm. The
                   major incentives for incorporating Bayesian reasoning in RL
                   are: 1) it provides an elegant approach to action-selection
                   (exploration/exploitation) as a function of the uncertainty
                   in learning; and 2) it provides a machinery to incorporate
                   prior knowledge into the algorithms. We first discuss models
                   and methods for Bayesian inference in the simple single-step
                   Bandit model. We then review the extensive recent literature
                   on Bayesian methods for model-based RL, where prior
                   information can be expressed on the parameters of the Markov
                   model. We also present Bayesian methods for model-free RL,
                   where priors are expressed over the value function or policy
                   class. The objective of the paper is to provide a
                   comprehensive survey on Bayesian RL algorithms and their
                   theoretical and empirical properties.",
  month         =  sep,
  year          =  2016,
  keywords      = "ReinforcementLearning",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1609.04436"
}

@INPROCEEDINGS{Bergstra2011-ap,
  title     = "Algorithms for {Hyper-Parameter} Optimization",
  booktitle = "{NIPS} Proceedings",
  author    = "Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and
               K{\'e}gl, Bal{\'a}zs",
  abstract  = "Several recent advances to the state of the art in image
               classification benchmarks have come from better configurations
               of existing techniques rather than novel approaches to feature
               learning. Traditionally, hyper-parameter optimization has been
               the job of humans because they can be very efficient in regimes
               where only a few trials are possible. Presently, computer
               clusters and GPU processors make it possible to run more trials
               and we show that algorithmic approaches can find better results.
               We present hyper-parameter optimization results on tasks of
               training neural networks and deep belief networks (DBNs). We
               optimize hyper-parameters using random search and two new greedy
               sequential methods based on the expected improvement criterion.
               Random search has been shown to be sufficiently efficient for
               learning neural networks for several datasets, but we show it is
               unreliable for training DBNs. The sequential algorithms are
               applied to the most difficult DBN learning problems from
               [Larochelle et al., 2007] and find significantly better results
               than the best previously reported. This work contributes novel
               techniques for making response surface models P (y|x) in which
               many elements of hyper-parameter assignment (x) are known to be
               irrelevant given particular values of other elements.",
  pages     = "2546--2554",
  year      =  2011,
  keywords  = "OptimizingHyperparameters"
}

@ARTICLE{Marmin2016-xy,
  title         = "Efficient batch-sequential Bayesian optimization with
                   moments of truncated Gaussian vectors",
  author        = "Marmin, S{\'e}bastien and Chevalier, Cl{\'e}ment and
                   Ginsbourger, David",
  abstract      = "We deal with the efficient parallelization of Bayesian
                   global optimization algorithms, and more specifically of
                   those based on the expected improvement criterion and its
                   variants. A closed form formula relying on multivariate
                   Gaussian cumulative distribution functions is established
                   for a generalized version of the multipoint expected
                   improvement criterion. In turn, the latter relies on
                   intermediate results that could be of independent interest
                   concerning moments of truncated Gaussian vectors. The
                   obtained expansion of the criterion enables studying its
                   differentiability with respect to point batches and
                   calculating the corresponding gradient in closed form.
                   Furthermore , we derive fast numerical approximations of
                   this gradient and propose efficient batch optimization
                   strategies. Numerical experiments illustrate that the
                   proposed approaches enable computational savings of between
                   one and two order of magnitudes, hence enabling
                   derivative-based batch-sequential acquisition function
                   maximization to become a practically implementable and
                   efficient standard.",
  month         =  sep,
  year          =  2016,
  keywords      = "BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1609.02700"
}

@INPROCEEDINGS{Gonzalez2016-tt,
  title     = "{GLASSES}: Relieving The Myopia Of Bayesian Optimisation",
  booktitle = "Proceedings of the 19th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Gonzalez, Javier and Osborne, Michael and Lawrence, Neil",
  abstract  = "We present GLASSES: Global optimisation with Look-Ahead through
               Stochastic Simulation and Expected-loss Search. The majority of
               global optimisation approaches in use are myopic, in only
               considering the impact of the next function value; the
               non-myopic approaches that do exist are able to consider only a
               handful of future evaluations. Our novel algorithm, GLASSES,
               permits the consideration of dozens of evaluations into the
               future. This is done by approximating the ideal look-ahead loss
               function, which is expensive to evaluate, by a cheaper
               alternative in which the future steps of the algorithm are
               simulated beforehand. An Expectation Propagation algorithm is
               used to compute the expected value of the loss. We show that the
               far-horizon planning thus enabled leads to substantive
               performance gains in empirical tests.",
  pages     = "790--799",
  year      =  2016,
  keywords  = "BayesOpt"
}

@ARTICLE{Thompson1933-rd,
  title     = "On the Likelihood that One Unknown Probability Exceeds Another
               in View of the Evidence of Two Samples",
  author    = "Thompson, William R",
  journal   = "Biometrika",
  publisher = "[Oxford University Press, Biometrika Trust]",
  volume    =  25,
  number    = "3/4",
  pages     = "285--294",
  year      =  1933,
  keywords  = "BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Genz1992-qt,
  title    = "Numerical Computation of Multivariate Normal Probabilities",
  author   = "Genz, Alan",
  abstract = "Abstract The numerical computation of a multivariate normal
              probability is often a difficult problem. This article describes
              a transformation that simplifies the problem and places it into a
              form that allows efficient calculation using standard numerical
              multiple integration algorithms. Test results are presented that
              compare implementations of two algorithms that use the
              transformation with currently available software.",
  journal  = "J. Comput. Graph. Stat.",
  volume   =  1,
  number   =  2,
  pages    = "141--149",
  year     =  1992,
  keywords = "BayesOpt"
}

@INPROCEEDINGS{Library2016-wv,
  title     = "Fall 2016 {AAAI} Symposia",
  booktitle = "Proceedings of Fall 2016 {AAAI} Symposia",
  author    = "Library, Digital and Intelligence, Artificial",
  publisher = "AAAI Press",
  pages     = "364",
  year      =  2016
}

@ARTICLE{Charikar2016-za,
  title         = "Learning from Untrusted Data",
  author        = "Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory",
  abstract      = "The vast majority of theoretical results in machine learning
                   and statistics assume that the available training data is a
                   reasonably reliable reflection of the phenomena to be
                   learned or estimated. Similarly, the majority of machine
                   learning and statistical techniques used in practice are
                   brittle to the presence of large amounts of biased or
                   malicious data. In this work we consider two frameworks in
                   which to study estimation, learning, and optimization in the
                   presence of significant fractions of arbitrary data. The
                   first framework, list-decodable learning, asks whether it is
                   possible to return a list of answers, with the guarantee
                   that at least one of them is accurate. For example, given a
                   dataset of $n$ points for which an unknown subset of $\alpha
                   n$ points are drawn from a distribution of interest, and no
                   assumptions are made about the remaining $(1-\alpha)n$
                   points, is it possible to return a list of
                   $\operatorname\{poly\}(1/\alpha)$ answers, one of which is
                   correct? The second framework, which we term the
                   semi-verified learning model, considers the extent to which
                   a small dataset of trusted data (drawn from the distribution
                   in question) can be leveraged to enable the accurate
                   extraction of information from a much larger but untrusted
                   dataset (of which only an $\alpha$-fraction is drawn from
                   the distribution). We show strong positive results in both
                   settings, and provide an algorithm for robust learning in a
                   very general stochastic optimization setting. This general
                   result has immediate implications for robust estimation in a
                   number of settings, including for robustly estimating the
                   mean of distributions with bounded second moments, robustly
                   learning mixtures of such distributions, and robustly
                   finding planted partitions in random graphs in which
                   significant portions of the graph have been perturbed by an
                   adversary.",
  month         =  nov,
  year          =  2016,
  keywords      = "Value Alignment;Reading List;COHRINT Reading List -- Brett",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.02315"
}

@INPROCEEDINGS{Israelsen2017-zb,
  title     = "Towards Adaptive Training of Agent-based Sparring Partners for
               Fighter Pilots",
  booktitle = "{InfoTech@Aerospace} Conference",
  author    = "Israelsen, Brett W and Ahmed, Nisar and Center, Kenneth and
               Green, Roderick and Jr, Winston Bennett",
  abstract  = "A key requirement for the current generation of artificial
               decision-makers is that they should adapt well to changes in
               unexpected situations. This paper addresses the situation in
               which an AI for aerial dog fighting, with tunable parameters
               that govern its behavior, must optimize behavior with respect to
               an objective function that is evaluated and learned through
               simulations. Bayesian optimization with a Gaussian Process
               surrogate is used as the method for investigating the objective
               function. One key benefit is that during optimization, the
               Gaussian Process learns a global estimate of the true objective
               function, with predicted outcomes and a statistical measure of
               confidence in areas that haven't been investigated yet. Having a
               model of the objective function is important for being able to
               understand possible outcomes in the decision space, for example
               this is crucial for training and providing feedback to human
               pilots. However, standard Bayesian optimization does not perform
               consistently or provide an accurate Gaussian Process surrogate
               function for highly volatile objective functions. We treat these
               problems by introducing a novel sampling technique called Hybrid
               Repeat/Multi-point Sampling. This technique gives the AI ability
               to learn optimum behaviors in a highly uncertain environment.
               More importantly, it not only improves the reliability of the
               optimization, but also creates a better model of the entire
               objective surface. With this improved model the agent is
               equipped to more accurately/efficiently predict performance in
               unexplored scenarios.",
  pages     = "1--15",
  month     =  jan,
  year      =  2017,
  address   = "Grapevine, TX",
  keywords  = "BayesOpt;My Papers;myPapers"
}

@ARTICLE{Vickers2003-co,
  title     = "How many repeated measures in repeated measures designs?
               Statistical issues for comparative trials",
  author    = "Vickers, Andrew J",
  abstract  = "BACKGROUND: In many randomized and non-randomized comparative
               trials, researchers measure a continuous endpoint repeatedly in
               order to decrease intra-patient variability and thus increase
               statistical power. There has been little guidance in the
               literature as to selecting the optimal number of repeated
               measures. METHODS: The degree to which adding a further measure
               increases statistical power can be derived from simple formulae.
               This ``marginal benefit'' can be used to inform the optimal
               number of repeat assessments. RESULTS: Although repeating
               assessments can have dramatic effects on power, marginal benefit
               of an additional measure rapidly decreases as the number of
               measures rises. There is little value in increasing the number
               of either baseline or post-treatment assessments beyond four, or
               seven where baseline assessments are taken. An exception is when
               correlations between measures are low, for instance, episodic
               conditions such as headache. CONCLUSIONS: The proposed method
               offers a rational basis for determining the number of repeat
               measures in repeat measures designs.",
  journal   = "BMC Med. Res. Methodol.",
  publisher = "BioMed Central",
  volume    =  3,
  pages     = "22",
  month     =  oct,
  year      =  2003,
  keywords  = "BayesOpt/Acquisition/InfillFxns",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ernest2016-vi,
  title    = "Genetic Fuzzy based Artificial Intelligence for Unmanned Combat
              Aerial Vehicle Control in Simulated Air Combat Missions",
  author   = "Ernest, Nicholas and Carroll, David",
  abstract = "Volume 6 • Issue 1 • 1000144 J Def Manag ISSN: 2167-0374 JDFM, an
              open access journal While this automates the process, the
              computational cost of the genetic algorithm for searching over a
              practically infinite solution space causes the traditional
              genetic fuzzy system to be infeasible in more complex problems.
              Methodologies have been developed to help mitigate this, but one
              method in particular is capable of applying fuzzy control to
              problems of this scope. The Genetic Fuzzy Tree (GFT) has shown an
              incredible ability to obtain unparalleled levels of performance
              in very large and complex problems that contain all of the
              difficulties that alternative intelligent systems have issues
              coping with. This new subtype of genetic fuzzy system was
              recently developed during Dr. Ernest's graduate studies, under
              the guidance of Dr.'s Cohen and Schumacher and supported by the
              Dayton Area Graduate Studies Institute. The aim of this initial
              work was to control a flight of ground strike UCAVs in a
              low-fidelity simulation environment [5,6]. The success of this
              study led to Psibernetix Inc. partnering with the US air force
              research laboratory (AFRL) to apply the GFT methodology to a much
              more complex problem. Just as UAVs represented a revolutionary
              capability for the USAF in the mid-1990s, Manned-Unmanned
              Autonomous Teaming in an air combat environment will certainly
              represent a revolutionary leap in capability of airpower in the
              near future. Air combat, as it is performed by human pilots today
              is a highly dynamic application of aerospace physics, skill, art,
              and intuition to maneuver a fighter aircraft and missile against
              an adversary moving at high speeds in three dimensions. Today's
              fighters close on each other at speeds in excess of 1,500 MPH
              while flying at altitudes above 40,000 feet. The selection and
              application of air-to-air tactics requires assessing a tactical
              advantage or disadvantage and reacting appropriately in
              microseconds. The cost of mistakes is high.",
  journal  = "J Def Manag",
  volume   =  06,
  number   =  01,
  year     =  2016,
  keywords = "AFRL\_STTR"
}

@BOOK{Forrester2008-zo,
  title     = "Engineering design via surrogate modelling: a practical guide",
  author    = "Forrester, Alexander and Sobester, Andras and Keane, Andy",
  publisher = "John Wiley \& Sons",
  year      =  2008,
  keywords  = "GPs"
}

@ARTICLE{Croarkin2016-kv,
  title    = "e-Handbook of Statistical Methods",
  author   = "Croarkin, Carroll and Tobian, Paul",
  journal  = "NIST/SEMATECH, Available online: http://www. itl. nist.
              gov/div898/handbook",
  year     =  2016,
  keywords = "TextBooks"
}

@BOOK{Pyzdek2003-lk,
  title     = "Quality engineering handbook",
  author    = "Pyzdek, Thomas and Keller, Paul A",
  publisher = "CRC Press",
  year      =  2003,
  keywords  = "TextBooks"
}

@MISC{Wikipedia2016-bq,
  title  = "Algebraic Connectivity --- \{W\}ikipedia\{,\} The Free Encyclopedia",
  author = "{Wikipedia}",
  year   =  2016
}

@BOOK{Anderson1999-ay,
  title     = "{LAPACK} Users' guide -- Symmetric Eigenvalue Reduction",
  author    = "Anderson, Edward and Bai, Zhaojun and Bischof, Christian and
               Blackford, Susan and Dongarra, Jack and Du Croz, Jeremy and
               Greenbaum, Anne and Hammarling, Sven and McKenney, A and
               Sorensen, D",
  publisher = "Siam",
  volume    =  9,
  year      =  1999
}

@MISC{Wikipedia2016-od,
  title  = "Simulated Annealing --- \{W\}ikipedia\{,\} The Free Encyclopedia",
  author = "{Wikipedia}",
  year   =  2016
}

@MISC{Wikipedia2016-ri,
  title  = "Jacobi eigenvalue algorithm --- \{W\}ikipedia\{,\} The Free
            Encyclopedia",
  author = "{Wikipedia}",
  year   =  2016
}
