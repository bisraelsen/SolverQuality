\section{Conclusions} \label{sec:conclusions}
Unmanned autonomous physical systems are able to tackle complex decision making problems for high-consequence applications, but in order to be able to reduce the amount of supervision required these systems need to be able to perform self-assessment, or introspection. We draw on \emph{Factorized Machine Self-Confidence (\famsec)} which is a framework of self-assessments that enable an APS to quantify its own capabilities.

Specifically, herein, we have motivated and derived a one of the factors of \famsec{} called `Solver Quality' (\xQ) that indicates the ability of some solver to perform on a given task. Calculating \xQ{} relies only on a supervised model of a trusted solver, and simulated reward distributions of candidate solvers. This approach was inspired by literature on empirical hardness models (EHMs). We have shown by numerical experiments that \xQ{}, as derived here, meets the desired criteria.

Concerning \xQ{}, it remains to be seen, and is currently left for future work, whether it actually helps users to be able to understand the capabilities and limitations of the APS. Evaluations with human participants are required.

The simulations run so far have not directly considered `different classes' of solvers, however as \xQ{} only depends on reward distributions \rwd{}, and \rwdstar{} this is not a limitation. Also, since the calculation of \xQ{} generally depends on \rwdstar{} predicted from \surrogate{} it would be prudent to enable the surrogate to predict \rwdstar{} as well as an associated uncertainty in order to have an indication of where \surrogate{} can be trusted.

Another direction for future work is to develop approaches for the remaining three \famsec{} factors. Each of the individual factors reflects a critical meta-assessment of the competency of the APS.
